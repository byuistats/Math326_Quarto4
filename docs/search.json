[
  {
    "objectID": "bf3.html",
    "href": "bf3.html",
    "title": "BF[3]",
    "section": "",
    "text": "This section will extend a factorial design with two factors to a full factorial design with three factors. The design and analysis of a fully factorial experiment with three factors is very similar to the two factor design. The one notable exception is that now there is the potential for higher order interactions. Instead of just a single two-way interaction to be considered, three-way interactions, as well as all possible two-way interactions, are included in the model\nTo illustrate this design and analysis we will use a fictitious dataset that closely mirrors the real experiment conducted by Robert Kaplan. The experiment’s goal was to show how the halo effect due to attractiveness effects influences an assessment. More precisely, how does an author’s attractiveness, their sex, and the sex of the reader impact the reader’s assessment of the author’s talent as a writer.\nTo carry out the experiment, a participant is treated as the reader. There were 48 readers total, 24 male and 24 female. The participant is asked to read an essay. Though all participants read the same essay, the sex of the author and the attractiveness of the author was varied. This was accomplished by attaching a picture of the author to the essay. There were 3 levels of attractiveness for each author sex: attractive, unattractive, and the control group (where no picture was attached). Each reader was randomly assigned to an author sex and attractiveness level such that for every combination of reader sex, author sex, and author attractiveness there were exactly 4 observations gathered, resulting in a total of 48 observations.\nIt is cleaner and easier to show the data in a different format than has been used up to this point. In Table 1, each factor has a column dedicated to show what factor level the observations in that row belong to. There are 12 unique factor level combinations, and therefore 12 rows in the table There are 4 observations per factor level combination, and they are displayed to the right of factor level combination designation.\n\n\nCode\nattractiveness &lt;- rep(c(\"attractive\", \"unattractive\", \"no-pic\"), 16)\nreader &lt;- rep( rep(c(\"male\", \"female\"), each = 3), 8)\nauthor &lt;- rep(rep(c(\"male\", \"female\"), each = 6), 4)\nround &lt;- rep(1:4, each = 12)\n\nthe_means &lt;- c(80, 60, 70, 70, 70 , 70, 90, 50, 70, 50, 90, 70)\n\nset.seed(20)\ntalent &lt;- c(round(rnorm(12, the_means, 5),0),\n        round(rnorm(12, the_means, 5),0),\n        round(rnorm(12, the_means, 5),0),\n        round(rnorm(12, the_means, 5),0))\n\ndf &lt;- tibble(attractiveness) %&gt;% add_column(author, reader, talent, round) %&gt;% arrange(attractiveness, author, reader)\ndf %&gt;% pivot_wider(id_cols = c(\"reader\", \"author\", \"attractiveness\"),\n                   names_from = round,\n                   values_from = talent) %&gt;% \n  arrange(reader, author, attractiveness) %&gt;% add_column(row = 1:12) %&gt;% relocate(row) %&gt;% \n  pander()\n\n\n\n\nTable 1: Table of Means for Each Factor Level Combination\n\n\n\n\n\n\n\n\n\n\n\n\nrow\nreader\nauthor\nattractiveness\n1\n2\n3\n4\n\n\n\n\n1\nfemale\nfemale\nattractive\n47\n49\n56\n50\n\n\n2\nfemale\nfemale\nno-pic\n69\n72\n61\n74\n\n\n3\nfemale\nfemale\nunattractive\n90\n94\n85\n94\n\n\n4\nfemale\nmale\nattractive\n63\n68\n64\n72\n\n\n5\nfemale\nmale\nno-pic\n73\n70\n67\n62\n\n\n6\nfemale\nmale\nunattractive\n68\n75\n58\n66\n\n\n7\nmale\nfemale\nattractive\n76\n90\n89\n93\n\n\n8\nmale\nfemale\nno-pic\n68\n71\n78\n65\n\n\n9\nmale\nfemale\nunattractive\n46\n52\n58\n48\n\n\n10\nmale\nmale\nattractive\n86\n77\n79\n75\n\n\n11\nmale\nmale\nno-pic\n79\n62\n67\n70\n\n\n12\nmale\nmale\nunattractive\n57\n67\n53\n66\n\n\n\n\n\n\n\n\nWe can still partition the observations, but with 3 factors the partitions become harder to show with just lines. Instead, the partitions will be described and a table of factor levels will be shown below.\n\n\nIn the appendix, partitioning is done using shading/coloring of cells. Though it is possible, the result is a bit overwhelming to the eye.\nIn this experiment there are 3 controlled factors. Observations can be partitioned according to what group they belong to for any one of the 3 controlled factors. This allows us to create 3 structural factors, 1 for each controlled factor:\n\nreader’s sex: 2 levels.\nauthor’s sex: 2 levels, and\nattractiveness (of the essay author): 3 levels,\n\nThe partition associated with each of these factors will allow us to calculate main effects.\nThe data can also be partitioned according to a shared factor level combination on a pair of factors. There are 3 possible factor pairs. In other words, three two-way interactions are created by partitioning the data this way:\n\nreader’s gender \\(\\times\\) author’s gender: 4 levels\nreader’s gender \\(\\times\\) author’s attractiveness: 6 levels\nauthor’s gender \\(\\times\\) author’s attractiveness: 6 levels\n\nFinally, data values can be partitioned according to what combination of the 3 treatment factors the observation belongs to. This results in a 3-way interaction factor:\n\nreader’s gender \\(\\times\\) author’s gender \\(\\times\\) author’s attractiveness: 12 levels\n\nThe table below summarizes the 7 structural factors in the model:\n\n\n\n\n\n\n\n\n\n\n\n\nreader\n# of levels :============: 2\nPartitions: Row ID #’s of Table 1 belonging to each level ==============================================================+ female: 1-6 | | male:    7-12 |\n\n\nauthor\n2\nfemale: 1-3, 7-10\nmale:    4-6, 11-12\n\n\n\nattractiveness\n3\nattractive:    1, 4, 7, 10\nno-pic:        2, 5, 8, 11\nunattractive: 3, 6, 9, 12\n\n\n\nreader \\(\\times\\) author\n4\n\n\n\n\n\n\n\nfemale reader\nmale reader\n\n\n\n\nfemale author: 1-3\nmale author:   4-6\nfemale author: 7-9\nmale author:   10-12\n\n\n\n\n\n\nreader \\(\\times\\) attractiveness\n6\n\n\n\n\n\n\n\nfemale reader\nmale reader\n\n\n\n\nattractive: 1, 4\nno-pic: 2, 5\nunattractive: 3, 6\nattractive: 7, 10\nno-pic: 8, 11\nunattractive: 9, 12\n\n\n\n\n\n\nauthor \\(\\times\\) attractiveness\n6\n\n\n\n\n\n\n\nfemale author\nmale author\n\n\n\n\nattractive: 1, 7\nno-pic: 2, 8\nunattractive: 3, 9\nattractive: 4, 10\nno-pic: 5, 11\nunattractive: 6, 12\n\n\n\n\n\n\nreader \\(\\times\\) author \\(\\times\\) attractiveness\n12\nfemale, female, attractive: 1\nfemale, female, no-pic: 2\nfemale, female, unattractive: 3\nfemale, male, attractive: 4\nfemale, male, no-pic: 5\nfemale, male, unattractive: 6\nmale, female, attractive: 7\nmale, female, no-pic:      8\nmale, female, unattractive: 9\nmale, male, attractive: 10\nmale, male, no-pic: 11\nmale, male, unattractive: 12\n\n\n\n\n\n\n\nEach factor (i.e. meaningful partition of the data) corresponds to a term in Equation 1:\n\\[\ny_\\text{ijkl} = \\mu + \\alpha_i + \\beta_j + \\gamma_k + (\\alpha\\beta)_\\text{ij} + (\\alpha\\gamma)_\\text{ik} + (\\beta\\gamma)_\\text{jk} + (\\alpha\\beta\\gamma)_\\text{ijk} + \\epsilon_\\text{ijkl}\n\\tag{1}\\]\nWhere\n\n\\(y_{ijkl}\\) is the \\(l^{th}\\) observation from the factor level combination of \\(\\alpha_i\\), \\(\\beta_j\\), and \\(\\gamma_k\\).\n\\(\\mu\\) is the grand mean of all the observations.\n\\(\\alpha\\) is the effect of reader’s gender, and \\(i\\) has 2 levels: i=1 for female, i=2 for male\n\\(\\beta\\) is the effect of author’s gender, and \\(j\\) has 2 levels: i=1 for female, i=2 for male\n\\(\\gamma\\) is the effect of author’s attractiveness, and \\(k\\) has 3 levels: k=1 for attractive, k=2 for neutral, and k=3 for unattractive\nThe \\((\\alpha\\beta)_\\text{ij}\\) is the interaction effect for reader gender with author gender on perceived talent level.\nThe \\((\\alpha\\gamma)_\\text{ik}\\) is the interaction effect for reader gender with author attractiveness on perceived talent.\nThe \\((\\beta\\gamma)_\\text{jk}\\) is the interaction effect for author gender with attractiveness on perceived talent.\nThe \\((\\alpha\\beta\\gamma)_\\text{ijk}\\) is the interaction effect for reader gender, author gender, and attractiveness on perceived talent.\n\\(\\epsilon\\) is the residual error term, and \\(l\\) is the replicate count within a factor level combination.\n\nThough we have seen two factor interactions and know how to deal with them, the three factor interaction is something new. Including this term in the model means that the impact of any of the three variables depends on the levels of the other two. Stated another way, the nature of the two-way interaction depends on the level of a third variable.\nThe main effects and two-way interactions are tested just as they were in the BF[2] model. The hypothesis for the 3-way interaction is\n\\[\nH_0: (\\alpha\\beta\\gamma)_\\text{ijk} = 0 \\text{ for all } ijk\n\\]\n\\[\nH_a: (\\alpha\\beta\\gamma)_\\text{ijk} \\ne 0 \\text{ for some } ijk\n\\]\n\n\n\nA three-way ANOVA model may be used to analyze data from a BF[3] design if the following requirements are satisfied. Note that these requirements are identical to the requirements of a BF[2] two-way ANOVA.\n\n\n\n\n\n\n\n\nRequirements\nMethod for Checking\nWhat You Hope to See\n\n\n\n\nConstant variance across factor levels\nResidual vs. Fitted Plot\nNo major disparity in vertical spread of point groupings\n\n\n\nLevene’s Test\nFail to reject \\(H_0\\)\n\n\nNormally Distributed Residuals\nNormal Q-Q plot\nStraight line, majority of points in boundaries\n\n\nIndependent residuals\nOrder plot\nNo pattern/trend\n\n\n\nFamiliarity with/critical thinking about the experiment\nNo potential source for bias"
  },
  {
    "objectID": "bf3.html#factor-structure",
    "href": "bf3.html#factor-structure",
    "title": "BF[3]",
    "section": "",
    "text": "We can still partition the observations, but with 3 factors the partitions become harder to show with just lines. Instead, the partitions will be described and a table of factor levels will be shown below.\n\n\nIn the appendix, partitioning is done using shading/coloring of cells. Though it is possible, the result is a bit overwhelming to the eye.\nIn this experiment there are 3 controlled factors. Observations can be partitioned according to what group they belong to for any one of the 3 controlled factors. This allows us to create 3 structural factors, 1 for each controlled factor:\n\nreader’s sex: 2 levels.\nauthor’s sex: 2 levels, and\nattractiveness (of the essay author): 3 levels,\n\nThe partition associated with each of these factors will allow us to calculate main effects.\nThe data can also be partitioned according to a shared factor level combination on a pair of factors. There are 3 possible factor pairs. In other words, three two-way interactions are created by partitioning the data this way:\n\nreader’s gender \\(\\times\\) author’s gender: 4 levels\nreader’s gender \\(\\times\\) author’s attractiveness: 6 levels\nauthor’s gender \\(\\times\\) author’s attractiveness: 6 levels\n\nFinally, data values can be partitioned according to what combination of the 3 treatment factors the observation belongs to. This results in a 3-way interaction factor:\n\nreader’s gender \\(\\times\\) author’s gender \\(\\times\\) author’s attractiveness: 12 levels\n\nThe table below summarizes the 7 structural factors in the model:\n\n\n\n\n\n\n\n\n\n\n\n\nreader\n# of levels :============: 2\nPartitions: Row ID #’s of Table 1 belonging to each level ==============================================================+ female: 1-6 | | male:    7-12 |\n\n\nauthor\n2\nfemale: 1-3, 7-10\nmale:    4-6, 11-12\n\n\n\nattractiveness\n3\nattractive:    1, 4, 7, 10\nno-pic:        2, 5, 8, 11\nunattractive: 3, 6, 9, 12\n\n\n\nreader \\(\\times\\) author\n4\n\n\n\n\n\n\n\nfemale reader\nmale reader\n\n\n\n\nfemale author: 1-3\nmale author:   4-6\nfemale author: 7-9\nmale author:   10-12\n\n\n\n\n\n\nreader \\(\\times\\) attractiveness\n6\n\n\n\n\n\n\n\nfemale reader\nmale reader\n\n\n\n\nattractive: 1, 4\nno-pic: 2, 5\nunattractive: 3, 6\nattractive: 7, 10\nno-pic: 8, 11\nunattractive: 9, 12\n\n\n\n\n\n\nauthor \\(\\times\\) attractiveness\n6\n\n\n\n\n\n\n\nfemale author\nmale author\n\n\n\n\nattractive: 1, 7\nno-pic: 2, 8\nunattractive: 3, 9\nattractive: 4, 10\nno-pic: 5, 11\nunattractive: 6, 12\n\n\n\n\n\n\nreader \\(\\times\\) author \\(\\times\\) attractiveness\n12\nfemale, female, attractive: 1\nfemale, female, no-pic: 2\nfemale, female, unattractive: 3\nfemale, male, attractive: 4\nfemale, male, no-pic: 5\nfemale, male, unattractive: 6\nmale, female, attractive: 7\nmale, female, no-pic:      8\nmale, female, unattractive: 9\nmale, male, attractive: 10\nmale, male, no-pic: 11\nmale, male, unattractive: 12"
  },
  {
    "objectID": "bf3.html#hypothesis-and-model",
    "href": "bf3.html#hypothesis-and-model",
    "title": "BF[3]",
    "section": "",
    "text": "Each factor (i.e. meaningful partition of the data) corresponds to a term in Equation 1:\n\\[\ny_\\text{ijkl} = \\mu + \\alpha_i + \\beta_j + \\gamma_k + (\\alpha\\beta)_\\text{ij} + (\\alpha\\gamma)_\\text{ik} + (\\beta\\gamma)_\\text{jk} + (\\alpha\\beta\\gamma)_\\text{ijk} + \\epsilon_\\text{ijkl}\n\\tag{1}\\]\nWhere\n\n\\(y_{ijkl}\\) is the \\(l^{th}\\) observation from the factor level combination of \\(\\alpha_i\\), \\(\\beta_j\\), and \\(\\gamma_k\\).\n\\(\\mu\\) is the grand mean of all the observations.\n\\(\\alpha\\) is the effect of reader’s gender, and \\(i\\) has 2 levels: i=1 for female, i=2 for male\n\\(\\beta\\) is the effect of author’s gender, and \\(j\\) has 2 levels: i=1 for female, i=2 for male\n\\(\\gamma\\) is the effect of author’s attractiveness, and \\(k\\) has 3 levels: k=1 for attractive, k=2 for neutral, and k=3 for unattractive\nThe \\((\\alpha\\beta)_\\text{ij}\\) is the interaction effect for reader gender with author gender on perceived talent level.\nThe \\((\\alpha\\gamma)_\\text{ik}\\) is the interaction effect for reader gender with author attractiveness on perceived talent.\nThe \\((\\beta\\gamma)_\\text{jk}\\) is the interaction effect for author gender with attractiveness on perceived talent.\nThe \\((\\alpha\\beta\\gamma)_\\text{ijk}\\) is the interaction effect for reader gender, author gender, and attractiveness on perceived talent.\n\\(\\epsilon\\) is the residual error term, and \\(l\\) is the replicate count within a factor level combination.\n\nThough we have seen two factor interactions and know how to deal with them, the three factor interaction is something new. Including this term in the model means that the impact of any of the three variables depends on the levels of the other two. Stated another way, the nature of the two-way interaction depends on the level of a third variable.\nThe main effects and two-way interactions are tested just as they were in the BF[2] model. The hypothesis for the 3-way interaction is\n\\[\nH_0: (\\alpha\\beta\\gamma)_\\text{ijk} = 0 \\text{ for all } ijk\n\\]\n\\[\nH_a: (\\alpha\\beta\\gamma)_\\text{ijk} \\ne 0 \\text{ for some } ijk\n\\]"
  },
  {
    "objectID": "bf3.html#assumptions",
    "href": "bf3.html#assumptions",
    "title": "BF[3]",
    "section": "",
    "text": "A three-way ANOVA model may be used to analyze data from a BF[3] design if the following requirements are satisfied. Note that these requirements are identical to the requirements of a BF[2] two-way ANOVA.\n\n\n\n\n\n\n\n\nRequirements\nMethod for Checking\nWhat You Hope to See\n\n\n\n\nConstant variance across factor levels\nResidual vs. Fitted Plot\nNo major disparity in vertical spread of point groupings\n\n\n\nLevene’s Test\nFail to reject \\(H_0\\)\n\n\nNormally Distributed Residuals\nNormal Q-Q plot\nStraight line, majority of points in boundaries\n\n\nIndependent residuals\nOrder plot\nNo pattern/trend\n\n\n\nFamiliarity with/critical thinking about the experiment\nNo potential source for bias"
  },
  {
    "objectID": "latin_square.html",
    "href": "latin_square.html",
    "title": "Latin Square",
    "section": "",
    "text": "The CB[1] design works well when there is only one variable to block on. What can be done when there are two nuisance factors to block on? If those two blocking factors and the treatment all have the same number of levels, then a latin square design should be used.\n\n\n\n\n\n\nWhen to use Latin Square\n\n\n\nLatin Square designs are appropriate when\n\nTwo blocking factors and one treatment factor\nAll three factors have the same number of levels\nTreatments can be assigned to experimental units\n\n\n\nPreviously, in the CB[1] design we used the toothbrush study and blocked on participant. If we only had 4 participants and we considered “order” a nuisance factor, we could use a Latin Square. By adding “order” as a blocking variable, we can ensure that the order of the treatments does not all become the same by random chance alone.\nBlocking on subjects and order of treatments is one of the most common applications of a Latin Square design in psychology. (Treatments, of course, must be something that can be assigned to experimental units). Be aware that in these types of experiments carry-over effects (such as learning and fatigue) can be problematic. Experimental protocols need to proactively address potential carry-over effects1.\nLatin Square designs are also common in agriculture, where they were originally developed. The field is divided into a square grid and treatments are randomly applied to each cell of the grid. The blocking variables are the row position and column position in the grid respectively.\nFigure 1 shows two images of Latin Square designs in agriculture, pulled from Bailey, Cameron, and Connelly’s 2008 article in American Mathematical Monthly 2. Figure 1 (a) is a picture of a 5×5 forestry experiment on a hill in Beddgelert in Wales. The experiment was “designed by Fisher, laid out in 1929, and photographed in about 1945”. Figure 1 (b) shows “a 6×6 experiment to compare methods of controlling aphids; conducted by Lesley Smart at Rothamsted Research, photographed in 2004.”\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n(b)\n\n\n\n\nFigure 1: Agricultural examples of latin squares\n\n\nRegardless of whether the application is psychology, agriculture or something else, the key design feature of a Latin Square design is that each treatment appears exactly once in each row and in each column of the square.\nThere are various extensions to this basic Latin Square idea. Graeco Latin Squares can be used if you have more than 2 variables to block on (provided all factors have the same number of levels, and the number is not 6). Replicated Latin Squares is useful if the number of experimental units is a multiple of (instead of exactly equal to) the number of treatment factor levels. Replicated Latin Squares is discussed briefly here and here.\n\n\nIn the factor diagram, one nuisance factor’s levels are associated with the row partitions of the dataset. The other nuisance factor’s levels are marked by partitioning the dataset by columns. The treatment is assigned a letter inside the factor diagram. Figure 2 is a factor structure diagram for a Latin Square design where the controlled factors all have 4 levels. This is an unrandomized layout of the treatments: each row (and column) follows the same sequence of treatments. In the Design section we will learn to randomize the design.\n\n\n\nFigure 2: Factor Structure for Latin Square Design with 4 Treatment Levels\n\n\n\n\n\nEach factor (i.e. meaningful partition of the data) in Figure 2 corresponds to a term on the right hand side of Equation 1:\n\\[\ny_{ijk} = \\mu + \\alpha_i + \\beta_j + \\gamma_k + \\epsilon_{ijk}\n\\tag{1}\\]\nWhere\n\n\\(y_{ij}\\) is the observation that belongs to level i of \\(\\alpha\\), level j of \\(\\beta\\), and level k of \\(\\gamma\\).\n\\(\\mu\\) is the grand mean of the entire dataset.\n\\(\\alpha\\) is the effect of the block factor partitioned by rows\n\\(\\beta\\) is the effect of the block factor partitioned by columns\n\\(\\gamma\\) is the effect of the treatment factor, designated by a letter value. Each treatment appears exactly once in each row and each column.\n\\(\\epsilon\\) is the residual error term\n\nThe Latin Square is an incomplete block design. In other words, each treatment does not show up in each block. In other words, not all subscript combinations of i, j, and k will be observed. Therefore there are insufficient observations to estimate and test interaction effects.\nThe hypothesis for the treatment is\n\\[H_0: \\gamma_k = 0 \\text{ for all }k\\]\n\\[H_a: \\gamma_k \\ne 0 \\text{ for some }k\\]\n\n\nThe focus of the study is on the treatment factor. However, you can test the block factors in the same way you test the treatment factor.\n\n\n\nAn ANOVA model may be used to analyze data from a CB[1] design if the following requirements are satisfied.\n\n\n\n\n\n\n\n\nRequirements\nMethod for Checking\nWhat You Hope to See\n\n\n\n\nConstant variance across factor levels[^1]\nResidual vs. Fitted Plot\nNo major disparity in vertical spread of point groupings\n\n\nNormally Distributed Residuals\nNormal Q-Q plot\nStraight line, majority of points in boundaries\n\n\nIndependent residuals\nOrder plot\nNo pattern/trend\n\n\n\nFamiliarity with/critical thinking about the experiment\nNo potential source for bias"
  },
  {
    "objectID": "latin_square.html#factor-structure",
    "href": "latin_square.html#factor-structure",
    "title": "Latin Square",
    "section": "",
    "text": "In the factor diagram, one nuisance factor’s levels are associated with the row partitions of the dataset. The other nuisance factor’s levels are marked by partitioning the dataset by columns. The treatment is assigned a letter inside the factor diagram. Figure 2 is a factor structure diagram for a Latin Square design where the controlled factors all have 4 levels. This is an unrandomized layout of the treatments: each row (and column) follows the same sequence of treatments. In the Design section we will learn to randomize the design.\n\n\n\nFigure 2: Factor Structure for Latin Square Design with 4 Treatment Levels"
  },
  {
    "objectID": "latin_square.html#model-and-hypotheses",
    "href": "latin_square.html#model-and-hypotheses",
    "title": "Latin Square",
    "section": "",
    "text": "Each factor (i.e. meaningful partition of the data) in Figure 2 corresponds to a term on the right hand side of Equation 1:\n\\[\ny_{ijk} = \\mu + \\alpha_i + \\beta_j + \\gamma_k + \\epsilon_{ijk}\n\\tag{1}\\]\nWhere\n\n\\(y_{ij}\\) is the observation that belongs to level i of \\(\\alpha\\), level j of \\(\\beta\\), and level k of \\(\\gamma\\).\n\\(\\mu\\) is the grand mean of the entire dataset.\n\\(\\alpha\\) is the effect of the block factor partitioned by rows\n\\(\\beta\\) is the effect of the block factor partitioned by columns\n\\(\\gamma\\) is the effect of the treatment factor, designated by a letter value. Each treatment appears exactly once in each row and each column.\n\\(\\epsilon\\) is the residual error term\n\nThe Latin Square is an incomplete block design. In other words, each treatment does not show up in each block. In other words, not all subscript combinations of i, j, and k will be observed. Therefore there are insufficient observations to estimate and test interaction effects.\nThe hypothesis for the treatment is\n\\[H_0: \\gamma_k = 0 \\text{ for all }k\\]\n\\[H_a: \\gamma_k \\ne 0 \\text{ for some }k\\]\n\n\nThe focus of the study is on the treatment factor. However, you can test the block factors in the same way you test the treatment factor."
  },
  {
    "objectID": "latin_square.html#assumptions",
    "href": "latin_square.html#assumptions",
    "title": "Latin Square",
    "section": "",
    "text": "An ANOVA model may be used to analyze data from a CB[1] design if the following requirements are satisfied.\n\n\n\n\n\n\n\n\nRequirements\nMethod for Checking\nWhat You Hope to See\n\n\n\n\nConstant variance across factor levels[^1]\nResidual vs. Fitted Plot\nNo major disparity in vertical spread of point groupings\n\n\nNormally Distributed Residuals\nNormal Q-Q plot\nStraight line, majority of points in boundaries\n\n\nIndependent residuals\nOrder plot\nNo pattern/trend\n\n\n\nFamiliarity with/critical thinking about the experiment\nNo potential source for bias"
  },
  {
    "objectID": "latin_square.html#r-randomization",
    "href": "latin_square.html#r-randomization",
    "title": "Latin Square",
    "section": "R randomization",
    "text": "R randomization\n\nWide format\nIf the design is laid out similar to the factor structure diagram it is considered to be in wider format. This code creates a table in wider format of the unrandomized design.\n\n#First create the unrandomized layout design, labeling the rows and columns as 1, 2, 3, 4\nls_design &lt;- tibble(`1` = c(`1`  = \"A\", `2` =\"B\", `3`=  \"C\", `4` = \"D\"), \n                             `2` = c(\"B\", \"C\", \"D\", \"A\"), \n                             `3` = c(\"C\", \"D\", \"A\", \"B\"), \n                             `4` = c(\"D\", \"A\", \"B\", \"C\"))\nls_design\n\n# A tibble: 4 × 4\n  `1`   `2`   `3`   `4`  \n  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n1 A     B     C     D    \n2 B     C     D     A    \n3 C     D     A     B    \n4 D     A     B     C    \n\n\nOne simple way is to carry out the randomization is to use the sample() command in conjunction with the [] notation. The sample() command randomly shuffles the values it is given. The square brackets allow you to reference rows and columns of a matrix. The first argument in the square brackets refers to rows, the second argument refers to columns.\n\n#Then randomize rows\nrow_randomized &lt;- ls_design[sample(nrow(ls_design)), ]\n\n#Then randomize columns\nall_randomized &lt;- row_randomized[ , sample(ncol(row_randomized))]\n\n\n#The above randomization steps can be combined into one command\nls_design[ sample(nrow(ls_design)), sample(ncol(ls_design))]\n\n# A tibble: 4 × 4\n  `1`   `3`   `4`   `2`  \n  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n1 C     A     B     D    \n2 B     D     A     C    \n3 A     C     D     B    \n4 D     B     C     A    \n\n\n\n\nLonger format\nThe R language often wants data in longer format. To create the unrandomized design in longer format, use this code:\n\ntibble(row_blocks = rep(1:4, each = 4), \n       column_blocks = rep(1:4, times = 4),\n       treatment = c(\"A\", \"B\", \"C\", \"D\", \n                     \"B\", \"C\", \"D\", \"A\", \n                     \"C\", \"D\", \"A\", \"B\",\n                     \"D\", \"A\", \"B\", \"C\"))\n\n# A tibble: 16 × 3\n   row_blocks column_blocks treatment\n        &lt;int&gt;         &lt;int&gt; &lt;chr&gt;    \n 1          1             1 A        \n 2          1             2 B        \n 3          1             3 C        \n 4          1             4 D        \n 5          2             1 B        \n 6          2             2 C        \n 7          2             3 D        \n 8          2             4 A        \n 9          3             1 C        \n10          3             2 D        \n11          3             3 A        \n12          3             4 B        \n13          4             1 D        \n14          4             2 A        \n15          4             3 B        \n16          4             4 C        \n\n\nTo randomize the rows and columns, put the vector 1:4 within the sample() command when defining the row blocks and the column blocks, as shown below:\n\n#randomize rows\nrandomized_design &lt;- tibble(row_blocks = rep(sample(1:4), each = 4),\n                            column_blocks = rep(sample(1:4), times = 4),\n                            treatment = c(\"A\", \"B\", \"C\", \"D\", \n                                          \"B\", \"C\", \"D\", \"A\", \n                                          \"C\", \"D\", \"A\", \"B\",\n                                          \"D\", \"A\", \"B\", \"C\"))\nrandomized_design\n\nAfter carrying out the experiment and gathering data, a vector containing the observed values can be added to the dataset using cbind() or dplyr::bind_col() from the tidyverse.\n\n#Store observed data in a vector. \n# These observed values are copmletely made up\nobserved_values &lt;- c(-1, 2, 6, 11, -5, 2, 7, 5, 4,-2, -3, 8, 8, 2, 1, 0)\ncbind(randomized_design, observed_values)"
  },
  {
    "objectID": "latin_square.html#degrees-of-freedom",
    "href": "latin_square.html#degrees-of-freedom",
    "title": "Latin Square",
    "section": "Degrees of Freedom",
    "text": "Degrees of Freedom\nAs is the case with other designs learned so far, the grand mean factor is outside of all other factors. There is only one grand mean associated with the dataset, so there is just one degree of freedom associated with grand mean.\nThe general rule for calculating degrees of freedom states that the degrees of freedom for a factor are equal to the number of levels of that factor minus the sum of degrees of freedom of all outside factors.\nGrand mean is the only factor outside of intersection (rows), time of day (column), and treatment (algorithm)5. Therefore, for each of these factors we can take their number of levels and subtract one; the degrees of freedom for the 3 structural factors are 4-1 = 3.\nThere is one residual for each observation, therefore the number of factor levels for residual is equal to the sample size, 16. The residual factor is inside of all other factors, so degrees of freedom for residual is 16 – (1 + 3 + 3 + 3) = 66."
  },
  {
    "objectID": "latin_square.html#factor-effects",
    "href": "latin_square.html#factor-effects",
    "title": "Latin Square",
    "section": "Factor Effects",
    "text": "Factor Effects\nTo calculate factor effects, we start by calculating means for each level of every factor.\n\nFactor Means\nBelow is a table that contains observed throughput for the combinations of intersection, time of day, and algorithm (represented as A, B, C or D).\n\nObserved Throughput for Traffic Light Timing Algorithm Experiment\n\n\n\n8am\n11am\n2pm\n5pm\n\n\n\n\nIntersection 1\nA (32)\nB (33)\nC (47)\nD (53)\n\n\nIntersection 2\nB (36)\nD (53)\nA (42)\nC (54)\n\n\nIntersection 3\nC (51)\nA (44)\nD (62)\nB (49)\n\n\nIntersection 4\nD (81)\nC (78)\nB (72)\nA (73)\n\n\n\nThe grand mean and the mean of each factor level will be calculated for the 3 structural factors.\nThe grand mean is simply the mean of all the observations and is equal to 53.8.\nThe calculation to find the mean of an intersection (row) are as follows:\n\\[\n\\bar{y}_\\text{intersection 1} = \\bar{y}_{1\\cdot\\cdot} = \\frac{32 + 33 + 47 +  53}{4} = 41.3\n\\]\n\nSimilar calculations can be applied to obtain the row mean for the other rows.\nNow find the mean for each time of day (column). The calculation for the first column is shown below. Similar calculations can be applied to obtain the mean for each of the other columns as well.\n\\[\n\\bar{y}_\\text{8am} = \\bar{y}_{\\cdot 1 \\cdot} = \\frac{32 + 36 + 51 + 81}{4} = 50\n\\]\nTo find the mean for each algorithm, we add together the 4 observations that belong to each algorithm and divide by four.\n$$ \\[\\begin{align}\n\\bar{y}_A = \\bar{y}_{\\cdot \\cdot 1} &= \\frac{32 + 44 + 42 + 73}{4} = 47.8 \\\\\n\n\\bar{y}_B = \\bar{y}_{\\cdot \\cdot 2} &= \\frac{36 + 33 + 72 + 79}{4} = 47.5 \\\\\n\n\\bar{y}_C = \\bar{y}_{\\cdot \\cdot 3} &= \\frac{51 + 78 + 47 + 54}{4} = 57.5 \\\\\n\n\\bar{y}_D = \\bar{y}_{\\cdot \\cdot 4} &= \\frac{81 + 53 + 62 + 53}{4} = 62.3\n\\end{align}\\] $$\nWe do not need to calculate means for residual error factor for two reasons. First, there is only one observation per level of residual error, so the mean is the observation itself. Second, nothing is inside of residual error. It is the last step in the process and its mean is not needed to calculate factor effects.\nThe means for each factor level are shown in Figure 4.\n\n\n\nFigure 4: Factor Level Means\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe means in the image are rounded to 1 decimal place to save on space, but calculations should take advantage of full decimal precision."
  },
  {
    "objectID": "latin_square.html#factor-effects-1",
    "href": "latin_square.html#factor-effects-1",
    "title": "Latin Square",
    "section": "Factor Effects",
    "text": "Factor Effects\nNow that means for each level of each factor are calculated, we can move on to calculate effects of the factor levels. We will use the general formula for calculating effect size,\n\\[\n\\text{factor level effect} = \\text{factor level mean} - \\text{sum of all outside factor effects}\n\\]\nFor the grand mean, there is only one level and there are no outside factors. Therefore, the effect due to grand mean is 53.8 (equivalent to its mean) and this affect is applied to all 16 observations.\nThe intersection factor has four levels: one for each intersection. To calculate the effect of an intersection, take the intersection mean and subtract it from the effect due to the grand mean factor. For the intersection 1 this looks like:\n\\[\n41.25 - 53.75 = -12.5\n\\]\nThis result indicates that the mean throughput at intersection 1 is 12.5 fewer cars than the grand mean. Effects for the other 3 intersections are found with a similar calcultion.\nTo find the effect of a specific time of day, subtract the grand mean from the level’s mean. For “8am”, the calculation is\n\\[\n50 - 53.75 = -3.75\n\\]\nEffects of the other times of day are similarly calculated.\nTo find the effect of timing algorithm A, subtract the grand mean from the mean of algorithm A:\n\\[\n47.75 - 53.75 = -6\n\\]\nSimilarly, the effect of algorithm B is \\(47.5 - 53.75 = 3.25\\), algorithm C’s effect is \\(57.5 - 53.75 = 3.75\\) and D’s effect is \\(62.26 - 53.75 = 8.5\\).\nLastly, the residuals (or residual effects) need to be calculated. The mean for each level of residual is simply the observation itself. Effects associated with an observation’s factor levels are subtracted from the observed value. Whatever is left over is considered the residual. In other words, we have applied the general rule for calculating effect size. For the residual factor, the effect can concisely be stated as “observed value - predicted value”.\nAs an example, the residual in the top left corner of the residual factor was obtained with this calculation:\n\\[\n32 - (53.75 + -12.5 + -3.25 + -6) = 0.5\n\\]\nFigure 5 uses the factor structure diagram to show the build-up of each observations as the summation of each of its factor level effects.\n\n\n\n\nFigure 5: Factor Effects"
  },
  {
    "objectID": "latin_square.html#completing-the-anova-table",
    "href": "latin_square.html#completing-the-anova-table",
    "title": "Latin Square",
    "section": "Completing the ANOVA Table",
    "text": "Completing the ANOVA Table\nNow that we have calculated degrees of freedom and effects for each factor, we can calculate the remaining pieces of the ANOVA table: Sum of Squares (SS), Mean Squares (MS), F-statistic and p-value. A completed ANOVA summary table contains the information we need for a hypothesis test of the treatment effect.\nIn an ANOVA table, each factor and their associated degrees of freedom are listed on the left. The total degrees of freedom are the total number of observations.\n\n\n\n\n\nSource\ndf\nSS\nMS\nFvalue\npvalue\n\n\n\n\nGrand Mean\n1\n\n\n\n\n\n\nIntersection (row)\n3\n\n\n\n\n\n\nTime of day (column)\n3\n\n\n\n\n\n\nAlgorithm (treatment)\n3\n\n\n\n\n\n\nResidual Error\n6\n\n\n\n\n\n\nTotal\n16\n\n\n\n\n\n\n\n\n\n\n\nTo get the sum of squares (SS) of a factor, the effects of the factor must be squared, and then summed. The factor effects were displayed in Figure 5 above. Figure 6 (below) shows squared effects for the factors, excluding the grand mean and the observations.\n\n\n\n\nFigure 6: Squared Factor Effects\n\n\n\nThe total sum of squares ($ SS_{total}$) represents all the squared variability that we will need to allocate to the various factors. It is calculated by squaring each observation and then summing them together:\n\\[\nSS_{total} = 32^2 + 33^2 + … + 73^2 = 49856\n\\]\nTo get the Sum of Squares for the grand mean factor we first square the effect of grand mean, \\(53.75^2 = 2889.0625\\). That value occurs 16 times in the dataset (once for each observation), so $ SS_$ = 2889.0625 * 16 = 46225.\nTo get the sum of squares for each factor, we simply add all the squared effects. Since for intersections, time of day, and timing algorithm, each effect is repeated exactly 4 times, we will use some multiplication to simplify the calculation:\n\\[\\begin{align}\n\nSS_\\text{intersections} &= 4*(156.3 + 56.3 + 5.1 + 495.1) = 2850.5 \\\\\n\nSS_\\text{time of day} &= 4 *(14.1 + 3.1 + 4 + 12.3) = 133.5 \\\\\n\nSS_\\text{algorithm} &= 4 * (36 + 39.1 + 14.1 + 72.3) = 645.5 \\\\\n\nSS_\\text{residual} &= 4 * (0.25 + 0.06 + 0 + 0.06) = 1.5\n\\end{align}\\]\nPutting this information into the ANOVA table gives us Table 1.\n\n\n\n\nTable 1: Sums of squares\n\n\nSource\ndf\nSS\nMS\nFvalue\npvalue\n\n\n\n\nGrand Mean\n1\n46225.0\n\n\n\n\n\nIntersection (row)\n3\n2850.5\n\n\n\n\n\nTime of day (column)\n3\n133.5\n\n\n\n\n\nAlgorithm (treatment)\n3\n645.5\n\n\n\n\n\nResidual Error\n6\n1.5\n\n\n\n\n\nTotal\n16\n49856.0\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou can verify that we have successfully partitioned out the SS_total, try adding the sum of squares for all the factors together. The result should be equal to the sum of squares you got by squaring the observed values and summing them.\nThe next step is to covert the total variability (sum of squares) to an average variability per factor (mean squares). To create an average from a total, you must divide by the number of unique pieces of information that were summed to create the total, in this case the degrees of freedom. The mean squares can be thought of as the sample variance between factor level means.\nObtain the mean squares for each factor by dividing its sum of squares by its degrees of freedom.\n\n\n\n\n\nSource\ndf\nSS\nMS\nFvalue\npvalue\n\n\n\n\nGrand Mean\n1\n46225.0\n46225.00\n\n\n\n\nIntersection (row)\n3\n2850.5\n950.17\n\n\n\n\nTime of day (column)\n3\n133.5\n44.50\n\n\n\n\nAlgorithm (treatment)\n3\n645.5\n215.17\n\n\n\n\nResidual Error\n6\n1.5\n0.25\n\n\n\n\nTotal\n16\n49856.0\n3116.00\n\n\n\n\n\n\n\n\n\nThe objective of the study was to evaluate differences in timing algorithms of traffic lights. The intersection and time of day factors were simply nuisance factors we blocked on to better isolate the effect of timing algorithm. For that reason, we will only show the hypothesis test of the treatment factor (algorithm) here, though a similar test could be done for the blocking factors (intersection and time of day).\nIn Equation 1, \\(\\gamma\\) represents the effect of algorithm. Our hypotheses therefore are\n\\[\nH_o: \\gamma_k = 0 \\text{, for all } k\n\\]\n\\[\nH_a: \\gamma_k \\ne 0 \\text{, for some } k\n\\]\nTo test the hypothesis, we need to compare the mean square (MS) for algorithm to the mean square for residual error (abbreviated as MSE). This ratio of variances is called an F statistic.\n\\[\n\\text{F statistic} = \\frac{MS_{algorithm}}{MS_{error}} = \\frac{215.1\\overline{6}}{0.25} = 860.\\overline{6}\n\\]\nThis F statistic has 3 and 6 degrees of freedom, written as \\(F_{3,6} = 860.\\overline{6}\\). This is a huge F statistic.\nThe associated p-value is approximately zero as calculated in Excel with the function = f.dist.rt(860.6667, 3, 6).\n\n\n\n\n\nSource\ndf\nSS\nMS\nFvalue\npvalue\n\n\n\n\nGrand Mean\n1\n46225.0\n46225.00\n\n\n\n\nIntersection (row)\n3\n2850.5\n950.17\n\n\n\n\nTime of day (column)\n3\n133.5\n44.50\n\n\n\n\nAlgorithm (treatment)\n3\n645.5\n215.17\n860.667\n0\n\n\nResidual Error\n6\n1.5\n0.25\n\n\n\n\nTotal\n16\n49856.0\n3116.00\n\n\n\n\n\n\n\n\n\nWe can conclude from these results that at least one of the timing algorithms has a statistically significant effect on the number of cars flowing through an intersection. As calculated earlier, Algorithm D had the largest positive effect on throughput at an intersection and it seems reasonable to recommend this algorithm to those in charge of traffic lights."
  },
  {
    "objectID": "latin_square.html#check-assumptions",
    "href": "latin_square.html#check-assumptions",
    "title": "Latin Square",
    "section": "Check Assumptions",
    "text": "Check Assumptions\nFor a more detailed explanation of the code, output, and theory behind these assumptions visit the Assumptions page.\n\nConstant Variance of Residuals\nThe residuals need to demonstrate constant variance, regardless of the fitted or predicted value. We use a residual plot to check this assumption.\n\n\nCode\nplot(my_ls_aov, which = 1)\n\n\n\n\n\nFigure 8: Checking constant variance\n\n\n\n\n\n\nIgnore the red line in this plot\nThe x-axis of Figure 8 shows the fitted. Fitted values, also called predicted values, is the sum of the effects contributing to a datapoint. It includes all effects except for the residual effect. The residual is plotted on the y-axis.\nThe points in the plot do not show any increase (or decreasing) in vertical spread as we move along the x-axis. Therefore, we conclude this requirement is met.\n\n\nNormally Distributed Residuals\nWe check the assumption that residuals are normally distributed in Figure 9. All the points are in the shaded region.\n\n\nCode\ncar::qqPlot(my_ls_aov$residuals, id = FALSE)\n\n\n\n\n\nFigure 9: Checking normality of residuals\n\n\n\n\n\n\nIndependent Residuals\nThe dataset we are analyzing does not include information about the order in which the data was collected. In fact, it is possible some conditions of the experiment were run simultaneously and there is no specific order. From what we know, there is no reason to think there is a potential order bias."
  },
  {
    "objectID": "latin_square.html#summary",
    "href": "latin_square.html#summary",
    "title": "Latin Square",
    "section": "Summary",
    "text": "Summary\nThe ANOVA model assumptions all appear to be met. We can trust the p-values in the ANOVA summary table. Thus we conclude algorithm has a significant effect on throughput. To gain further insight, pairwise comparisons for the algorithm levels could be run."
  },
  {
    "objectID": "latin_square.html#footnotes",
    "href": "latin_square.html#footnotes",
    "title": "Latin Square",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA carry-over effect occurs when the effect of a treatment applied to block spreads beyond the borders of the block. For example, a fertilizer applied to a particular area is carried by wind or water to some adjacent area that is supposed to be receiving a different treatment.\nIn the case of temporal (rather than spatial) blocks, a carry-over effect occurs when the effect of the previous treatment influences the outcome for a particular subject even after the subject has begun a new phase of the experiment under a different treatment condition.\nCarry over effects also include when effects of repetition (such as learning or fatigue) are mixed with effects of the treatment, and the two become confounded.↩︎\nBailey, R. & Cameron, Peter & Connelly, R.. (2008). Sudoku, Gerechte Designs, Resolutions, Affine Space, Spreads, Reguli, and Hamming Codes. American Mathematical Monthly. 115. 10.1080/00029890.2008.11920542.↩︎\nSince this sequence is randomly generated, running the same code on your machine will give a different sequence each time you run it. You can create the sequence by first running set.seed(6), and then sample(1:4). If sample(1:4) command is run again without resetting the random seed or doing any other random sampling, the result will be the sequence which was obtained for the columns in step 2.↩︎\nSince this sequence is randomly generated, running the same code on your machine will give a different sequence each time you run it. You can create the sequence by first running set.seed(6), and then sample(1:4). If sample(1:4) command is run again without resetting the random seed or doing any other random sampling, the result will be the sequence which was obtained for the columns in step 2.↩︎\nDeciding whether the treatment factor is inside or outside of another factor is a little unusual since the partitions for treatment are not contiguous. However, the definition of outside or inside is still the same. Let’s look at the relationship between treatment at grand mean.\n\nTreatment A fits nicely inside of the Grand Mean partition. The partitions for the other treatment levels also fit in Grand Mean; so Treatment is “inside” of Grand Mean.\nTreatment and the Column Factor can also be investigated. In this case, level A of Treatment does not fit nicely inside of the column factor. In fact, one level of Treatment spans all 4 levels of the column factor. Treatment is certainly not inside of the Column Factor.\n\nWe can also see that the Column Factor partitions do not fit inside of Treatment.\n\nThe factors are crossed since each level of Treatment appears in combination with each level of the Column Factor. A similar process can be carried out to determine that Treatment and the Column Factor are also crossed.↩︎\nThe counting free numbers approach of determining degrees of freedom for residuals may be less clear to apply than it was in other designs due to the non-continuous partitions of the treatment factor. There are 3 structural factors: rows, columns, and treatments. (In the traffic example, that corresponds to intersections, times of day, and timing algorithm respectively). The residuals in each of these partitions must sum to zero.\nFor the treatment, all the residuals for observations from Treatment A must sum to zero. Similarly, the observations from Treatment B will have residuals that sum to zero. And so on for each treatment. It is not obvious in the factor diagram which observation should be chosen as “locked” and which ones we can count as free to vary. But consider for a moment that in any row, each of the treatments appears. We will select a row (let’s use the top row for convenience), and say that each of those residuals is “locked”. This essentially reduces the residual factor from a 4x4 table of free values, to a 3x4 table.\n\nNow apply the fact that in order to sum to zero across rows, the last value in each row is “locked”. Similarly, the sum of the residuals in each column of the remaining 3x4 table of free values must also sum to zero. Therefore, the last (bottom) residual of each column is also “locked”.\n\nThis leads to a general result for finding residual degrees of freedom for a Latin Square: (# rows – 1) x (# columns – 2).↩︎"
  },
  {
    "objectID": "bf3.html#factor-effects",
    "href": "bf3.html#factor-effects",
    "title": "BF[3]",
    "section": "Factor Effects",
    "text": "Factor Effects\nThe first step in performing a decomposition is to calculate the factor level means. The grand mean is 69.1458333.\nThe mean for each factor and its corresponding levels is displayed below.\n\n\nTable 3: Means for Each Factor Level Combination\n\n\n\n\n(a) Mean Talent by Reader Gender\n\n\n\n\n\n\nreader\nmean\n\n\n\n\nfemale\n68.62\n\n\nmale\n69.67\n\n\n\n\n\n\n(b) Mean Talent by Author Gender\n\n\n\n\n\n\nauthor\nmean\n\n\n\n\nfemale\n69.79\n\n\nmale\n68.5\n\n\n\n\n\n\n(c) Mean Talent by Author Attractiveness\n\n\n\n\n\n\nattractiveness\nmean\n\n\n\n\nattractive\n70.88\n\n\nno-pic\n69.25\n\n\nunattractive\n67.31\n\n\n\n\n\n\n\n\n(d) Mean Talent by Reader Gender x Author Gender\n\n\n\n\n\n\n\nreader\nauthor\nmean\n\n\n\n\nfemale\nfemale\n70.08\n\n\nfemale\nmale\n67.17\n\n\nmale\nfemale\n69.5\n\n\nmale\nmale\n69.83\n\n\n\n\n\n\n(e) Mean Talent by Reader Gender x Author Attractiveness\n\n\n\n\n\n\n\nreader\nattractiveness\nmean\n\n\n\n\nfemale\nattractive\n58.62\n\n\nfemale\nno-pic\n68.5\n\n\nfemale\nunattractive\n78.75\n\n\nmale\nattractive\n83.12\n\n\nmale\nno-pic\n70\n\n\nmale\nunattractive\n55.88\n\n\n\n\n\n\n(f) Mean Talent by Author Gender x Author Attractiveness\n\n\n\n\n\n\n\nauthor\nattractiveness\nmean\n\n\n\n\nfemale\nattractive\n68.75\n\n\nfemale\nno-pic\n69.75\n\n\nfemale\nunattractive\n70.88\n\n\nmale\nattractive\n73\n\n\nmale\nno-pic\n68.75\n\n\nmale\nunattractive\n63.75\n\n\n\n\n\n\n\n\n(g) Mean Talent by Reader Gender x Author Gender x Author Attractiveness\n\n\n\n\n\n\n\n\nreader\nauthor\nattractiveness\nmean\n\n\n\n\nfemale\nfemale\nattractive\n50.5\n\n\nfemale\nfemale\nno-pic\n69\n\n\nfemale\nfemale\nunattractive\n90.75\n\n\nfemale\nmale\nattractive\n66.75\n\n\nfemale\nmale\nno-pic\n68\n\n\nfemale\nmale\nunattractive\n66.75\n\n\nmale\nfemale\nattractive\n87\n\n\nmale\nfemale\nno-pic\n70.5\n\n\nmale\nfemale\nunattractive\n51\n\n\nmale\nmale\nattractive\n79.25\n\n\nmale\nmale\nno-pic\n69.5\n\n\nmale\nmale\nunattractive\n60.75\n\n\n\n\n\n\nWe will now compute the factor level effects. A couple of example computations for the interaction factors are shown below. The general rule for factor level effects is implemented, it states that the size of an effect is equal to the factor level mean minus the sum of the effects of all outside factors. The computation for each effect of each factor will not be shown.\nThe full factorial model, as defined in Equation 1, will be used to describe the calculations.\nFirst, let’s compute the effect of a male reader \\(\\times\\) attractive author on perceived talent.\n\\[\\begin{align}\n(\\hat{\\alpha\\gamma})_\\text{male reader, attractive author} &= \\bar{y}_\\text{male reader, attractive author} - (\\hat{\\mu} + \\hat{\\alpha}_\\text{male reader} + \\hat{\\gamma}_\\text{attractive author}) \\\\\n\n\\hat{(\\alpha\\gamma)}_{2, 1} &= \\bar{y}_{2 \\cdot 1 \\cdot} - (y_{\\cdot\\cdot\\cdot\\cdot} + \\hat{\\alpha_2} + \\hat{\\gamma_1}) \\\\\n\n&= 83.12 - (69.15  + 0.52 + 1.73) \\\\\n&= 11.72\n\n\\end{align}\\]\nThus, as we saw earlier in Figure 1, the effect of being attractive is made greater when the reader is a male. Specifically, the mean talent ratings is 11.72 points higher predicted by adding the effect of male reader and the effect of attractive author to the grand mean separately.\nTo calculate the effect of a three-way interaction, the general rule is again applied. This time though, there are more calculations since all two-way interactions are inside of the 3 way interaction. Below, the effect of male reader \\(\\times\\) female author \\(\\times\\) attractive author is calculated.\n\\[\\begin{align}\n\\hat{(\\alpha\\beta\\gamma)}_{2,1,1} &= \\bar{y}_{211\\cdot} - (\\hat{\\mu} + \\hat{\\alpha_2} + \\hat{\\beta_1} + \\hat{\\gamma_1} + \\hat{\\alpha\\beta}_{21} + \\hat{\\alpha\\gamma}_{21} + \\hat{\\beta\\gamma}_{11}) \\\\\n\n&= 87 - (69.15  + 0.52 + .64 + 1.73 + -.81 + 11.72 + -2.77) \\\\\n&= 6.82\n\n\\end{align}\\]\nThus, their appears to be a synergistic effect on perceived talent when all three of the levels male reader \\(\\times\\) female author \\(\\times\\) attractive author are present together. Specifically, the mean talent rating is 6.82 points higher than the partial fit (which only includes main effects and two-way interaction effects) would have predicted.\nSimilar calculations can be done for every factor level effect."
  },
  {
    "objectID": "bf3.html#degrees-of-freedom",
    "href": "bf3.html#degrees-of-freedom",
    "title": "BF[3]",
    "section": "Degrees of Freedom",
    "text": "Degrees of Freedom\nDegrees of freedom can also be calculated using the general rule: degrees of freedom for a factor is equal to the levels of that factor minus the sum of degrees of freedom for all outside factors. The degrees of freedom calculations are only shown for two factors, leaving it up to the reader to calculate/verify the degrees of freedom for the other factors as presented in the ANOVA summary table in XXX.\nThe degrees of freedom calculation for the reader by attractiveness interaction factor is shown below. There are 2 reader gender levels \\(\\times\\) 3 attractiveness levels = 6 total levels for this interaction factor.\n\\[\\begin{align}\ndf_\\text{reader, attractiveness} &= levels_\\text{reader, attractiveness} - (df_{\\mu} + df_\\text{reader} + df_\\text{attractiveness}) \\\\\n\ndf_{\\alpha\\gamma} &= levels_{\\alpha\\gamma} - (df_\\text{grand mean} + df_\\alpha + df_\\gamma \\\\\n\n&= 6 - (1  + 1 + 2) \\\\\n&= 2\n\n\\end{align}\\]\nThere are 2 degrees of freedom for the reader \\(\\times\\) attractiveness factor.\nThe degrees of freedom calculation for the 3 way interaction is shown below.\n\\[\\begin{align}\ndf_\\text{reader, author, attractiveness} &= levels_\\text{reader, author, attractiveness} - (df_{\\mu} + df_\\text{reader} + df_\\text{author} + df_\\text{attractiveness} + df_\\text{reader, author} + df_\\text{reader, attractiveness} + df_\\text{author, attractiveness}) \\\\\n\ndf_{\\alpha\\beta\\gamma} &= levels_{\\alpha\\beta\\gamma} - (df_\\mu + df_\\alpha + df_\\beta + df_\\gamma + df_{\\alpha\\beta} + df_{\\alpha\\gamma} + df_{\\beta\\gamma} \\\\\n\n&= 12 - (1  + 1 + 1 + 2 + 1 + 2 + 2 ) \\\\\n&= 2\n\n\\end{align}\\]\nThe three way interaction also has two degrees of freedom."
  },
  {
    "objectID": "bf3.html#completing-the-anova-table",
    "href": "bf3.html#completing-the-anova-table",
    "title": "BF[3]",
    "section": "Completing the ANOVA Table",
    "text": "Completing the ANOVA Table\nAfter degrees of freedom and effects for each factor are calculated, the remaining pieces of the ANOVA table can also be calculated.\nTo calculate sum of squares for a factor, the effects of each factor level associated with that factor are squared and then multiplied by the number of replicates in that level.1 These products are then summed across levels to get the total sum of squares for the factor.\nTo obtain the mean squares for a factor, the factor’s sum of squares is divided by the factor’s degrees of freedom.\nFinally, the F-statistic is obtained by computing the ratio of a factor’s mean square to the mean squared error. The degrees of freedom associated with this F statistic are the degrees of freedom for the factor and the degrees of freedom for error. The p-value is obtained by finding the area under the F distribution to the right of the F statistic."
  },
  {
    "objectID": "bf3.html#describe-the-data",
    "href": "bf3.html#describe-the-data",
    "title": "BF[3]",
    "section": "Describe the Data",
    "text": "Describe the Data\nWhen working with a dataset you should get to know your data through numerical and graphical summaries. Numerical summaries typically consist of means, standard deviations, and sample sizes for each factor level. Graphical summaries most usually are boxplots, scatterplots, and/or interaction plots with the means displayed.\nInteractive code and additional explanations of numerical summaries and plots in R are found at R Instructions-&gt;Descriptive Summaries section of the book.\nSo far, we have already seen various numerical and graphical summaries of the data on this page. This summaries below would be a good starting point if you are taking a look at the data for the first time.\n\nNumerical Summaries\nA good place to start is calculating summary statistics like mean, standard deviation and sample size for the factor levels of each experimental factor.\n\n\nCode\n#Note: the df data frame was created in the first R chunk in the Overview section of this page\ndf %&gt;% group_by(reader) %&gt;% summarise(mean = mean(talent),\n                                      n = n(),\n                                      std_dev = sd(talent),\n                                      .groups = 'keep') %&gt;% \n  pander(caption = \"Numerical Summary by Reader Gender\")\ndf %&gt;% group_by(author) %&gt;% summarise(mean = mean(talent),\n                                      n = n(),\n                                      std_dev = sd(talent),\n                                      .groups = 'keep') %&gt;% \n  pander(caption = \"Numerical Summary by Author Gender\")\ndf %&gt;% group_by(attractiveness) %&gt;% summarise(mean = mean(talent),\n                                      n = n(),\n                                      std_dev = sd(talent),\n                                      .groups = 'keep') %&gt;% \n  pander(caption = \"Numerical Summary by Attractiveness Gender\")\n\n\n\nTable 4: Numerical Summaries of Experimental Factors\n\n\n\n\n(a) Numerical Summary by Reader Gender\n\n\n\n\n\n\n\n\nreader\nmean\nn\nstd_dev\n\n\n\n\nfemale\n68.62\n24\n12.79\n\n\nmale\n69.67\n24\n13.18\n\n\n\n\n\n\n(b) Numerical Summary by Author Gender\n\n\n\n\n\n\n\n\nauthor\nmean\nn\nstd_dev\n\n\n\n\nfemale\n69.79\n24\n16.67\n\n\nmale\n68.5\n24\n7.684\n\n\n\n\n\n\n(c) Numerical Summary by Attractiveness Gender\n\n\n\n\n\n\n\n\nattractiveness\nmean\nn\nstd_dev\n\n\n\n\nattractive\n70.88\n16\n15.02\n\n\nno-pic\n69.25\n16\n5.31\n\n\nunattractive\n67.31\n16\n16.04\n\n\n\n\n\n\n\nMuch of the above information can also be seen in an effective visual.\n\n\nGraphical Summaries\nBelow are some basic boxplots talent perception, grouped by levels of each experimental factor. Jittered points of the response are overlayed on each boxplot. The effect of attractiveness is detectable, but the impact of reader gender and author gender are not obvious since they exist as part of an interaction. Interaction plots can be investigated at any time in the analysis process. However, sometimes the number of interactions to check can large, and may not be something to start with. In such cases, you may run the model first and then spend energy trying to understand only those interactions that look to be significant.\n\n\nCode\n#Note: the df data frame was created in the first R chunk in the Overview section of this page\n\n# Chart 1\ndf %&gt;% ggplot(aes(x = reader, y = talent)) + \n  geom_boxplot(outlier.shape = NA) +\n  geom_jitter() +\n  labs(title = \"Reader Gender\") +\n  theme(axis.title = element_blank(),\n        plot.title = element_text(hjust = .5))\n\n# Chart 2\ndf %&gt;% ggplot(aes(x = author, y = talent)) + \n  geom_boxplot(outlier.shape = NA) +\n  geom_jitter() +\n  labs(title = \"Author Gender\") +\n  theme(axis.title = element_blank(),\n        plot.title = element_text(hjust = .5))\n\n# Chart 3\ndf %&gt;% ggplot(aes(x = attractiveness, y = talent)) + \n  geom_boxplot(outlier.shape = NA) +\n  geom_jitter() +\n  labs(title = \"Author Attractiveness\") +\n  theme(axis.title = element_blank(),\n        plot.title = element_text(hjust = .5))\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n(c)\n\n\n\n\nFigure 2: Graphical Summaries of Experimental Factors"
  },
  {
    "objectID": "bf3.html#footnotes",
    "href": "bf3.html#footnotes",
    "title": "BF[3]",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis assumes a balanced design. Formulas/calculations for unbalanced designs may need adjustments, depending on the sum of square type that is desired.↩︎\nA marginal mean is a mean calculated in the “margins” (or on the endges) of a two-way table. In other words, it is a mean of a factor level without considering the other factors in the model. Stated another way, it is averaging over all the observations that belong to reader=Male, without breaking out the observations into separate groups to account for the author’s gender.↩︎\nIf it was questionable, Levene’s test may help with the decision of whether the assumption was met or not. In reality though, this is about as nice of a residual plot as one could hope for.↩︎"
  },
  {
    "objectID": "bf2.html",
    "href": "bf2.html",
    "title": "BF[2]",
    "section": "",
    "text": "When researchers want to study the effects of two factors on the same response variable a factorial design can be considered. Factorial experiments involve two or more factors that are crossed.\n\n\n\n\n\n\nTip\n\n\n\nFull factorial crossing occurs when each combination of factor levels is present in the study.\n\n\nCompare a factorial design with the one-at-a-time approach. In a one-at-a-time approach, each factor would be investigated in a separate experiment. Each experiment would evaluate the effect of just one factor on the response.\nFactorial designs are a way to simultaneously study the effects of multiple factors using just one experiment. Factorial designs have a couple of major advantages over one-factor-at-a-time studies.\n\nThey are a more efficient use of our time and material: I can get information about both of my factors from just one observation\nThey allow the random error to be allocated across a greater number of factors, thereby reducing unexplained variance (i.e. mean square error) and increasing the statistical power of the F-test.\nThey allow the estimation of interaction effects. Or in other words, we can observe how one factor’s effect on the response changes for different levels of the other factor.\n\nWe will expand on the simple toothpaste example to illustrate BF[2] concepts. The study is summarized here.\nResearchers wanted to know which of 4 types of toothbrushes was best at reducing plaque: manual (this is the traditional/usual type of brush), oscillating bristles, sonic, and ultrasonic. The response variable was the percent of teeth surface area covered with plaque. Four teeth (first molar in each quadrant of the mouth) were measured on each person to calculate the total percent area covered. Six subjects were assigned to each type of brush.\nResearchers also wanted to study the effect of name brand tooth paste compared to its off brand equivalent. This is the second controlled factor in the experiment. It has two levels (name brand and off brand). Twelve subjects used name brand paste, and a different 12 subjects used the off brand. Toothpaste brand is crossed with toothbrush type to create a BF[2].\n\n\nBased on the description above, the factor structure for this experiment is displayed in Figure 1:\n\n\n\n\nFigure 1: Factor Structure Diagram\n\n\n\nTwo levels of toothpaste multiplied by 4 levels of toothbrush results in 8 factor level combinations total. This is represented by the 8 partitions in the interaction factor. These 8 factor level combinations are obtained by overlaying the 2 controlled factor partitions. When the toothbrush factor and toothpastepaste partitions are overlayed, they cross each other and create new, meaningful partitions: the interaction factor. Since there were 24 subjects and the study is balanced, we end up with \\(24\\div 8 = 3\\) replicates in each factor level combination.\n\n\n\nEach factor (i.e. meaningful partition of the data) in Figure 1 corresponds to a term in Equation 1:\n\\[\ny_\\text{ijk} = \\mu + \\alpha_i + \\beta_j + (\\alpha\\beta)_\\text{ij} + \\epsilon_\\text{ijk}\n\\tag{1}\\]\nWhere\n\n\\(y_{ijk}\\) is the \\(k^{th}\\) observation from the factor level combination of \\(\\alpha_i\\) and \\(\\beta_j\\).\n\\(\\mu\\) is the grand mean of all the observations.\n\\(\\alpha\\) is the effect of toothbrush, and \\(i\\) goes from 1 to 4 since there are 4 toothbrush types\n\\(\\beta\\) is the effect of toothpaste, and \\(j\\) is either 1 or 2 since there are 2 levels (Name brand and off brand).\nThe \\((\\alpha\\beta)_\\text{ij}\\) is called the interaction effect.\n\\(\\epsilon\\) is the residual error term, and \\(k\\) is the replicate count within a factor level combination.\n\nThere are at least three hypotheses to test with this model. A hypothesis for each main effect, and a hypothesis for the interaction effect.\nA hypothesis for the main effect of toothbrush type:\n\\[H_0: \\alpha_\\text{i} = 0 \\text{ for all } i\\]\n\\[H_a: \\alpha_\\text{i} \\ne 0 \\text{ for some } i\\]\nA hypothesis for the main effect of toothpaste brand:\n\\[H_0: \\beta_\\text{j} = 0 \\text{ for all } j\\]\n\\[H_a: \\beta_\\text{j} \\ne 0 \\text{ for some } j\\]\nA hypothesis for the interaction of toothbrush and toothpaste.\n\\[\nH_0: (\\alpha\\beta)_\\text{ij} = 0 \\text{ for all } ij\n\\]\n\\[\nH_a: (\\alpha\\beta)_\\text{ij} \\ne 0 \\text{ for some } ij\n\\]\nWhen the interaction term is not significant a predicted value for an observation can be obtained by simply adding the grand mean to the main effects \\(\\hat{\\alpha}_i\\) and \\(\\hat{\\beta}_j\\). This is equivalent to treating the effect of \\((\\alpha\\beta)_\\text{ij} = 0\\) for all values of \\(i\\) and \\(j\\).\nWhen the interaction effect is significant reject the null hypothesis and accept the alternative hypothesis: at least one factor level combination has a none zero effect.\n\n\n\nA two-way ANOVA model may be used to analyze data from a BF[2] design if the following requirements are satisfied. Note that these requirements are identical to the requirements of a BF[1] one-way ANOVA.\n\n\nThe BF[] designation refers to the design of the experiment. The reference to one- or two-way ANOVA refers to the analysis technique applied to the resulting data.\n\n\n\n\n\n\n\n\nRequirements\nMethod for Checking\nWhat You Hope to See\n\n\n\n\nConstant variance across factor levels\nResidual vs. Fitted Plot\nNo major disparity in vertical spread of point groupings\n\n\n\nLevene’s Test\nFail to reject \\(H_0\\)\n\n\nNormally Distributed Residuals\nNormal Q-Q plot\nStraight line, majority of points in boundaries\n\n\nIndependent residuals\nOrder plot\nNo pattern/trend\n\n\n\nFamiliarity with/critical thinking about the experiment\nNo potential source for bias"
  },
  {
    "objectID": "bf2.html#factor-structure",
    "href": "bf2.html#factor-structure",
    "title": "BF[2]",
    "section": "",
    "text": "Based on the description above, the factor structure for this experiment is displayed in Figure 1:\n\n\n\n\nFigure 1: Factor Structure Diagram\n\n\n\nTwo levels of toothpaste multiplied by 4 levels of toothbrush results in 8 factor level combinations total. This is represented by the 8 partitions in the interaction factor. These 8 factor level combinations are obtained by overlaying the 2 controlled factor partitions. When the toothbrush factor and toothpastepaste partitions are overlayed, they cross each other and create new, meaningful partitions: the interaction factor. Since there were 24 subjects and the study is balanced, we end up with \\(24\\div 8 = 3\\) replicates in each factor level combination."
  },
  {
    "objectID": "bf2.html#hypothesis-and-model",
    "href": "bf2.html#hypothesis-and-model",
    "title": "BF[2]",
    "section": "",
    "text": "Each factor (i.e. meaningful partition of the data) in Figure 1 corresponds to a term in Equation 1:\n\\[\ny_\\text{ijk} = \\mu + \\alpha_i + \\beta_j + (\\alpha\\beta)_\\text{ij} + \\epsilon_\\text{ijk}\n\\tag{1}\\]\nWhere\n\n\\(y_{ijk}\\) is the \\(k^{th}\\) observation from the factor level combination of \\(\\alpha_i\\) and \\(\\beta_j\\).\n\\(\\mu\\) is the grand mean of all the observations.\n\\(\\alpha\\) is the effect of toothbrush, and \\(i\\) goes from 1 to 4 since there are 4 toothbrush types\n\\(\\beta\\) is the effect of toothpaste, and \\(j\\) is either 1 or 2 since there are 2 levels (Name brand and off brand).\nThe \\((\\alpha\\beta)_\\text{ij}\\) is called the interaction effect.\n\\(\\epsilon\\) is the residual error term, and \\(k\\) is the replicate count within a factor level combination.\n\nThere are at least three hypotheses to test with this model. A hypothesis for each main effect, and a hypothesis for the interaction effect.\nA hypothesis for the main effect of toothbrush type:\n\\[H_0: \\alpha_\\text{i} = 0 \\text{ for all } i\\]\n\\[H_a: \\alpha_\\text{i} \\ne 0 \\text{ for some } i\\]\nA hypothesis for the main effect of toothpaste brand:\n\\[H_0: \\beta_\\text{j} = 0 \\text{ for all } j\\]\n\\[H_a: \\beta_\\text{j} \\ne 0 \\text{ for some } j\\]\nA hypothesis for the interaction of toothbrush and toothpaste.\n\\[\nH_0: (\\alpha\\beta)_\\text{ij} = 0 \\text{ for all } ij\n\\]\n\\[\nH_a: (\\alpha\\beta)_\\text{ij} \\ne 0 \\text{ for some } ij\n\\]\nWhen the interaction term is not significant a predicted value for an observation can be obtained by simply adding the grand mean to the main effects \\(\\hat{\\alpha}_i\\) and \\(\\hat{\\beta}_j\\). This is equivalent to treating the effect of \\((\\alpha\\beta)_\\text{ij} = 0\\) for all values of \\(i\\) and \\(j\\).\nWhen the interaction effect is significant reject the null hypothesis and accept the alternative hypothesis: at least one factor level combination has a none zero effect."
  },
  {
    "objectID": "bf2.html#assumptions",
    "href": "bf2.html#assumptions",
    "title": "BF[2]",
    "section": "",
    "text": "A two-way ANOVA model may be used to analyze data from a BF[2] design if the following requirements are satisfied. Note that these requirements are identical to the requirements of a BF[1] one-way ANOVA.\n\n\nThe BF[] designation refers to the design of the experiment. The reference to one- or two-way ANOVA refers to the analysis technique applied to the resulting data.\n\n\n\n\n\n\n\n\nRequirements\nMethod for Checking\nWhat You Hope to See\n\n\n\n\nConstant variance across factor levels\nResidual vs. Fitted Plot\nNo major disparity in vertical spread of point groupings\n\n\n\nLevene’s Test\nFail to reject \\(H_0\\)\n\n\nNormally Distributed Residuals\nNormal Q-Q plot\nStraight line, majority of points in boundaries\n\n\nIndependent residuals\nOrder plot\nNo pattern/trend\n\n\n\nFamiliarity with/critical thinking about the experiment\nNo potential source for bias"
  },
  {
    "objectID": "bf2.html#conceptual-understanding",
    "href": "bf2.html#conceptual-understanding",
    "title": "BF[2]",
    "section": "Conceptual Understanding",
    "text": "Conceptual Understanding\nThe concept behind an interaction should feel quite familiar. It is something we deal with everyday and is very common in science. You may have experienced an interaction effect in something as simple as your daily commute:\n\nConsider a factor to indicate which route you take to work. Route has two levels: using the main roads and using back roads. The time to reach your destination is the response. During rush hour, the main roads are clogged with traffic and result in a longer commute time than taking the back roads. However, in non-rush hour times, the main roads result in a faster commute time. Thus, the effect of taking main roads depends on whether you are traveling during rush hour or not.\n\nThe effect of route was reversed for different levels of rush hour. Not all interactions work this way. Some interactions increase/decrease the magnitude of an effect without completely changing its direction. We can tweak the situation of the commute time example to illustrate this:\n\nDuring non-rush hour periods, on average back roads result in a commute time that is 5 minutes faster than main roads. During rush hour periods however, the benefit of taking back rounds compared to main roads increases to 15 minutes. Thus, the size of the effect of back roads increased (is amplified) for rush hour compared to non-rush hour.\n\nThe above descriptions cover just two possible outcomes for this commute time experiment. It may be helpful to visualize the possible outcome scenarios for this two factor (route and rush hour) study. This can effectively be done with an interaction plot. An interaction plot shows the means for each factor level combination and usually connects the means from the same factor level with a line to help the reader visually group means and detect effects.\nFigure 2 shows four possible outcomes of the traffic study where NO interaction is present. The upper left panel of the plot shows a situation where there are no main effects or interactions apparent. The mean is the same regardless of the factor level combination. The upper right panel of the plot shows a large route effect but no effect due to rush hour. This can be seen because the commute time for back roads is high but commute time for main roads is low; however, for a given route there is no difference in the mean for rush hour vs. not rush hour.\nThe bottom left panel shows a non-zero effect for the rush hour factor, as seen by the sizable difference between the levels of rush hour within a route. However, the flat lines indicate that the mean commute time for route is not changing and therefore route has no effect on commute time. Lastly, the bottom right panel is a situation where both main effects appear to be present - but there is still no interaction apparent.\n\n\nR code instructions to create interaction plots are at the bottom of the R Instructions&gt;Descriptive Summaries page.\n\n\n\n\n\nFigure 2: Scenarios with NO Interaction Present\n\n\n\n\nThe line segments within each graph of Figure 2 are parallel (or coincide), which is a visual indicator that no interaction is present.\n\n\n\n\n\n\nTip\n\n\n\nFactors with no interaction will have (nearly) parallel line segments in the interaction plot.\n\n\nSo what does an interaction plot look like when there is an interaction present? The key things to notice is that the line segments in the plot are not parallel. Figure 3 contains 3 examples of interaction plots that show the presence of an potential interaction.\n\n\n\n\n\nFigure 3: Scenarios Indicative of an Interaction\n\n\n\n\nPanel A of Figure 3 illustrates an example where the effect of Route reverses, depending on the value for Rush Hour. In Panel B, the effect of Rush Hour is much greater when using main roads than for back roads. In Panel C, main roads take longer regardless of time of day, but the effect of switching from back roads to main roads is much larger during rush hour than in non-rush hour times.\nThere are a few key points to remember when working with interactions.\nFirst, exercise caution when interpreting a main effect if an interaction is present. The definition of an interaction is that a factor level’s effect changes for different values of the other factor. Therefore, it does not make sense to interpret the hypothesis test of a controlled factor if it is part of a significant interaction. Instead, get in the habit of describing the nature of the interaction.\nTo illustrate the danger of interpreting main effect hypothesis tests when the interaction is significant consider Panel A of Figure 3. If the hypothesis test for Route had a large p-value, it is tempting to say there is insufficient evidence that Route has an effect on commute time. However, the interaction plot shows quite the opposite. Route has an important effect on the response, since the level of Route drastically changes the impact of Rush Hour on commute times. Even though the mean commute time for “back” and “main” may be similar in this scenario, Route is indirectly having an effect on commute time through its interaction with Rush Hour.\nConversely, imagine a scenario where Route’s main effect had a small p-value and the Route - Rush Hour interaction was also significant. If the interaction is like that depicted in Panel C of Figure 3, simply stating that Route is a significant factor does not tell the whole story. The effect of Route in during rush hour is large and may be significant (steep blue line), but the effect of Route in non-rush hour times may not be large enough to reach significance (the nearly flat red line).\n\n\n\n\n\n\nInterpreting Main Effects and Interactions\n\n\n\nWhen a significant interaction is present, do not interpret the hypothesis tests of its main effects without providing additional information.\n\n\nSecond, don’t rely on interaction plots alone to detect the presence/absence of interactions. Though interaction plots are a a helpful tool, they do not adequately show the repsonse variability in each factor level combination. In other words, even when line segments are not parallel a hypothesis test is still needed to determine if an interaction is real or just due to random error. Furthermore, two lines may look nearly parallel but could actually represent a significant interaction.\nLastly, beware of a common mistake that students make. Students commonly state an interaction means that the level of one factor affects the values of another factor. This is a lie from Satan! The key misunderstanding here is thinking that the value of one factor affects the other factor. In reality, it is the factor’s effect on the response that changes for different levels of the other factor. The two factors do not affect each other."
  },
  {
    "objectID": "bf2.html#factor-effects",
    "href": "bf2.html#factor-effects",
    "title": "BF[2]",
    "section": "Factor Effects",
    "text": "Factor Effects\nHow can an interaction affect be estimated? Like all factor effects, its estimate is calculated using the general rule.\n\n\n\n\n\n\nGeneral Rule for Calculating Factor Level Effect\n\n\n\nFactor level effect = mean of the factor level - Sum(effects of all outside factors)\n\n\nFrom the general rule we can see that before we estimate the interaction effect we first need to estimate the outside factor effects (i.e. main effects). Factor level means need to be calculated in order to calculate estimated effects.\nIt is also important to recall that the grand mean factor is outside of all other factors, while the residual error factor is inside of all other factors.\n\nFactor Level Means\nFigure 6 shows our data set with partition lines for structural factors in place. We will now proceed to calculate the factor level means for each factor.\n\n\n\nFigure 6: Full data set with partitions\n\n\nThe grand mean is the mean of all the observations:\n\\[\n\\hat{\\mu} = \\bar{y}_\\cdots = \\frac{19.12 + 18.56 + 25.58 + 24.39 + 24.21 + ... + 23.42}{24} = 22.76\n\\]\nNow find the mean for each level of toothbrush type.\n\\[\n\\bar{y}_\\text{manual} = \\bar{y}_{1\\cdot\\cdot} = \\frac{19.12 + 24.21 + 26.88 + 21.6 + 23.4 + 23.35}{6} = 23.10\n\\]\n\\[\n\\bar{y}_\\text{oscillating} = \\bar{y}_{2\\cdot\\cdot} = \\frac{18.56 + 20.00 + 19.87 + 22.09 + 17.62 + 21.72}{6} = 19.98\n\\]\n\\[\n\\bar{y}_\\text{sonic} = \\bar{y}_{3\\cdot\\cdot} = \\frac{25.58 + 23.31 + 18.99 + 23.09 + 23.81 + 21.27}{6} = 22.68\n\\]\n\\[\n\\bar{y}_\\text{ultrasonic} = \\bar{y}_{4\\cdot\\cdot} = \\frac{24.39 + 21.45 + 32.74 + 24.21 + 25.67 + 23.42}{6} = 25.31\n\\]\nNow find the mean for each level of toothpaste brand.\n\\[\n\\bar{y}_\\text{name brand} = \\bar{y}_{\\cdot 1 \\cdot} = \\frac{19.12 + 18.56 + \\cdots + 18.99 + 32.74}{12} = 22.93\n\\]\n\\[\n\\bar{y}_\\text{off brand} = \\bar{y}_{\\cdot 2 \\cdot} = \\frac{21.60 + 22.09 + \\cdots + 21.27 + 23.42}{12} = 22.60\n\\]\nThere are 8 different combinations of toothbrush type and toothpaste, so the interaction factor has 8 levels total. We calculate a mean for each one, but will only show the calculation for the first 3.\n\\[\n\\bar{y}_\\text{manual and name brand} = \\bar{y}_{11\\cdot} = \\frac{19.12 + 24.21 + 26.88}{3} = 23.40\n\\]\n\\[\n\\bar{y}_\\text{manual and off brand} = \\bar{y}_{12\\cdot} = \\frac{21.60 + 23.40 + 23.35}{3} = 22.78\n\\]\n\\[\n\\bar{y}_\\text{oscillating and name brand} = \\bar{y}_{21\\cdot} = \\frac{18.56 + 20.00 + 19.87}{3} = 19.48\n\\]\nThe means for the residual error factor levels is the observed value itself since there is just 1 observation per level. Therefore, there are no calculations to show.\nFigure 7 displays all the factor level means inside the factor structure.\n\n\n\n\nFigure 7: Factor level means\n\n\n\n\n\nFactor Level Effects\n\nGrand mean effect\nNow that we have calculated means for each level of each factor, we can move on to calculate the effects of the factor levels.1\nFor the grand mean, there is only one level and there are no outside factors. Therefore, the effect due to grand mean is 22.76 (equivalent to its mean) and this affect is applied to all 24 observations.\n\n\nToothbrush effects\nThe toothbrush factor has four levels: one for each brush type. We will use the general rule for calculating factor level effects. To calculate the effect of a toothbrush, take the toothbrush mean and subtract it from the grand mean factor’s effect. For the manual brush, this looks like:\n\\[\n23.09 - 22.76 = 0.33\n\\]\nUsing the manual brush has the effect of increasing a person’s plaque area percentage by 0.33 percentage points on average compared to the grand mean. In a similar way2 you can find the effect for the oscillating brush \\(19.98 - 22.76 = -2.79\\). This means the amount of plaque decreased by 2.79 on average with this brush compared to the grand mean. For a sonic toothbrush, the effect is \\(22.68 - 22.76 = -0.09\\). For an ultrasonic brush the effect is \\(25.31 - 22.76 = 2.55\\).\n\n\nIt is interesting to note that the factor effects for brush type are the same, whether toothpaste brand is included in the analysis or not.\n\n\nToothpaste effects\nCalculating the effects for the second controlled factor in the experiment follows a similar pattern and also uses the general rule for calculating effect sizes. Remember that toothbrush is not outside or inside of toothpaste, rather the two factors are crossed. To calculate the effect of using the name brand toothpaste, take the name brand mean and subtract it from the grand mean factor’s effect:\n\\[\n22.93 - 22.76 = 0.16\n\\]\nA similar calculation is performed for off the brand toothpaste.\n\\[\n22.60 - 22.76 = -0.16\n\\]\nWith only two levels, it becomes obvious that the effects of a factor’s levels will always sum to zero. You may want to go back to the toothbrush level effects and verify this is true.\n\n\nInteraction effects\nThe general rule says that effects of outside factors must be subtracted from the factor level mean. We pause to review the relationship of the other factors to the interaction factor to determine if they are inside, outside, or crossed with each other.\n\n\n\n\n\n\n\n(a) Interaction inside of brush\n\n\n\n\n\n\n\n(b) Interaction inside of toothpaste\n\n\n\n\nFigure 8: Interaction Effects\n\n\nIn Figure 8 (a) you can see that each level of the interaction will fit nicely within a level of toothbrush, this means toothbrush is outside of interaction (equivalently, interaction is inside of toothbrush). The same holds true for the relationship of toothpaste brand and interaction, as shown in Figure 8 (b).\nTherefore, to calculate the interaction effect for using an ultrasonic brush with name brand toothpaste we will subtract the effects of the grand mean factor, ultrasonic brush, and name brand paste from the “ultrasonic, name brand” level mean.\n\\[\n26.19 - (22.76 + 2.55 + 0.16) = .15\n\\]\nLet’s take a deeper look to understand why this works. It can be helpful to remember our assembly line analogy. We will walk through this assembly line, showing a graph to illustrate how the effects are added at each station.\nAn observation from the “ultrasonic brush, name brand paste” group starts with the grand mean value of 22.76. The observation belongs to the ultrasonic group, where plaque tends to be higher, specifically 2.55 higher on average (2.55 is the effect of ultrasonic). Figure 9 show the starting point of the grand mean and the addition of the brush effect.\n\n\n\n\n\nFigure 9: Grand mean + brush effect\n\n\n\n\nAt the next step, because the observation belongs to the Name Brand Toothpaste group we would tack on an additional 0.16 of plaque coverage, as shown in Figure 10.\n\n\n\n\n\nFigure 10: Grand mean + brush effect + paste effect\n\n\n\n\nWhat we haven’t accounted for yet is the fact that the ultrasonic brush and name brand toothpaste have appeared together. If the interaction is significant then we can expect a synergistic effect (in either direction) and will need to add/subtract more to the response. Otherwise, the interaction effects will be small (relative to the error variance).\nThere is an old saying, “the whole is greater than the sum of its parts”. In a way, the interaction effect is the measure of how much greater. Figure 11 shows that after summing the main effects of each controlled factor, the remaining distance to the mean of the factor level combination is the interaction effect. We calculated this above using the general rule and found the distance to be 0.15.\n\n\n\n\n\nFigure 11: Interaction effect is remaining distance to factor level mean\n\n\n\n\nThe previous 3 figures are placed side by side below in the figure below so that the additive progression of the effects can more easily be seen.\n\n\n\n\n\n\n\n\n(a) ?(caption)\n\n\n\n\n\n\n\n(b) ?(caption)\n\n\n\n\n\n\n\n(c) ?(caption)\n\n\n\n\nFigure 12: Additive Effects\n\n\nAdditive Effects\n\nLastly, the residuals (or residual effects) need to be calculated. The mean for each level of residual is simply the observation itself. Effects associated with that observation’s factor levels are subtracted from the observed value. Whatever is left over is considered the residual. In other words, we have applied the general rule for calculating effect size. For the residual factor, the effect can concisely be stated as “observed value - predicted value”.\nAs an example, the residual in the top left corner of the residual factor was obtained with this calculation:\n\\[\n19.12 - (22.76 + 0.329 + 0.16 + 0.15) = -4.28\n\\]\nAll other residuals were similarly obtained. Ultimately, Figure 13 displays all the factor level effects that are summed to obtain each observation.\n\n\n\n\nFigure 13: Factor level effects"
  },
  {
    "objectID": "bf2.html#degrees-of-freedom",
    "href": "bf2.html#degrees-of-freedom",
    "title": "BF[2]",
    "section": "Degrees of Freedom",
    "text": "Degrees of Freedom\nWe can use our understanding of inside vs. outside factors to determine the degrees of freedom (df) for the grand mean, treatment factors, interaction and residual errors. We start with 24 observations - or pieces of information. In other words, we have 24 degrees of freedom that need to be allocated to the factors.\n\n\n\n\n\n\nGeneral Rule for Degrees of Freedom\n\n\n\ndf\\(_\\text{factor}\\) = Total levels of a factor minus the sum of the df of all outside factors\nAn alternative way to find degrees of freedom is to count the number of unique pieces of information in a factor.\n\n\nIn the toothbrush and toothpaste example, grand mean has one level and there are no factors outside of grand mean. Therefore, its degrees of freedom equals one. This will always be the case.\nRemember, the degrees of freedom represent the number of unique pieces of information contributing to the estimation of the effects for that factor. In this case, as soon as you estimate the grand mean for just one of the observations, you know it for all the observations. In other words, only 1 value was free to vary. As soon as it was known all the other values for the grand mean effect were also known. Therefore, there is just one unique piece of information in the grand mean factor. Grand mean has just 1 degree of freedom.\nIn this case there are two controlled factors, or treatment factors: toothbrush and toothpaste. For toothbrush there are four levels of the factor. Grand mean is the only factor outside of toothbrush. Take the number of levels for toothbrush (4) and subtract the degrees of freedom for grand mean (1), which yields 4-1 = 3 degrees of freedom.\n\n\nThe degrees of freedom for toothbrush is it the same here as it was for the BF[1].\nWe could just as easily have used the other approach to finding the degrees of freedom: counting the unique pieces of information. Upon examining the factor for toothbrush in Figure 13 you can see there are 4 unique numbers. We know the effects for toothbrush must sum to zero, so the 4th effect is not free to vary. As soon as I know the effect for 3 of the brushes, I can fill in all the effects for the toothbrush factor.\nA similar approach is taken for toothpaste. Here it is even more obvious that the effects for toothpaste sum to zero. After estimating the toothbrush effect for one observation, I can fill in the toothpaste effects for all the other observations. Therefore, the degrees of freedom for toothpaste is 1.\nUsing the general rule, I know there are 2 levels for toothpaste and grand mean is the only outside factor. Since grand mean has 1 degree of freedom, I get \\(2-1 = 1\\) degree of freedom for toothpaste.\nNow we must calculate degrees of freedom for the interaction term. Take a closer look at the interaction effects in Figure 13. You can see that the numbers repeat within each cell. There are 8 cells total. You can also see that the effects inside a column of values sum to zero, as do the values in a row. Therefore, I really only need to know a value in 3 of the cells of the interaction factor before I can fill in the effects for all the other cells in that factor.\nThis is in perfect harmony with an application of the general rule. The interaction factor has 8 factor levels. Factors outside of the interaction include: grand mean (1 df), toothbrush (3 df), and toothpaste (1 df). Applying the general rule with these values yields \\(8 - (1 + 3 + 1) = 3\\) degrees of freedom for the interaction factor.\nPerhaps the easiest way to find the degrees of freedom for an interaction that is created by crossing two other factors is to multiply the degrees of freedom of the two other factors. In this case, toothbrush and toothpaste are crossed, so you would get \\(3*1 = 3\\), which matches the answer found using other methods.\nFinally, the residual degrees of freedom can be found using the general rule. Since the residual error factor is inside of all other factors, this is the same as finding how many degrees of freedom are leftover after calculating degrees of freedom for all other factors. In this example, there were 24 observations total, so we subtract the degrees of freedom for the other factors from 24. This returns \\(24 - (1+3+1+3) = 16\\) degrees of freedom for residuals.\nThe other approach to finding the degrees of freedom for residuals is to group the residuals by the smallest structural factor partitions (in this case the interaction). Inside each of those partitions the residuals sum to zero. For example, applying the interaction partition to residuals gives the values \\(-4.28\\), \\(0.81\\), and \\(3.48\\). Since we know the 3 residuals sum to zero in each partition, we only need to know 2 values per partition in order to fill in the third residual effect. With 8 partitions applied to the residuals, we have \\(2x8 = 16\\) degrees of freedom for residual error."
  },
  {
    "objectID": "bf2.html#completing-the-anova-table",
    "href": "bf2.html#completing-the-anova-table",
    "title": "BF[2]",
    "section": "Completing the ANOVA Table",
    "text": "Completing the ANOVA Table\nNow that we have calculated degrees of freedom and effects for each factor , we can calculate the remaining pieces of the ANOVA table: Sum of Squares (SS), Mean Squares (MS), F-statistic and p-value. A completed ANOVA summary table contains the information we need for a hypothesis test of the main effects (for controlled factors) and their interaction.\nIn an ANOVA table, each factor and their associated degrees of freedom are listed on the left. Note: the total degrees of freedom are the total number of observations.\n\n\n\n\n\nSource\ndf\nSS\nMS\nFvalue\npvalue\n\n\n\n\nGrand Mean\n1\n\n\n\n\n\n\nBrush\n3\n\n\n\n\n\n\nToothpaste\n1\n\n\n\n\n\n\nBrush:Toothpaste\n3\n\n\n\n\n\n\nResidual Error\n16\n\n\n\n\n\n\nTotal\n24\n\n\n\n\n\n\n\n\n\n\n\nTo get the sum of squares (SS) of a factor, each value displayed in that particular factor first needs to be squared. Figure 13 shows the effects, while Figure 14 shows the squared effects.\n\n\n\n\n\n\n\n\nFigure 14: Squared factor level effects\n\n\nThen, for each factor, all the squared values are summed up to get the sum of squares. The total sum of squares is obtained by summing the squared observations as shown in Equation 3. This represents the total variability in the dataset that will then be allocated or partitioned to the various factors, starting with the grand mean.\n\\[\nSS_\\text{total} = 365.6 + 344.5 + 654.3 + ... + 452.4 + 548.2  = 12,674.30\n\\tag{3}\\]\nFor grand mean, the squared effect of 518.2 is listed 24 times, once for each observation. Summing the squared effects gets:\n\\[\nSS_\\text{Grand Mean} = 518.2* 24 = 12,437.43\n\\]\n\n\n\n\n\n\nNote\n\n\n\nThe rounded numbers are displayed throughout this section, but all calculations are done using the unrounded numbers.\n\n\nThe toothbrush factor has four different effects: one for each level of the factor. For each effect, the squared value is multiplied by the number of observations within the level of the factor. Then, the results are added across all levels to get the sum of squares due to toothbrush.\n\\[\nSS_\\text{toothbrush} = 6*(0.11) + 6*(7.77) + 6*(0.01) + 6*(6.50) = 86.31\n\\tag{4}\\] \nThe sum of squares is similarly calculated for toothpaste brand (Equation 5) and brush by paste interaction (Equation 4).\n\\[\nSS_\\text{toothpaste} = 12*(0.03) + 12*(0.03) = 0.62\n\\tag{5}\\]\n\\[\nSS_\\text{brush x paste} = 3*(0.02) + 3*(0.44) + 3*(0.04) + 3*(0.52) +3*(0.02) + 3*(0.44) + 3*(0.04) + 3*(0.52)  = 6.12\n\\]\nThe effect for the residual error factor has 24 unique values. The squared residuals are summed together in Equation 6.\n\\[\nSS_\\text{residual} = 18.35 + 0.84 + 8.72 + ... + 2.11 + 1.03  = 143.82\n\\tag{6}\\]\nYou can check that the sums of squares (SS) has been allocated to the factors correctly by adding up the SS for each factor and verifying that it also equals the result found in Equation 3.\nPutting this information into the ANOVA table gets us the result shown in Table 1.\n\n\n\n\nTable 1: Sums of squares\n\n\nSource\ndf\nSS\nMS\nFvalue\npvalue\n\n\n\n\nGrand Mean\n1\n12437.43\n\n\n\n\n\nBrush\n3\n86.31\n\n\n\n\n\nToothpaste\n1\n0.62\n\n\n\n\n\nBrush:Toothpaste\n3\n6.12\n\n\n\n\n\nResidual Error\n16\n143.82\n\n\n\n\n\nTotal\n24\n12674.30\n\n\n\n\n\n\n\n\n\n\n\nRecall that SS is a measure of total variability. Of the three structural factors (brush, toothpaste, and their interaction) it is clear to see that brush is contributing the most variability. Some of the difference in SS may be due to difference in number of levels for each factor. Adding levels to a factor allows more variability to be attributed to that factor. We will convert this total variability (sum of squares) into a mean variability (mean square) measure to properly account for differences in number of factor levels. This allows us to compare the factors’ variability on a standardized scale.\nTo calculate a mean square (MS), simply divide SS by degrees of freedom for a factor. The mean square calculations are:\n\\[\nMS_\\text{Grand Mean} = \\frac{SS_\\text{Grand Mean}}{df_\\text{Grand Mean}} = \\frac{12437.43}{1} = 12437.43\n\\]\n\n\\[\nMS_\\text{Brush} = \\frac{SS_\\text{Brush}}{df_\\text{Brush}} = \\frac{86.31}{3} = 28.77\n\\]\n\n\\[\nMS_\\text{Toothpaste} = \\frac{SS_\\text{Toothpaste}}{df_\\text{Toothpaste}} = \\frac{0.62}{1} = 0.62\n\\]\n\n\\[\nMS_\\text{Brush:Toothpaste} = \\frac{SS_\\text{Brush:Toothpaste}}{df_\\text{Brush:Toothpaste}} = \\frac{6.12}{3} = 2.04\n\\]\n\n\\[\nMS_\\text{Residual Error} = \\frac{SS_\\text{Residual Error}}{df_\\text{Residual Error}} = \\frac{143.82}{16} = 8.99\n\\]\n\n\n\n\n\n\nSource\ndf\nSS\nMS\nFvalue\npvalue\n\n\n\n\nGrand Mean\n1\n12437.43\n12437.43\n\n\n\n\nBrush\n3\n86.31\n28.77\n\n\n\n\nToothpaste\n1\n0.62\n0.62\n\n\n\n\nBrush:Toothpaste\n3\n6.12\n2.04\n\n\n\n\nResidual Error\n16\n143.82\n8.99\n\n\n\n\nTotal\n24\n12674.30\n\n\n\n\n\n\n\n\n\n\nFor each structural factor in the design there are a set of hypothesis we want to test using the F statistic.\nSpecifically, we will want to test whether toothbrush type has an effect on plaque coverage, whether toothpaste brand has an effect on plaque coverage, and whether the interaction between brush and paste has an effect on plaque coverage.\nMore specifically, for each factor we test whether the factor level effects are all equal to zero. We can express the hypotheses mathematically using the terms of Equation 2.\nA hypothesis for the main effect of toothbrush type:\n\\[H_0: \\alpha_\\text{i} = 0 \\text{ for all } i\\]\n\\[H_a: \\alpha_\\text{i} \\ne 0 \\text{ for some } i\\]\nA hypothesis for the main effect of toothpaste brand:\n\\[H_0: \\beta_\\text{j} = 0 \\text{ for all } j\\]\n\\[H_a: \\beta_\\text{j} \\ne 0 \\text{ for some } j\\]\nA hypothesis for the interaction of toothbrush and toothpaste.\n\\[\nH_0: (\\alpha\\beta)_\\text{ij} = 0 \\text{ for all } ij\n\\]\n\\[\nH_a: (\\alpha\\beta)_\\text{ij} \\ne 0 \\text{ for some } ij\n\\]\nTo test these hypotheses we need to compare the mean square (MS) for a factor to the mean square for residual error (abbreviated as MSE). The MSE is the estimate of unexplained, random error. If a factor’s MS is similar in size to the MSE, the variance in that factor may just be random error; and the effect of the factor levels are zero. On the other hand, if the variability in the factor, as measured by its MS, is much larger than the random error observed in the experiment (represented by MSE), then it is reasonable to believe the factor levels have a non-trivial contribution to the variability. In other words, the factor has a significant effect on the response.\nThe F statistic is a ratio of these two errors and is obtained by dividing the factor’s mean square (MS) by the MSE. The F statistic calculations are\n\n\\(F_\\text{brush} = 28.77/8.99 = 3.20\\)\n\\(F_\\text{toothpaste} = 0.62/8.99 = 0.07\\)\n\\(F_\\text{brush:toothpaste} = 2.04 / 8.99 = .23\\)\n\nThis F statistic follows a well defined distribution, called the F distribution. The F distribution is defined by two values for degrees of freedom3:\n\nthe numerator degrees of freedom, which is the degrees of freedom for the factor being tested\nthe denominator degrees of freedom, which is the degrees of freedom for residual error\n\nThe area under this distribution curve to the right of our F statistic is called the p-value. The p-value represents the probability of getting an F statistic at least as large as the one obtained, assuming the null hypothesis (of no effect) is true.\nThe F statistic, numerator degrees of freedom, and denominator degrees of freedom are the 3 required inputs to calculate a p-value. P-values can be obtained manually using the applet mentioned above, the f.dist.rt()4 function in excel or the pf() function in R5. Usually though, R will show p-values as part of the standard output of the ANOVA model/table and it is recommended you stick to those values when reporting answers.\nThe completed ANOVA table for this BF[2] toothbrush and toothpaste example is shown in\n\n\n\n\n\nSource\ndf\nSS\nMS\nFvalue\npvalue\n\n\n\n\nGrand Mean\n1\n12437.43\n12437.43\n\n\n\n\nBrush\n3\n86.31\n28.77\n3.20\n0.051\n\n\nToothpaste\n1\n0.62\n0.62\n0.07\n0.800\n\n\nBrush:Toothpaste\n3\n6.12\n2.04\n0.23\n0.876\n\n\nResidual Error\n16\n143.82\n8.99\n\n\n\n\nTotal\n24\n12674.30\n\n\n\n\n\n\n\n\n\n\nBecause the p-value for the interaction (p-value = 0.876) is much higher than any traditional level of significance threshold we might have chosen (0.01, 0.05, or 0.1), we fail to reject the null hypothesis. There is insufficient evidence to say the interaction effect has an effect on plaque coverage. Because the interaction is NOT significant, we can proceed to interpret the main effects test results.\n\n\n\n\n\n\nSignificant Interaction Effects\n\n\n\nIf the interaction effect is significant, great caution should be taken when interpreting the hypothesis test results for the factors involved in the interaction; it may not be valid to interpret the hypothesis test results. Instead, if the interaction is significant, it is better to describe the nature of the interaction with graphs, numbers, and words.\n\n\nToothpaste’s p-value is high (p-value = 0.8), indicating no evidence that toothpaste brand has an effect on plaque coverage. The p-value for brush is marginally significant (p-value = 0.051). When (in)significance is borderline, rather than making bold statements based on a small amount of (in)significance, it is helpful to dig a little deeper. Consider things like sample size, effect size (practical significance), outliers, and how closely assumptions are met. After weighing those considerations carefully, take a stance and state your belief about the role of the factor on the response. Explain your rationale, then keep an open mind and stay curious."
  },
  {
    "objectID": "bf2.html#describe-the-data",
    "href": "bf2.html#describe-the-data",
    "title": "BF[2]",
    "section": "Describe the Data",
    "text": "Describe the Data\nWhen working with a dataset the first thing to do is get to know your data through numerical and graphical summaries. Numerical summaries typically consist of means, standard deviations, and sample sizes for each factor level. Graphical summaries most usually are boxplots, scatterplots, and/or interaction plots with the means displayed.\nInteractive code and additional explanations of numerical summaries and plots in R are found at R Instructions-&gt;Descriptive Summaries section of the book.\n\nNumerical Summaries\nAfter loading required packages, we will read in the data and do some wrangling.\n\n\nCode\nbf2 &lt;- read_csv(\"data/toothpaste_BF2.csv\") \n\n\nWe then calculate summary statistics for each factor level separately.\n\n\nCode\n#Descriptive stats for levels of Brush\nfavstats(Plaque~Brush, data = bf2) |&gt; \n  kable(digits = 2) |&gt; \n  kable_styling(full_width = TRUE)\n#Descriptive stats for levels of Toothpaste\nfavstats(Plaque~Toothpaste, data = bf2) |&gt; \n  kable(digits = 2) |&gt; \n  kable_styling(full_width = TRUE)\n\n\n\nTable 2: Numerical Summary for Each Factor\n\n\n\n\n(a) Brush\n\n\nBrush\nmin\nQ1\nmedian\nQ3\nmax\nmean\nsd\nn\nmissing\n\n\n\n\nManual\n19.12\n22.04\n23.38\n24.01\n26.88\n23.09\n2.60\n6\n0\n\n\nOscillating\n17.62\n18.89\n19.94\n21.29\n22.09\n19.98\n1.74\n6\n0\n\n\nSonic\n18.99\n21.73\n23.20\n23.68\n25.58\n22.67\n2.27\n6\n0\n\n\nUltrasonic\n21.45\n23.62\n24.30\n25.35\n32.74\n25.31\n3.90\n6\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Toothpaste\n\n\nToothpaste\nmin\nQ1\nmedian\nQ3\nmax\nmean\nsd\nn\nmissing\n\n\n\n\nNameBrand\n18.56\n19.68\n22.38\n24.69\n32.74\n22.92\n4.18\n12\n0\n\n\nOffBrand\n17.62\n21.69\n23.22\n23.52\n25.67\n22.60\n2.00\n12\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn Table 2 (a) we see that the oscillating brush has lowest mean plaque (19.98). Table 2 (b) shows the mean plaque for the two types of toothpaste is very close, 22.92 for Name Brand and 22.60 for Off Brand.\nYou can also look at descriptive statistics for factor level combinations. This is only advisable if there is sufficient sample size at each combination and the number of combinations is manageable. Table 3 shows what that looks like for our current example. Due to the many combinations, it is a bit difficult to interpret the output. Furthermore, using a 5 number summary on a sample size of 3 observations is not advisable so some of the summary statistics have been dropped.\nThe combination of Oscillating with Name Brand has the lowest mean (19.48) and the lowest standard deviation (0.80).\n\n\nCode\nfavstats(Plaque~Brush+Toothpaste, data = bf2) |&gt; \n  select(Brush.Toothpaste, mean, sd, n) |&gt; \n  kable(digits = 2) |&gt; \n  kable_styling(full_width = FALSE)\n\n\n\n\nTable 3: Numerical Summary\n\n\nBrush.Toothpaste\nmean\nsd\nn\n\n\n\n\nManual.NameBrand\n23.40\n3.94\n3\n\n\nOscillating.NameBrand\n19.48\n0.80\n3\n\n\nSonic.NameBrand\n22.63\n3.35\n3\n\n\nUltrasonic.NameBrand\n26.19\n5.86\n3\n\n\nManual.OffBrand\n22.78\n1.03\n3\n\n\nOscillating.OffBrand\n20.48\n2.48\n3\n\n\nSonic.OffBrand\n22.72\n1.31\n3\n\n\nUltrasonic.OffBrand\n24.43\n1.14\n3\n\n\n\n\n\n\n\n\n\n\nGraphical Summaries\nGraphs are also valuable tools to help you get to know your data. Since there are only 3 observation in each factor level combination, a dotplot/scatterplot is more appropriate than a boxplot. This is shown in Figure 15\n\n\nCode\n#Note, I have to turn Brush into a factor because it started out as a character variable\ndotplot(Plaque~factor(Brush)|factor(Toothpaste), data = bf2, \n        xlab = \"\")\n\n\n\n\n\nFigure 15: Graphical Summary\n\n\n\n\nYou could also gain insight about the main effects by cutting the data by each factor separately, as shown in Figure 16.\n\n\n\n\n\n\n\n(a) Brush\n\n\n\n\n\n\n\n(b) Toothpaste\n\n\n\n\nFigure 16: Data cut by one factor at a time\n\n\nFinally, an interaction plot is ideal for investigating a BF[2]. The plot only shows factor level means. Therefore, it does not give a good sense of the underlying distribution of data and should not be used as the only visual assessment of your data. However, it is extremely good at providing insight into possible interactions.\nIn Figure 17 the lines are not parallel. In fact, they cross twice. Though this seems to suggest an interaction is present, our ability to claim an interaction exists will depend on the results of a hypothesis test. The hypothesis test is greatly influenced by sample size and the variability at each of the factor level means presented in the plot.\n\n\n\n\n\nFigure 17: Interaction plot for Brush vs. Toothpaste"
  },
  {
    "objectID": "bf2.html#create-the-model",
    "href": "bf2.html#create-the-model",
    "title": "BF[2]",
    "section": "Create the Model",
    "text": "Create the Model\nYou then create the model using the aov() function. To see results of the F-test you can feed your model into a summary() or anova() function.\n\n\nsummary() and anova() functions give the same output with some slight differences in formatting when called on a model created with aov(). The summary() function is more general; it can also take linear regression models created with 'lm() as inputs. The output provided by summary() will change based on the type of object it is called on. This is discussed in more detail at the bottom of the Unbalanced page.\n\nmyaov &lt;- aov(Y ~ X1*X2, data=YourDataSet)\nsummary(myaov)\n\n\nmyaov is the user defined name in which the results of the aov() model are stored\nY is the name of a numeric variable in your dataset which represents the quantitative response variable.\nX1 and X2 are names of qualitative variables in your dataset. They should have class(X) equal to factor or character. If that is not the case, use factor(X) inside the aov(Y ~ factor(X1)*...) command.\nYourDataSet is the name of your data set.\n\nThe * in the code above is a shortcut for writing out the whole model. It can be read as, “include each term by itself, and all possible interaction terms”. The long way of writing out the model uses a colon, :, to define interaction terms and is shown below. When writing it this way each term must be explicitly stated.\n\nmyaov &lt;- aov(Y ~ X1+X2+X1:X2, data=YourDataSet)\nsummary(myaov)\n\nBelow are the results for the full BF[2] model using the toothbrush and toothpaste example. You should notice that these results match what we got when we performed the decomposition manually to build the ANOVA summary table. Even though the interaction plot showed crossing lines, we see the interaction between Brush and Toothpaste is not significant (p-value = .8763). This means we can interpret the hypothesis tests for each of the main effects.\nToothpaste is not significant (p-value = .7966) and Brush is marginally significant (p-value = .0517, which is close to the traditional alpha level of 0.05). Our exploratory analysis showed that oscillating brush resulted in the lowest amount of plaque and the ultrasonic brush resulted in the highest plaque measure.\n\n\nCode\nbf2_aov &lt;- aov(Plaque~Brush*Toothpaste,data=bf2)\nsummary(bf2_aov)\n\n\n                 Df Sum Sq Mean Sq F value Pr(&gt;F)  \nBrush             3  86.31  28.769   3.201 0.0517 .\nToothpaste        1   0.62   0.618   0.069 0.7966  \nBrush:Toothpaste  3   6.12   2.040   0.227 0.8763  \nResiduals        16 143.82   8.989                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIn order to trust these hypothesis test results we need to verify that the assumptions are met."
  },
  {
    "objectID": "bf2.html#check-assumptions",
    "href": "bf2.html#check-assumptions",
    "title": "BF[2]",
    "section": "Check Assumptions",
    "text": "Check Assumptions\nFor a more detailed explanation of the code, output, and theory behind these assumptions visit the Assumptions page.\n\nConstant Variance of Residuals\nThere needs to be constant variance of residuals across the factor level combinations. First, we can check the residual plot.\n\n\nCode\nplot(bf2_aov, which = 1)\n\n\n\n\n\nFigure 18: Checking constant variance\n\n\n\n\nIn Figure 18 there are 8 distinct vertical groupings of points, one for each factor level combination. There are 3 observations, and so 3 residuals also, for each factor level combination. There does seem to be a slight trend for points with larger predicted values to be more spread out. On the far left of the plot the points appear closer together, as you move to the right the points tend to be more spread out (thought not always). This phenomenon raises the concern that the assumption of constant variance across factor level combinations may be violated.\nLevene’s test can provide insight on whether the trend in the residual plot is drastic enough to constitute a violation of the constant variance assumption.\nRecall that the null hypothesis for Levene’s test is that the variance of residuals is equal for each and every factor level combination. The results of the test below shows a p-value of 0.5409. We therefore fail to reject a null hypothesis of equal variances and proceed with the analysis (assuming the other assumptions are met).\n\n\nCode\nleveneTest(bf2_aov) %&gt;% kable(digits = 2)\n\n\n\n\n\n\nDf\nF value\nPr(&gt;F)\n\n\n\n\ngroup\n7\n0.88\n0.54\n\n\n\n16\n\n\n\n\n\n\n\n\n\n\n\nNormally Distributed Residuals\nWe check the assumption that residuals are normally distributed in Figure 19. Most of the points are in the shaded region. Row 1 of the dataset appears in the upper right corner, far away from the boundaries. A couple of other points are very close, but just outside of the boundary. Due to the robust nature of ANOVA, the mild violation of this assumption is not a concern.\n\n\nCode\ncar::qqPlot(bf2_aov$residuals, id = FALSE)\n\n\n\n\n\nFigure 19: Checking normality of residuals\n\n\n\n\n\n\nIndependent Residuals\nThe dataset we are analyzing does not include information about the order in which the data was collected. In fact, it may be possible there were multiple teams of researchers collecting the data simultaneously and there is no specific order. From what we know, there is no reason to think there is a potential order bias. Nevertheless, the order plot in Figure 20 can be used to investigate trends in residuals by row number in the dataset. This plot does not show any patterns or trends. The assumption of independent residuals seems to be satisfied.\n\n\nCode\nplot(bf2_aov$residuals)\n\n\n\n\n\nFigure 20: Checking independent residuals assumption\n\n\n\n\n\n\nAssumptions Summary\nThe assumptions appear to be met, meaning the p-values should be valid and reliable. However, since the F-test for toothbrush is such a close call, mild-moderate assumption violations may cause the p-value of the test to be slightly off. Even after attempting various transformations of the response variable, Plaque, the degree to which assumptions are met does not improve (in fact, in some cases it gets worse).\nRather than fuss about the whether F-test assumptions are met or not, you can recognize the F-test for Brush is close enough to merit further investigation. In this case, it is recommended to proceed with contrasts/pairwise tests of toothbrush type which do not require constant variance. (You should be aware of what assumption are required for those tests and be sure to check them).\n\n\nCode\nTukeyHSD(bf2_aov, which = \"Brush\")\n\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = Plaque ~ Brush * Toothpaste, data = bf2)\n\n$Brush\n                             diff       lwr       upr     p adj\nOscillating-Manual     -3.1166667 -8.069047  1.835714 0.3089079\nSonic-Manual           -0.4183333 -5.370714  4.534047 0.9948323\nUltrasonic-Manual       2.2200000 -2.732381  7.172381 0.5864034\nSonic-Oscillating       2.6983333 -2.254047  7.650714 0.4280732\nUltrasonic-Oscillating  5.3366667  0.384286 10.289047 0.0324891\nUltrasonic-Sonic        2.6383333 -2.314047  7.590714 0.4469155\n\n\nIn the above output results for all pairwise comparisons with Tukey’s HSD adjustment for multiple comparisons are displayed. Only the Ultrasonic-Oscillating comparison is significantly different, with a p-value of .03. Therefore, we can state that the ultrasonic brush performs significantly worse than the oscillating brush at plaque reduction."
  },
  {
    "objectID": "bf2.html#notation-for-estimated-effects",
    "href": "bf2.html#notation-for-estimated-effects",
    "title": "BF[2]",
    "section": "Notation for Estimated Effects",
    "text": "Notation for Estimated Effects\nHere are symbolic representations for the estimated effects in the BF[2] model, as shown in Equation 1.\n\\[\n\\hat{\\alpha}_i = \\bar{y}_{i \\cdot \\cdot} - \\bar{y}_{\\cdots}\n\\]\n\\[\n\\hat{\\beta}_j = \\bar{y}_{\\cdot j \\cdot} - \\bar{y}_{\\cdots}\n\\]\n\\[\n\\begin{align}\n\\hat{\\alpha}\\hat{\\beta}_{ij} & = \\bar{y}_{i j \\cdot} - (\\bar{y}_{\\cdot \\cdot \\cdot} + \\hat{\\alpha}_i  + \\hat{\\beta}_j ) \\\\\n&= \\bar{y}_{i j \\cdot} - (\\bar{y}_{\\cdot \\cdot \\cdot} + (\\bar{y}_{i \\cdot \\cdot} - \\bar{y}_{\\cdot \\cdot \\cdot})  + (\\bar{y}_{\\cdot j \\cdot} - \\bar{y}_{\\cdot \\cdot \\cdot}) ) \\\\\n& = \\bar{y}_{i j \\cdot} - (\\bar{y}_{i \\cdot \\cdot} + \\bar{y}_{\\cdot j \\cdot} - \\bar{y}_{\\cdot \\cdot \\cdot})\n\\end{align}\n\\]"
  },
  {
    "objectID": "bf2.html#unreplicated-or-single-observation-per-cell",
    "href": "bf2.html#unreplicated-or-single-observation-per-cell",
    "title": "BF[2]",
    "section": "Unreplicated or Single Observation per Cell",
    "text": "Unreplicated or Single Observation per Cell\nWhat can be done in the case of an experiment where there is only one observation per factor level combination?\nUnder construction."
  },
  {
    "objectID": "bf2.html#footnotes",
    "href": "bf2.html#footnotes",
    "title": "BF[2]",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThere are symbolic/mathematical representations of the factor effect formulas located in the appendix.↩︎\nThe values used in calculations here and throughout this page are rounded for a cleaner display. However, you should use unrounded values for these calculations. Rounded values and significant digits is the reason the arithmetic in some of these calculations appears off by one one-hundredth.↩︎\nYou can explore behavior of the F distribution for various combinations of degrees of freedom using this applet↩︎\nThis is the formula used in Excel to obtain the p-values for brush, toothpaste, and brush:toothpaste interaction respectively:\n\nbrush: `=f.dist.rt(3.20054, 3, 16)\ntoothpaste: =f.dist.rt(0.06871, 1, 16)\nbrush:toothpaste : =f.dist.rt(.226924, 3, 16)\n\n↩︎\nThis is the formula used in R to obtain the p-values for brush, toothpaste, and brush:toothpaste interaction respectively:\n\nbrush: 1-pf(3.2005, 3, 16)\ntoothpaste: 1-pf(0.06871, 1, 16)\nbrush:toothpaste : 1-pf(0.2269, 3, 16)\n\n↩︎"
  },
  {
    "objectID": "bf3.html#create-the-model",
    "href": "bf3.html#create-the-model",
    "title": "BF[3]",
    "section": "Create the Model",
    "text": "Create the Model\nThe 3-way Basic Factorial model is created in R just as the 2-way model was: using the aov() function. To see results of the F-test you can feed your model into a summary() or anova() function.\n\nmyaov &lt;- aov(Y ~ X1 * X2 * X3, data=YourDataSet)\nsummary(myaov)\n\n\nmyaov is the user defined name in which the results of the aov() model are stored\nY is the name of a numeric variable in your dataset which represents the quantitative response variable.\nX1, X2, and X3 are names of qualitative variables in your dataset. They represent the independent variables in the study. They should have class(X) equal to factor or character. If that is not the case, use factor(X) inside the aov(Y ~ factor(X1)*...) command.\nYourDataSet is the name of your data set.\n\nThe * in the code above is a shortcut for writing out the whole model. It can be read as, “include each term by itself, and all possible interaction terms”. The long way of writing out the model uses a colon, :, to define interaction terms and is shown below. When writing it this way each term must be explicitly stated.\n\nmyaov &lt;- aov(Y ~ X1 + X2 + X3 + X1:X2 + X1:X3 + X2:X3 + X1:X2:X3, data=YourDataSet)\nsummary(myaov)\n\nFigure 3 shows the results for the full BF[3] model for the study of halo effect on perception of talent. The 3-way interaction is significant here (p-value - 1.081e-7). Two of the two-way interactions are significant. None of the main effects show significance.\n\n\nCode\nhalo_aov &lt;- aov(talent ~ reader * author * attractiveness, data = df)\nsummary(halo_aov) %&gt;% pander()\n\n\n\n\n\nAnalysis of Variance Model\n\n\n\n\n\n\n\n\n\n\n \nDf\nSum Sq\nMean Sq\nF value\nPr(&gt;F)\n\n\n\n\nreader\n1\n13.02\n13.02\n0.4004\n0.5309\n\n\nauthor\n1\n20.02\n20.02\n0.6156\n0.4378\n\n\nattractiveness\n2\n101.8\n50.9\n1.565\n0.223\n\n\nreader:author\n1\n31.69\n31.69\n0.9744\n0.3302\n\n\nreader:attractiveness\n2\n4490\n2245\n69.03\n4.793e-13\n\n\nauthor:attractiveness\n2\n259.3\n129.6\n3.987\n0.02729\n\n\nreader:author:attractiveness\n2\n1683\n841.7\n25.88\n1.081e-07\n\n\nResiduals\n36\n1171\n32.52\nNA\nNA\n\n\n\n\nFigure 3: BF[3] ANOVA summary table\n\n\n\n\n\n\n\n\n\nKeeping Lower Order Effects\n\n\n\nWhen a significant interaction is discovered, all lower order interactions and main effects should remain in the model - even if they are not significant.\n\n\nWhen a 2-way interaction is found to be significant, then both of the simple factors (or main effects factors) should be included in the model. If a 3-way interaction is found to be significant, then the 3 main effects and all corresponding 2-way interactions should all be included in the ANOVA model - even if they are not statistically significant.\nInterpreting lower order interactions and main effects in the presence of a significant higher order interaction should be done with caution. One should investigate the interaction plots in order to describe how the effect of one variable may depend on the value of another. Simply comparing marginal means2 and interpreting main effects is not sufficient and may lead to incorrect conclusions.\nIn this example, we will not interpret any of the main effects or two-way interactions alone, since the 3-way interaction is significant. In the Interaction section of this page, an interpretation of the interaction was already discussed. To recap that section, we saw in Figure 1 that for male readers, attractiveness of author tended to correspond to higher perceptions of talent of the author - regardless of the author’s gender. However, for female readers, the impact of author attractiveness depended on author gender: attractive female authors were perceived as less talented than unattractive female authors, whereas attractiveness had negligible effect on perceived talent of male authors.\nIn order to trust these hypothesis test results we need to verify that ANOVA model assumptions are met."
  },
  {
    "objectID": "bf3.html#check-assumptions",
    "href": "bf3.html#check-assumptions",
    "title": "BF[3]",
    "section": "Check Assumptions",
    "text": "Check Assumptions\nFor a more detailed explanation of the code, output, and theory behind these assumptions visit the Assumptions page.\n\nConstant Variance of Residuals\nThere needs to be constant variance of residuals across the factor level combinations. First, we can check the residual plot.\n\n\nCode\nplot(halo_aov, which = 1)\n\n\n\n\n\nFigure 4: Checking constant variance\n\n\n\n\nIn the residual plot, Figure 4, the vertical spread of the points is roughly constant as one moves from left to right across the x-axis of the plot. It is clear that the assumption of constant variance of the residuals is met.3 Rarely does a plot look better than this.\n\n\nNormally Distributed Residuals\nWe check the assumption that residuals are normally distributed in Figure 5. All the points are in the shaded region, so we conclude this assumption is met.\n\n\nCode\ncar::qqPlot(halo_aov$residuals, id = FALSE)\n\n\n\n\n\nFigure 5: Checking normality of residuals\n\n\n\n\n\n\nIndependent Residuals\nThe dataset we are analyzing does not include information about the order in which the data was collected. Independence of observations often becomes a concern when there is re-use of equipment, researcher(s) making multiple measurements over time, or any other process that is repeated in each experimental run. Those types of situations can lead to order bias, which can make residuals time/order dependent rather than independent.\nNothing in this experiment suggests a potential for order bias. If we assume that random selection of participants has taken place, and that participants have randomly been assigned to treatments as indicated, there is little reason to doubt that observations (and consequently residuals too) are independent. There is no need to provide a plot in this case.\n\n\nAssumptions Summary\n\n\nRarely are assumptions so clearly and obviously met. In this case, the data was simulated in order to replicate effects and interactions similar to what the original study found. Real data does not usually behave as nicely as simulated data.\nThe assumptions appear to be met, meaning the p-values should be valid and reliable."
  }
]