[
  {
    "objectID": "Anova_F-test.html",
    "href": "Anova_F-test.html",
    "title": "ANOVA and the F-Test",
    "section": "",
    "text": "With the effects model defined we will want to test whether the treatment factor(s) has a statistically significant effect on the response variable. In other words we are interested in testing the hypotheses:\n\\[\nH_0: \\alpha_1 = \\alpha_2 = ... = 0\n\\]\n\\[\nH_a: \\alpha_i \\ne 0 \\text{ for at least one i}\n\\tag{1}\\]\nWhere \\(\\alpha_i\\) represents the effect of a factor level.\n\n\nTesting whether the effects are all equal to each other is incomplete, you must include the zero. Since all level effects must sum to zero for a given factor, the only way all effects can be equal is if they are all zero.\nAnalysis of variance (ANOVA) is a statistical technique that allows us to simultaneously test all factor level effects at the same time.\nBy the end of this section you should be able to state a hypothesis for each term in an ANOVA effects model, calculate and explain the meaning of all the pieces of an ANOVA summary table output, and use the ANOVA summary table to conduct a test of the stated hypothesis."
  },
  {
    "objectID": "Anova_F-test.html#multiple-t-tests",
    "href": "Anova_F-test.html#multiple-t-tests",
    "title": "ANOVA and the F-Test",
    "section": "Multiple t-tests",
    "text": "Multiple t-tests\nAt this point it is reasonable to ask if we couldn’t arrive at the same conclusion by simply conducting multiple t-tests. For example, you could do a t-test for each factor level to determine if the effect is significantly different from zero. Or you might consider testing each combination of factor level effects to see if they are equal to each other.\nThe multiple t-test approach has a couple of a drawbacks. The first drawback is that it becomes a real burden to run, present and interpret a lot of tests if there are many levels to a factor. If there are only 3 or 4 factor levels and only 1 or 2 factors in the analysis, conducting many tests may simply be an annoyance. However, if many factors and/or many factor levels are involved the magnitude of tests may bog down your analysis.\nThe other main drawback of using multiple t-tests is more substantive and has to do with the probability of a Type I error. Suppose the treatment factor in our study has 3 levels. The null hypothesis associated with an ANOVA that tests all factor level effects simultaneously is:\n\\[\nH_0: \\alpha_1 = \\alpha_2 =  \\alpha_3 = 0\n\\]\nTesting at a 0.05 significance level means there is a 0.05 probability we will incorrectly reject this null hypothesis (i.e. commit a Type I error). Conversely, there is 0.95 probability we will NOT commit a Type I error.\nIf we attempt to approach the problem by conducting multiple t-tests, we would test the following set of null hypotheses:\n\\(H_0: \\alpha_1 = 0\\) and \\(H_0: \\alpha_2 = 0\\) and \\(H_0: \\alpha_3 = 0\\).\nWe may conduct each of these tests at the 0.05 significance level. Incorrectly rejecting the null hypothesis on any one of these tests would result in the same Type 1 error as incorrectly rejecting the null hypothesis of our ANOVA test of all the effects simultaneously.\nSo what is the probability of committing a Type 1 error in at least 1 of these 3 tests? The simplest way to find the probability of committing at least one Type 1 error in the 3 tests is to calculate \\(1 – P(\\text{no Type 1 errors in all three tests})\\). As previously stated, the significance level (0.05) of each test represents the probability of a Type 1 error. Therefore, the probability of not committing a Type 1 error on each test is 0.95. If we treat the tests as independent, we can find the probability of NOT committing a Type 1 error in all of the tests by multiplying the probabilities:\n\\[\n0.95 * 0.95 * 0.95 = 0.857\n\\]\nWe can subsequently find \\(1-0.857 = .143\\) is the probability of committing a Type 1 error in at least one of the tests, assuming all the null hypotheses are true. This is often referred to as the family wise error rate. The Type 1 error probability (0.143) in this family of t-tests is almost 3 times higher than the ANOVA Type 1 probability of 0.05.\nIn summary, ANOVA allows us to keep the number of tests manageable and it greatly simplifies how Type 1 error is addressed.\nIf we consider a study with more than 1 factor, there are additional advantages of ANOVA. Unlike a multiple t-test approach, while testing one factor’s effects the ANOVA test can account for the other factors’ impact on the response. In this regard, ANOVA is similar to regression."
  },
  {
    "objectID": "Anova_F-test.html#regression",
    "href": "Anova_F-test.html#regression",
    "title": "ANOVA and the F-Test",
    "section": "Regression",
    "text": "Regression\nANOVA and linear regression are more similar than they are different. ANOVA and linear regression each have their own vocabulary because they were developed under different circumstances. ANOVA was developed to deal with agricultural experiments where the independent variables were primarily categorical. Linear regression tends to be introduced as a tool to analyze data where independent variables are quantitative. Though the language and output associated with each technique may appear different on the surface, the underlying “math” (i.e. the linear algebra) for both techniques is identical. It is not uncommon to have a study where there are multiple quantitative independent variables and multiple categorical independent variables. Thus, the differences between the two lie more in the problems they tend to be applied to and the vocabulary of the researcher than in any meaningful difference in results."
  },
  {
    "objectID": "Anova_F-test.html#mean-squares-ms",
    "href": "Anova_F-test.html#mean-squares-ms",
    "title": "ANOVA and the F-Test",
    "section": "Mean Squares (MS)",
    "text": "Mean Squares (MS)\nYou can think of Mean Squares (MS) as synonymous with variance. The F statistic is a ratio of variances:\n\\[\nF = \\frac{\\text{Variation between factor levels}}{\\text{Variation within factor levels}} = \\frac{\\text{Mean squares of treatment factor means}}{\\text{Mean squares of residual errors}}\n\\]\nWith this is mind, we can fill in the F column of the ANOVA table for Treatment Factor.\n\n\nTable 2: Blank ANOVA summary table for an experiment with 1 treatment factor\n\n\n\n\n\n\n\n\n\n\nSource\ndf\nSS\nMS\nF\np-value\n\n\n\n\nGrand Mean\n\n\n\n\n\n\n\nTreatment Factor\n\n\n\n\\(\\frac{MS_\\text{Treatment Factor}}{MS_\\text{Residual Error}}\\)\n\n\n\nResidual Error\n\n\n\n\n\n\n\nTotal\n\n\n\n\n\n\n\n\n\nTo find the variation between factor level means, calculate the sample variance of factor level means and multiply it by the number of replicates in each factor level. (In the case of unbalanced data where you do not have the same number of replicates in each factor level, weighting by sample size is needed).\nThe Mean Squares of the residual error factor (i.e. Mean Squared Error, MSE) represents the within factor level variation. To calculate it you can find the sample variance within each factor level and then take the mean of those variances. (In the case of unbalanced data where you do not have the same number of replicates in each factor level, you would take a weighted average).\nFigure 4 (a) and Figure 4 (b) below show deviations necessary to calculate the between group and within group variance (or in other words the mean squares treatment and the mean squares error). The figures are based on data from an experiment with 3 factor levels. Figure 4 (a) shows variation between factor level means. It shows the factor level means plotted as blue lines and the grand mean as a red line. In this chart we see the deviation from each factor level mean to the grand mean represented as a gray dashed line. As mentioned above, the mean squares for treatment could be computed as the variance of the 3 factor level means2, and multiplied by 15 (the number of replicates in each factor level).\n\n\n\n\n\n\n\n(a) Deviations to Get MS Treatment Factor\n\n\n\n\n\n\n\n(b) Deviations to Get MS Error\n\n\n\n\nFigure 4: Between Group vs. Within Group\n\n\nFigure 4 (b) shows variation within factor levels. Each point is plotted in a cluster according to the factor level it belongs to. The deviation from each point to its respective factor level mean is depicted with a gray dashed line. The mean square error could be computed by finding the sample variance within each group3 and then taking the mean of those 3 variance estimates.\nThinking about things in this way is helpful to understand conceptually what is happening. However, the ANOVA summary table captures interim steps for calculating mean squares slightly different. Since Mean Squares is synonymous with variance, now is a good time to review the sample variance formula.\n\\[\ns^2 = \\frac{\\sum{(y_i - \\bar{y})}^2}{n-1}\n\\tag{2}\\]\nUpon closer examination of Equation 2 you can see that this formula is essentially a mean. In fact, you can think of variance as a mean of squared deviations (a.k.a. errors). Any mean is built using 2 parts:\n\n\nRecall that an effect is defined as a deviation from the mean.\n\nnumerator: a sum or total\ndenominator: the number of pieces of information used to create the sum in the numerator\n\nTherefore, in a mean square calculation, the numerator is the sum of squares and the denominator is the degrees of freedom.\n\n\n\n\n\n\n\n\n\n\n\nSource\ndf\nSS\nMS\nF\np-value\n\n\n\n\nGrand Mean\n\n\n\n\n\n\n\nTreatment Factor\n\n\n\\(\\frac{SS_\\text{Treatment Factor}}{df_\\text{Treatment Factor}}\\)\n\\(\\frac{MS_\\text{Treatment Factor}}{MS_\\text{Residual Error}}\\)\n\n\n\nResidual Error\n\n\n\\(\\frac{SS_\\text{Residual Error}}{df_\\text{Residual Error}}\\)\n\n\n\n\nTotal"
  },
  {
    "objectID": "Anova_F-test.html#sum-of-squares-ss",
    "href": "Anova_F-test.html#sum-of-squares-ss",
    "title": "ANOVA and the F-Test",
    "section": "Sum of Squares (SS)",
    "text": "Sum of Squares (SS)\nLet’s talk about the numerator first, this will be the sum of squared deviations, or Sum of Squares for short. The Sum of Squares (SS) is a measure of the total variability in a dataset. A naïve approach to calculating total variability in a dataset is to measure the distance from each value to the mean of the dataset. The problem with this approach is that those distance measures will always sum to zero.\nTo avoid this problem, statisticians square the distances before summing them. This results in a value that summarizes the total amount of spread in the dataset. This quantity, the Sum of Squares, is important and so it has its own column in the ANOVA summary table.\nIn the table below an equation for each factor’s SS is listed using terms from the factor effects model. We’ll walk through the meaning of each of those equations.\n\n\n\n\n\n\n\n\n\n\n\nSource\ndf\nSS\nMS\nF\np-value\n\n\n\n\nGrand Mean\n\n\\(n*\\bar{y}_\\text{..}^2\\)\n\n\n\n\n\nTreatment Factor\n\n\\[ \\sum (\\hat{\\alpha}_i^2*n_i)\\]\n\\(\\frac{SS_\\text{Treatment Factor}}{df_\\text{Treatment Factor}}\\)\n\\(\\frac{MS_\\text{Treatment Factor}}{MS_\\text{Residual Error}}\\)\n\n\n\nResidual Error\n\n\\[ \\sum \\hat{\\epsilon}_\\text{ij}^2 \\]\n\\(\\frac{SS_\\text{Residual Error}}{df_\\text{Residual Error}}\\)\n\n\n\n\nTotal\n\n\\(\\sum y_\\text{ij}^2\\)\n\n\n\n\n\n\n\n\nA deviation from the mean can be thought of as an effect. That is why the symbols for factor effects are used in the SS column in the ANOVA summary table.\nFirst, let’s review the factor effects model to better understand the equations in the SS column above.\n\\[\ny_\\text{ij} = \\mu + \\alpha_i + \\epsilon_\\text{ij}\n\\]\nWe can walk through each equation in the SS column, one by one.\n\nGrand Mean SS: \\(\\bar{y}_\\text{..}\\) is the grand mean. Its value should be squared and then multiplied by \\(n\\), which is the total number of observations in the study.\nTreatment Factor SS: Recall that \\(\\hat{\\alpha}_i\\) is the estimated effect of each factor level. After squaring each effect, multiply it by the number of observations in that level, \\(n_i\\). Finally, the \\(\\sum\\) symbol means to add all those products together.\nResidual Error SS: \\(\\hat\\epsilon_\\text{ij}\\) is the symbol for an observed residual. Each residual must be squared and then all those squared residuals are summed together.\nTotal SS: This is the sum of all the other sums of squares. Or it can be found by squaring each observed value and then adding up all those squared observations."
  },
  {
    "objectID": "Anova_F-test.html#degrees-of-freedom",
    "href": "Anova_F-test.html#degrees-of-freedom",
    "title": "ANOVA and the F-Test",
    "section": "Degrees of Freedom",
    "text": "Degrees of Freedom\nDegrees of freedom can be thought of as the number of unique pieces of information that contributed to the variance estimate, mean squares.\n\n\n\n\n\n\nDegrees of Freedom\n\n\n\nThe number of unique pieces of information that contributed to the variance estimate.\n\n\nIn our dataset we have a certain number of observations. All those observations can be used to estimate the variance in the dataset. But you will notice in Equation 2 the data has already been used to estimate the grand mean (\\(\\bar{y}\\) estimates \\(\\mu\\)). In other words, before we can estimate the variance we must use the data to estimate the mean. Estimating the mean “uses up” one degree of freedom. This is why the denominator of the sample variance formula divides by \\(n-1\\) instead of by \\(n\\).\nFor additional explanation, consider this simple example. There are three data points and you know that the mean of these 3 data points is 10. The value of the first data point could be any number, it is free to vary. The value of the second data point could also be any number, it is free to vary. The third number’s value is not free to vary. It is constrained by the fact that the mean of the 3 data points must be 10. The values of the first two datapoints will determine the value of the third under the constraint of a known (or estimated) mean.\nThe example described above is summarized in Table 3. The first number is represented as an \\(a\\) and the second number if represented with a \\(b\\).\n\n\nTable 3: Only n-1 values are free to vary when the mean of the values is known\n\n\n\n\n\n\n\n\n\nvalue 1\nvalue 2\nvalue 3\n\nMean of 3 Values\n\n\n\n\na\nb\n\\(3*10 - (a+b)\\)\n-&gt;\n10\n\n\nfree to vary\nfree to vary\ndepends on other two values\n\n\n\n\n\n\nHow does this apply to the analysis of variance? Initially you have \\(n\\) observations, or in other words \\(n\\) unique pieces of information that can be used to estimate variance of the dataset. As you try to break the dataset’s variance into its component pieces, you will need to reallocate the \\(n\\) pieces of information to each factor (grand mean, treatment factor, residual error) for use in estimating each factor’s mean square. To paraphrase the law of the conservation of mass, “the number of observations can neither be created nor destroyed”. The sum of degrees of freedom for all the factors must equal the number of observations in the dataset.\nWe will reason through the degrees of freedom calculation for each of the 3 sources in the ANOVA table. Keep in mind, we are using the simplest experiment, just one treatment factor, to illustrate these concepts. In more complex designs, there will be additional factors listed in the “sources” column.\nAs was mentioned, every time we use the data to estimate a parameter we use a degree of freedom. Or in other words, every time we use the data to estimate a parameter we lose a degree of freedom for our mean square error estimate. To find the grand mean we average over all the values in the dataset; that uses up one degree of freedom because we have estimated one mean, the grand mean.\nWhen calculating the degrees of freedom for the treatment factor you might be tempted to think that the degrees of freedom is equal to the number of factor levels because you have to estimate a mean for each level. But, remember the simple example depicted in figure Table 3. Because I have already estimated the grand mean, the last factor level is not free to vary and therefore is not estimated directly. The mean of the last factor level will have to be a number that satisfies the constraint that the mean of all the factor level means is the grand mean.\nExplained another way, consider the fact that all the factor level effects must sum to zero. If there are \\(i\\) factor levels, you only need to estimate effects for \\(i-1\\) levels. The last level’s effect is a function of the other factor level effects, it does not need to be estimated and therefore does not need a degree of freedom.\nFinally, consider the residual error factor. The non-technical definition of the term residual means “left over”. The degrees of freedom for the residual error factor is whatever degrees of freedom are left over after calculating degrees of freedom for all other factors.\nIn summary, to estimate the degrees of freedom for a factor, start with its number of levels and then subtract the number of means that need to be calculated in order to calculate the factor’s level effects. That may sound a bit confusing, luckily the general rule states the exact same thing in a simpler, more understandable way.\n\n\n\n\n\n\nGeneral rule to find degrees of freedom for a factor\n\n\n\n\\[\n\\text{df} = \\text{number of levels} - \\text{sum of df of outside factors}\n\\]"
  },
  {
    "objectID": "Anova_F-test.html#footnotes",
    "href": "Anova_F-test.html#footnotes",
    "title": "ANOVA and the F-Test",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nF is a random variable. This means we never know exactly what the value of F will be based on random selection/assignments. However, we do know that under the null hypothesis some values of F are more likely than others. Specifically, if the null hypothesis were true then values of 1 are more likely and large values tend to be less likely. The F distribution represents a probability distribution of the variance ratios. In a given study just one random sample is taken and one variance ratio (F) is calculated. However, a different selection of experimental units (or different assignment to factor levels) would result in different estimates of variances, and consequently different F statistics. The F distribution represents all possible F statistics under the null hypothesis that a factor has no significant effect.\nWhen a treatment factor has no significant effect on the response variable, the expected F statistic is 1. This is because there are two sources of variability contributing to our estimate of the treatment factor variance: 1) variance in factor level means and 2) variance within a factor level. The denominator of the F statistic is simply the variance within a factor level (since each factor level has a slightly different variance, it is actually the average or pooled variance within factor levels). So the F statistic looks like this:\n\\[\nF = \\frac{\\text{variance in factor level means} + \\text{variance within factor levels}}{\\text{variance within factor levels}}\n\\]\nIf there really is no significant effect across factor levels, then the variance in factor level means goes to zero and we are left with:\n\\[\nF = \\frac{\\text{variance within factor levels}}{\\text{variance within factor levels}} = 1\n\\]\nThe values for degrees of freedom affect the shape and the spread of the F distribution. Visit this applet to learn more and interact with the family of F distributions.↩︎\nThe variance would be calculated by squaring the deviations, summing them up and then dividing by the degrees of freedom, which is 2 in this case.↩︎\nThe variance is calculated by squaring the deviations, and then summing them together and dividing by the degrees of freedom, which in this case is 14 for each group.↩︎"
  },
  {
    "objectID": "z_multiple_comparisons.html",
    "href": "z_multiple_comparisons.html",
    "title": "Multiple Comparison",
    "section": "",
    "text": "In this page, the approach to dealing with multiple contrasts is explained. Specifically, whether and how to adjust the tests of multiple contrasts to account for an inflated family wise error rate. A few techniques are described, with a focus on when to use which technique. Lastly, instructions and example code is provided for carrying out each technique in R.\nIn most good experiments, researchers are interested in more than just one contrast. Conducting multiple tests on the levels of a factor can inflate the family wise Type I error rate, as illustrated in the “Multiple T-tests” section of the ANOVA page. There is considerable disagreement among statisticians about how to approach the issue of multiple tests. The debate primarily focuses on whether to proactively take steps that will mitigate the inflated family wise error rate or not. To further complicate matters, if an adjustment is desired, there are multiple techniques to choose from. An exhaustive presentation of the arguments on either side is not attempted in this text. Though, a few of the key ideas will naturally emerge as the different approaches are discussed.\n\n\nThe terms “contrast” and “comparison” are treated here as synonymous. Comparison is used more often to refer to testing a difference in factor level means. Read Contrasts for more explanation of these terms."
  },
  {
    "objectID": "z_multiple_comparisons.html#bonferroni",
    "href": "z_multiple_comparisons.html#bonferroni",
    "title": "Multiple Comparison",
    "section": "Bonferroni",
    "text": "Bonferroni\nThe Bonferroni adjustment is best for a handful of pre-planned contrasts. As the number of contrasts grows, the adjustment quickly becomes too conservative (i.e. makes it too hard to find significance). In that case, other methods may strike a better balance between Type I and Type II errors.\n\n\nType I error occurs when a true hypothesis is rejected. Type II error occurs when a false hypothesis is not rejected.\nIt can be shown that in order to not exceed a desired family wise error rate of \\(\\alpha_\\text{fw}\\), for a set of \\(k\\) contrasts, each individual contrast should be tested at a significance level of \\(\\frac{\\alpha_\\text{fw}}{k}\\). For example, if we desire a family wise error rate of 0.05 and plan to do 5 tests, each individual contrast must have a p-value less than \\(\\frac{0.05}{5} = 0.01\\) to be considered significant. The test itself has not changed, only the benchmark p-value for claiming significance. This adjustment is easy to understand and to calculate."
  },
  {
    "objectID": "z_multiple_comparisons.html#scheffé",
    "href": "z_multiple_comparisons.html#scheffé",
    "title": "Multiple Comparison",
    "section": "Scheffé",
    "text": "Scheffé\nUsing experimental results to suggest which contrasts to test is often referred to as “data snooping” or exploratory analysis. For example, you may look at graphical and numerical summaries to see which means (or combinations of means) will be promising to test. The problem with this approach is that you have essentially done a quick, informal test of many differences when you looked at descriptive statistics and plots. In other words, you have already tested the means and combinations of the means in your mind. The Type I error rate of the contrasts will be different (higher) than stated because you are only formally applying the test to those that look significant already.\nScheffé’s method is most useful when many contrasts are tested, especially when going beyond pairwise comparisons of factor level means to test combined factor levels (i.e. complex comparisons). In data exploration, there are theoretically an infinite number of contrasts you could test. This is not a problem for Scheffé’s adjustment because, unlike other adjustment techniques, Scheffé’s adjustment does not depend on the number of comparisons to be made. Rather, it is determined only by the number of factor levels and the number of observations.\nThe F statistic used to test a contrast in Scheffé’s adjustment is related to the omnibus F test for the factor itself, and is given by:\n\\[\nF_\\text{Scheffé} = (k-1)*F\n\\]\nWhere \\(F\\) is the statistic for the F test of the factor as usual. \\(k\\) is the number of levels in that factor. \\(F_\\text{Scheffé}\\) has \\(df_\\text{numerator} = k-1\\) and \\(df_\\text{denominator} = df_\\text{residual}\\). Scheffé’s test output is often in terms of a t test. Recall that the t statistic is simply the square root of the F statistic.\nIf you are interested in only a specific set of hypotheses (all pairwise comparisons, or all treatment levels vs. control, or all levels compared to the “best” level, etc.) there may be another adjustment technique that will provide better statistical power. Two of which are mentioned below."
  },
  {
    "objectID": "z_multiple_comparisons.html#methods-designed-for-all-pairwise-comparisons",
    "href": "z_multiple_comparisons.html#methods-designed-for-all-pairwise-comparisons",
    "title": "Multiple Comparison",
    "section": "Methods Designed for All Pairwise Comparisons",
    "text": "Methods Designed for All Pairwise Comparisons\nWhether it is an exploratory analysis or a pre-planned set of contrasts, many researchers want to test all factor level means against each other. This is usually referred to as testing all pairwise comparisons. Because this situation is so common, two approaches are described below.\n\nTukey’s HSD\nIf a multiple comparison adjustment is desired for testing all pairwise comparisons, a standard approach is to apply Tukey’s Honest Significant Difference (HSD) technique. Occasionally, in the case of few factor levels, Bonferroni’s adjustment may result in more significant findings. If that is the case, use Bonferroni’s correction instead.\nThe calculation for this test is based on the distribution of \\(Q\\). The test statistic \\(Q\\) is sometimes called the studentized range distribution. “Range” is a reference to the numerator where the difference between a maximum and a minimum is calculated. “Studentized” because we are dividing by the estimated standard error, a technique for standardizing famously employed by Student’s (a.k.a. William Gossett) t test. \\(Q\\) is calculated as:\n\\[\nQ = \\frac{max(T_i) - min(T_i)}{\\sqrt{\\frac{MSE}{n}}}\n\\tag{1}\\]\n\n\\(MSE\\) is an estimate of the random error variance\n\\(n\\) is the number of replicates at each factor level. For unequal sample sizes the formula changes somewhat and the result of this test may have a lower Type I error rate than claimed\nThe observations for each level are generated by a different random variable: \\(k\\) variables total, one for each level. Each random variable is normally distributed with mean zero and standard deviation estimated by the denominator of Equation 1. \\(max(T_i)\\) is the maximum value of the \\(k\\) random variables, \\(min(T_i)\\) is the minimum.\nThe distribution of \\(Q\\) will depend on the number of treatments being compared, \\(k\\), and the number of degrees of freedom for error.\n\n\n\nFisher’s LSD\nMaking adjustments for multiple contrasts is a conservative approach, meaning it is more difficult to claim significance compared to when no adjustment is made. Though these adjustments prevent understating the probability of Type I error, they increase the probability of Type II error for each individual contrast. In exploratory analysis where you are looking for hints of what to study further, a Type II error may be a greater concern than Type I.\nFor example, consider a screening study intended to narrow down the number of factors studied in a future experiment. In this case, accidentally ruling out something early on that actually does have a significant impact on the response is more egregious than letting a non-significant factor through to the next experiment where it’s non-significance will be discovered.\nFisher’s Least Significant Difference (LSD) employs no adjustment at all to the pairwise comparisons. However, before proceeding to test pairwise comparisons, the F test for the factor must be significant. Using the less powerful F test as a gatekeeper to the more powerful pairwise t tests serves as a partial protection against extreme Type I errors inflation.\nIn summary, Fisher’s LSD is a two step process. First, verify the F test for the factor is significant. Second, if it is, proceed with all pairwise comparisons without any adjustment. Fisher’s LSD tends to be used in studies where many factors are present, especially screening/exploratory studies."
  },
  {
    "objectID": "z_multiple_comparisons.html#bonferonni",
    "href": "z_multiple_comparisons.html#bonferonni",
    "title": "Multiple Comparison",
    "section": "Bonferonni",
    "text": "Bonferonni\nThere are two ways to implement the Bonferroni adjustment illustrated below using the contrasts that were tested in the R Instructions section of the Contrast page.\n\n\nSee R instructions for Fisher’s LSD for a shortcut to apply Bonferroni adjustment to all pairwise comparisons.\n\nRecalculate an Alpha Level by Hand\nFirst, the Bonferroni adjustment is shown using output provided for a test of contrasts without adjustment. The code and output from the R Instructions section of the Contrast page is provided again here for convenience. These two contrasts test the mean of the manual brush against the oscillating brush mean, as well as the manual brush mean against the mean of all the other brushes combined.\n\n\nCode\nbrush_means &lt;- emmeans(plaque_aov, \"Brush\")\ncontrast_results &lt;- contrast(brush_means,\n                             list(man_v_osc = c(1,-1,0,0),\n                                  man_v_others = c(1,-(1/3),-(1/3),-(1/3))), \n                             adjust=\"none\")\n\n#kable commands are for formatting the output\ncontrast_results |&gt; \n  kable(digits = 2) |&gt; \n  kable_styling(full_width = TRUE)\n\n\n\n\n\ncontrast\nestimate\nSE\ndf\nt.ratio\np.value\n\n\n\n\nman_v_osc\n3.12\n1.58\n20\n1.97\n0.06\n\n\nman_v_others\n0.44\n1.29\n20\n0.34\n0.74\n\n\n\n\n\n\n\n\nA new alpha level against which to compare the p-values can be calculated by hand. Since there are two tests, if the intent was to keep the family wise error rate of 0.05, the alpha level for each individual test is \\(\\frac{0.05}{2} = 0.025\\). To be considered significant, the contrast’s p-value must be less than 0.025.\nman_v_osc has a p-value of 0.06; man_v_others has a p-value of 0.74. Since neither contrast has a p-value less than 0.025 we conclude that the contrasts are not statistically significant.\n\n\nMake the Adjustment in R\nSecond, the Bonferroni adjustment is applied by changing the value of the adjust = argument in the contrast() function from “none” to “bon”.\n\n\nCode\nbon_results &lt;- contrast(brush_means,\n                        list(man_v_osc = c(1,-1,0,0),\n                             man_v_others = c(1,-(1/3),-(1/3),-(1/3))), \n                        adjust=\"bon\")\n\n#kable commands are for formatting the output\nbon_results |&gt; \n  kable(digits = 2) |&gt; \n  kable_styling(full_width = TRUE)\n\n\n\n\n\ncontrast\nestimate\nSE\ndf\nt.ratio\np.value\n\n\n\n\nman_v_osc\n3.12\n1.58\n20\n1.97\n0.13\n\n\nman_v_others\n0.44\n1.29\n20\n0.34\n1.00\n\n\n\n\n\n\n\n\nIn these results the p-values have simply been multiplied by 2 because there were 2 contrasts in the set being tested. Each test can be compared to our desired family wise error rate as usual. Since each test has a p-value greater than 0.05 we conclude that the contrasts are not statistically significant.\nNote, for man_v_others the p-value is capped at 1.00 since a p-value cannot exceed 1.00."
  },
  {
    "objectID": "z_multiple_comparisons.html#scheffé-1",
    "href": "z_multiple_comparisons.html#scheffé-1",
    "title": "Multiple Comparison",
    "section": "Scheffé",
    "text": "Scheffé\nSimilar to how the Bonferroni adjustment was applied, to apply a Scheffé adjustment, change the value of the adjust = argument in the contrast() function from “none” to “scheffe”.\n\n\nCode\ncontrast(brush_means, \n         list(man_v_osc = c(1,-1,0,0),\n              man_v_others = c(1,-(1/3),-(1/3),-(1/3))),\n         adjust=\"scheffe\") |&gt; \n\n  #kable commands are for formatting the output\n  kable(digits = 2) |&gt; \n  kable_styling(full_width = TRUE)\n\n\n\n\n\ncontrast\nestimate\nSE\ndf\nt.ratio\np.value\n\n\n\n\nman_v_osc\n3.12\n1.58\n20\n1.97\n0.17\n\n\nman_v_others\n0.44\n1.29\n20\n0.34\n0.94\n\n\n\n\n\n\n\n Each contrast’s p-value can be compared to our desired family wise error rate of 0.05. Since each test has a p-value greater than 0.05 we conclude that the contrasts are not statistically significant. Notice the p-values with Scheffé’s adjustment are higher than the p-values with Bonferroni’s adjustment. This will always be the case if the number of contrasts being tested is small."
  },
  {
    "objectID": "z_multiple_comparisons.html#tukey",
    "href": "z_multiple_comparisons.html#tukey",
    "title": "Multiple Comparison",
    "section": "Tukey",
    "text": "Tukey\nIn our toothbrush example, if we want to compare each factor level mean to the other we can apply Tukey’s HSD adjustment using a base R function. The arguments to the function are the name of the model and the name of the factor whose means should be tested. Notice the emmeans grid does not have to be created and the contrast coefficients do not explicitly need to be input. The output provides an estimate of each pairwise comparison, as well as adjusted simultaneous confidence intervals and p-values.\n\n\nCode\nTukeyHSD(plaque_aov, \"Brush\") |&gt; pander()\n\n\n\nBrush:\n\n\n\n\n\n\n\n\n\n\n \ndiff\nlwr\nupr\np adj\n\n\n\n\nOscillating-Manual\n-3.117\n-7.55\n1.317\n0.2332\n\n\nSonic-Manual\n-0.4183\n-4.852\n4.015\n0.9933\n\n\nUltrasonic-Manual\n2.22\n-2.214\n6.654\n0.5129\n\n\nSonic-Oscillating\n2.698\n-1.735\n7.132\n0.348\n\n\nUltrasonic-Oscillating\n5.337\n0.9029\n9.77\n0.01487\n\n\nUltrasonic-Sonic\n2.638\n-1.795\n7.072\n0.367\n\n\n\n\n\n\n\nWith a p-value of 0.01, only the Ultrasonic-Oscillating contrast is significant. This significance is driven by the large difference in means between the two levels, Ultrasonic’s mean is 5.337 higher than Oscillating. All other pairwise comparisons have p-values greater than 0.05 and so are not considered significant."
  },
  {
    "objectID": "z_multiple_comparisons.html#sec-fisher",
    "href": "z_multiple_comparisons.html#sec-fisher",
    "title": "Multiple Comparison",
    "section": "Fisher",
    "text": "Fisher\nFirst, look at the ANOVA summary table to see if the F test for brush is significant.\n\n\nCode\nsummary(plaque_aov)\n\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)  \nBrush        3  86.31  28.769   3.822 0.0258 *\nResiduals   20 150.56   7.528                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nThe p-value for Brush is significant at the 0.05 level. You can then proceed with an unadjusted test of all pairwise comparisons for the Brush factor. The syntax for this function is a bit different. You do not have to create the model first.\nIt requires the response vector as the first argument, the factor vector as the 2nd argument, and an adjustment for multiple comparisons (if any) as the 3rd argument.\n\n\nCode\npairwise.t.test(df$Plaque, df$Brush, p.adjust.method = \"none\") |&gt; \n  pander()\n\n\n\nmethod: t tests with pooled SD\n\ndata.name: df\\(Plaque and df\\)Brush\np.value:\n\n\n\n\n\n\n\n\n\n \nManual\nOscillating\nSonic\n\n\n\n\nOscillating\n0.06315\nNA\nNA\n\n\nSonic\n0.7944\n0.104\nNA\n\n\nUltrasonic\n0.1764\n0.003052\n0.1114\n\n\n\np.adjust.method: none\n\n\n\n\n\n The output is a triangular matrix of p-values. The p-value for Ultrasonic vs. Oscillating is 0.0031 and is the only significant pairwise comparison at the 0.05 level; though Oscillating vs. Manual is close, with a p-value of 0.0632.\n\n\n\n\n\n\nNote\n\n\n\nThe pairwise.t.test() can receive “bon” as an input to the p.adjust.method = argument, which will perform all pairwise comparisons and apply the Bonferroni adjustment. When the number of factor levels is small Bonferroni is preferred over the Tukey’s HSD because it provides more statistical power."
  },
  {
    "objectID": "testrmd.html",
    "href": "testrmd.html",
    "title": "Describing Data",
    "section": "",
    "text": "Calculating treatment means for one factor:\n\nlibrary(mosaic)\nlibrary(pander)\nfavstats(Y~X, data=YourDataSet)\n\nDP Example code:\n\n\n library(mosaic) mosaic is an R Package that is useful in the teaching of statistics to beginning programmers.  library(pander) pander is an R Package that makes R output look pretty  favstats( a function from the mosaic package that returns a set of favorite summary statistics  Temp This is our response variable. From ?airquality you can see in the help file that Temp is the maximum daily temp in degrees F at La Gaurdia Aiport during 1973  ~ “~” is the tilde symbol. It can be interpreted as “y broken down by x”; “y modeled by x”; “y explained by x”, etc. Where y is on the left of the tilde and x is on the right.   Month, “Month” is a column from the airquality dataset that can be treated as qualitative.  data = airquality You have to tell R what dataset the variables Temp and Month come from. ‘airquality’ is a preloaded dataset in R.   ) Functions must always end with a closing parenthesis.   Click to view output  Click to View Output."
  },
  {
    "objectID": "split_plot.html",
    "href": "split_plot.html",
    "title": "The SP/RM[1, 1] Design",
    "section": "",
    "text": "The designation SP/RM[1, 1] stands for a Split Plot, or Repeated Measures design with 1 factor (the between-block factor) applied to the larger experimental units (blocks), and 1 factor (the within-block factor) applied to smaller experimental units. The block factor is another important part of the model, but it is considered a nuisance factor and so does not show up specifically in the SP/RM[1, 1] designation.\n\n\nConsider an example from agriculture. Farmer John would like to compare yield for a specific variety of maize treated with 2 different fungicides: generic vs premium. Farmer John has 4 plots of land available for planting. He can assign a fungicide to each plot completely at random, see Figure 1.\n\n\n\nFigure 1: Layout of Farmer John’s 4 Plots\n\n\nFarmer John may be interested in the variation of yield within each plot. Even if he takes multiple yield measurements within each plot and measures yield at each sample point, he still only has 2 replicates of each fungicide.\nSo far there is nothing new here and this can be analyzed as a BF[1]. Each plot is an experimental unit and is assigned one of the fungicides completely at random.\n\n\nFarmer John suspects that the impact of fungicide type is different for different varieties of maize. He wants to add 3 maize varieties to the experiment, but because of mechanical constraints he cannot completely randomize combinations of fungicide with maize variety. While maize variety can be applied to smaller areas within a plot, he can only apply the fungicide to an entire plot.\nNothing has changed for the fungicide treatment level. But it can be seen in Figure 2 we have a new level of experimental unit: sub-plots! Variety is assigned to sub-plots randomly within each plot. Variety is unreplicated inside of a single plot, but looking across plots we see variety replicated 4 times.\n\n\n\nFigure 2: Layout of Fungicide Assigned to Plots with Maize Variety Assigned to Subplots\n\n\nBecause each plot belongs to only 1 level of fungicide, we say plot is nested within fungicide.\n\n\n\n\n\n\nNesting occurs when each level of a factor appears with only 1 level of another factor. The block factor will always be nested inside of the between-block factor.\nFactor crossing occurs when all levels of one factor appear with each and every level of another factor. Variety is crossed with block because each variety appears in each block. Variety is also crossed with fungicide.\nIn more general terms, the within-block factor is crossed with the block factor and is also crossed with the between-block factor.\n\n\n\n\n\n\n\nFigure 3 shows the factor structure for Farmer John’s experiment. The partition lines for the plot factor are dashed to emphasize that the plots are nested in fungicide. You may also notice that the plot arrangement in Figure 3 is not quite like the arrangement in Figure 2. Remember, the factor structure diagram represents how the data is arranged and partitioned and does not necessarily represent how the plots are positioned in the physical, real world.\n\n\n\n\nFigure 3: Factor Structure for Farmer John’s Experiment\n\n\n\nFigure 4 shows a generic factor structure for a split plot / repeated measures design, without the context of the Farmer John example.\n\n\n\n\nFigure 4: Factor Structure for Generic SP/RM[1,1] Design\n\n\n\nThere are two levels of experimental units. Plot is the experimental unit for Fungicide, and sub-plot is the experimental unit for maize variety.\nYou can tell what the experimental unit is for a factor based on how/when the randomization takes place. The fungicide was randomized to plots, and maize variety was randomly assigned to sub-plots within a whole-plot.\n\n\n\n\n\n\nReplication Inside of Blocks\n\n\n\nThe factor structure diagrams above show examples of designs without replication inside of blocks. However, it is not uncommon to have replication within blocks. For example, Farmer John could have split each plot into 6 sub-plots, which would allow variety to be replicated twice in each plot. This poses no problem in the analysis, and in fact gives us more precision in our estimates.\nReplication of the within-block factor inside of a block also allows the estimation of the block by within-block interaction (e.g. plot by variety interaction). Usually blocks are random factors, so this likely results in an interaction between a fixed and random factor. Analyzing this type of interaction requires some additional assumptions/decisions that are not discussed in this book. For that reason, we will continue to make the assumption the block by within-block interaction effect is zero and will keep our designs simple with no replication inside of blocks.\n\n\n\n\n\nThere is not a consensus on notation for a split-plot / repeated measures design with nested factors. Different textbooks will represent the models differently algebraically1. The notation selected for this text book opts for the most simple approach to subscripts.\nEach factor (i.e. meaningful partition of the data) in Figure 4 corresponds to a term on the right hand side of Equation 1.\n\\[\ny_{ijk} = \\mu + \\alpha_i + \\beta_{ij} + \\gamma_k + (\\alpha \\gamma)_{ik} + \\epsilon_{ijk}\n\\tag{1}\\]\nWhere\n\n\\(y_{ijk}\\) is the observation that belongs to level i of \\(\\alpha\\), level j of \\(\\beta\\), and level k of \\(\\gamma\\).\n\\(\\mu\\) is the grand mean of all yield data.\n\\(\\alpha_i\\) is the between-block effect (also called the whole plot effect). In the Farmer John example i goes from 1 to 2 because there are 2 levels of fungicide.\n\\(\\beta_{ij}\\) is the block effect. The double subscript ij is necessary because a block is nested inside of the between-block factor level. In the farm context, a subscript value of 12 refers to the second plot that received fungicide A; whereas a subscript of 22 refers to the second plot that received fungicide B. So, even though the value for j is two in both instances, the subscript refers to two completely different plots!\n\n\n\nBlock is the experimental unit for the between-block factor and thus serves as the error term for the between-block factor.\n\n\\(\\gamma_k\\) is the effect of the within-block factor. In the Farmer John example, k goes from 1 to 3 because there are 3 varieties of maize.\n\\((\\alpha \\gamma)_{ik}\\) represents the interaction between fungicide and variety. More generically, it represents the interaction of the between-block factor and the within-block factor.\n\\(\\epsilon_{ijk}\\) is the residual error term and is used to test the within-block factor and the interaction.\n\nA hypothesis for the main effect of fungicide type:\n\\[H_0: \\alpha_\\text{i} = 0 \\text{ for all } i\\]\n\\[H_a: \\alpha_\\text{i} \\ne 0 \\text{ for some } i\\]\nA hypothesis for the main effect of variety:\n\\[H_0: \\gamma_\\text{k} = 0 \\text{ for all } k\\]\n\\[H_a: \\gamma_\\text{k} \\ne 0 \\text{ for some } k\\]\nA hypothesis for the interaction of fungicide and variety.\n\\[\nH_0: (\\alpha\\gamma)_\\text{ik} = 0 \\text{ for all } ik\n\\]\n\\[\nH_a: (\\alpha\\gamma)_\\text{ik} \\ne 0 \\text{ for some } ik\n\\]\n\n\n\nANOVA tests are appropriate for a split-plot analysis if the following requirements are satisfied:\n\n\n\n\n\n\n\n\nRequirements\nMethod for Checking\nWhat You Hope to See\n\n\n\n\nConstant variance across factor levels\nResidual vs. Fitted Plot\nNo major disparity in vertical spread of point groupings\n\n\nThe residuals are normally distributed\nNormal Q-Q plot\nStraight line, majority of points in boundaries\n\n\nIndependent residuals\nOrder plot\nNo pattern/trend\n\n\n\nFamiliarity with/critical thinking about the experiment\nNo potential source for bias"
  },
  {
    "objectID": "split_plot.html#motivating-example",
    "href": "split_plot.html#motivating-example",
    "title": "The SP/RM[1, 1] Design",
    "section": "",
    "text": "Consider an example from agriculture. Farmer John would like to compare yield for a specific variety of maize treated with 2 different fungicides: generic vs premium. Farmer John has 4 plots of land available for planting. He can assign a fungicide to each plot completely at random, see Figure 1.\n\n\n\nFigure 1: Layout of Farmer John’s 4 Plots\n\n\nFarmer John may be interested in the variation of yield within each plot. Even if he takes multiple yield measurements within each plot and measures yield at each sample point, he still only has 2 replicates of each fungicide.\nSo far there is nothing new here and this can be analyzed as a BF[1]. Each plot is an experimental unit and is assigned one of the fungicides completely at random.\n\n\nFarmer John suspects that the impact of fungicide type is different for different varieties of maize. He wants to add 3 maize varieties to the experiment, but because of mechanical constraints he cannot completely randomize combinations of fungicide with maize variety. While maize variety can be applied to smaller areas within a plot, he can only apply the fungicide to an entire plot.\nNothing has changed for the fungicide treatment level. But it can be seen in Figure 2 we have a new level of experimental unit: sub-plots! Variety is assigned to sub-plots randomly within each plot. Variety is unreplicated inside of a single plot, but looking across plots we see variety replicated 4 times.\n\n\n\nFigure 2: Layout of Fungicide Assigned to Plots with Maize Variety Assigned to Subplots\n\n\nBecause each plot belongs to only 1 level of fungicide, we say plot is nested within fungicide.\n\n\n\n\n\n\nNesting occurs when each level of a factor appears with only 1 level of another factor. The block factor will always be nested inside of the between-block factor.\nFactor crossing occurs when all levels of one factor appear with each and every level of another factor. Variety is crossed with block because each variety appears in each block. Variety is also crossed with fungicide.\nIn more general terms, the within-block factor is crossed with the block factor and is also crossed with the between-block factor."
  },
  {
    "objectID": "split_plot.html#factor-structure",
    "href": "split_plot.html#factor-structure",
    "title": "The SP/RM[1, 1] Design",
    "section": "",
    "text": "Figure 3 shows the factor structure for Farmer John’s experiment. The partition lines for the plot factor are dashed to emphasize that the plots are nested in fungicide. You may also notice that the plot arrangement in Figure 3 is not quite like the arrangement in Figure 2. Remember, the factor structure diagram represents how the data is arranged and partitioned and does not necessarily represent how the plots are positioned in the physical, real world.\n\n\n\n\nFigure 3: Factor Structure for Farmer John’s Experiment\n\n\n\nFigure 4 shows a generic factor structure for a split plot / repeated measures design, without the context of the Farmer John example.\n\n\n\n\nFigure 4: Factor Structure for Generic SP/RM[1,1] Design\n\n\n\nThere are two levels of experimental units. Plot is the experimental unit for Fungicide, and sub-plot is the experimental unit for maize variety.\nYou can tell what the experimental unit is for a factor based on how/when the randomization takes place. The fungicide was randomized to plots, and maize variety was randomly assigned to sub-plots within a whole-plot.\n\n\n\n\n\n\nReplication Inside of Blocks\n\n\n\nThe factor structure diagrams above show examples of designs without replication inside of blocks. However, it is not uncommon to have replication within blocks. For example, Farmer John could have split each plot into 6 sub-plots, which would allow variety to be replicated twice in each plot. This poses no problem in the analysis, and in fact gives us more precision in our estimates.\nReplication of the within-block factor inside of a block also allows the estimation of the block by within-block interaction (e.g. plot by variety interaction). Usually blocks are random factors, so this likely results in an interaction between a fixed and random factor. Analyzing this type of interaction requires some additional assumptions/decisions that are not discussed in this book. For that reason, we will continue to make the assumption the block by within-block interaction effect is zero and will keep our designs simple with no replication inside of blocks."
  },
  {
    "objectID": "split_plot.html#hypothesis-and-model",
    "href": "split_plot.html#hypothesis-and-model",
    "title": "The SP/RM[1, 1] Design",
    "section": "",
    "text": "There is not a consensus on notation for a split-plot / repeated measures design with nested factors. Different textbooks will represent the models differently algebraically1. The notation selected for this text book opts for the most simple approach to subscripts.\nEach factor (i.e. meaningful partition of the data) in Figure 4 corresponds to a term on the right hand side of Equation 1.\n\\[\ny_{ijk} = \\mu + \\alpha_i + \\beta_{ij} + \\gamma_k + (\\alpha \\gamma)_{ik} + \\epsilon_{ijk}\n\\tag{1}\\]\nWhere\n\n\\(y_{ijk}\\) is the observation that belongs to level i of \\(\\alpha\\), level j of \\(\\beta\\), and level k of \\(\\gamma\\).\n\\(\\mu\\) is the grand mean of all yield data.\n\\(\\alpha_i\\) is the between-block effect (also called the whole plot effect). In the Farmer John example i goes from 1 to 2 because there are 2 levels of fungicide.\n\\(\\beta_{ij}\\) is the block effect. The double subscript ij is necessary because a block is nested inside of the between-block factor level. In the farm context, a subscript value of 12 refers to the second plot that received fungicide A; whereas a subscript of 22 refers to the second plot that received fungicide B. So, even though the value for j is two in both instances, the subscript refers to two completely different plots!\n\n\n\nBlock is the experimental unit for the between-block factor and thus serves as the error term for the between-block factor.\n\n\\(\\gamma_k\\) is the effect of the within-block factor. In the Farmer John example, k goes from 1 to 3 because there are 3 varieties of maize.\n\\((\\alpha \\gamma)_{ik}\\) represents the interaction between fungicide and variety. More generically, it represents the interaction of the between-block factor and the within-block factor.\n\\(\\epsilon_{ijk}\\) is the residual error term and is used to test the within-block factor and the interaction.\n\nA hypothesis for the main effect of fungicide type:\n\\[H_0: \\alpha_\\text{i} = 0 \\text{ for all } i\\]\n\\[H_a: \\alpha_\\text{i} \\ne 0 \\text{ for some } i\\]\nA hypothesis for the main effect of variety:\n\\[H_0: \\gamma_\\text{k} = 0 \\text{ for all } k\\]\n\\[H_a: \\gamma_\\text{k} \\ne 0 \\text{ for some } k\\]\nA hypothesis for the interaction of fungicide and variety.\n\\[\nH_0: (\\alpha\\gamma)_\\text{ik} = 0 \\text{ for all } ik\n\\]\n\\[\nH_a: (\\alpha\\gamma)_\\text{ik} \\ne 0 \\text{ for some } ik\n\\]"
  },
  {
    "objectID": "split_plot.html#assumptions",
    "href": "split_plot.html#assumptions",
    "title": "The SP/RM[1, 1] Design",
    "section": "",
    "text": "ANOVA tests are appropriate for a split-plot analysis if the following requirements are satisfied:\n\n\n\n\n\n\n\n\nRequirements\nMethod for Checking\nWhat You Hope to See\n\n\n\n\nConstant variance across factor levels\nResidual vs. Fitted Plot\nNo major disparity in vertical spread of point groupings\n\n\nThe residuals are normally distributed\nNormal Q-Q plot\nStraight line, majority of points in boundaries\n\n\nIndependent residuals\nOrder plot\nNo pattern/trend\n\n\n\nFamiliarity with/critical thinking about the experiment\nNo potential source for bias"
  },
  {
    "objectID": "split_plot.html#factor-effects",
    "href": "split_plot.html#factor-effects",
    "title": "The SP/RM[1, 1] Design",
    "section": "Factor Effects",
    "text": "Factor Effects\nWe can use our understanding of inside vs. outside factors to estimate the effect size of the grand mean, treatment factor, blocks, and residual errors. Recall the general rule for estimating effect size of a factor:\n\n\n\n\n\n\nGeneral Rule for Effect Size\n\n\n\nEffect size = Factor level mean - sum of the effects of all outside factors\n\n\nWe start by calculating factor level means.\n\nFactor Level Means\nFigure 5 shows our data set with partition lines for structural factors in place. We will proceed to calculate the factor level means for each factor.\n\n\n\n\nFigure 5: Partitioned Split Plot Fungicide Example\n\n\n\nThe grand mean is the mean of all the observations\n\\[\n\\hat{\\mu} = \\bar{y}_{\\cdot\\cdot\\cdot} = \\frac{200 + 214 + 214 + 214 + 202 + ... + 201}{24} = 209\n\\]\nNow find the mean for each level of fungicide.\n\\[\n\\bar{y}_\\text{Fung. A} = \\bar{y}_{1 \\cdot \\cdot} = \\frac{200 + 214 + 202 + 221 + 192 + 211}{6} = 206.\\overline{66}\n\\]\n\\[\n\\bar{y}_\\text{Fung. B} = \\bar{y}_{2 \\cdot \\cdot} = \\frac{214 + 214 + 220 + 204 + 215 + 201}{6}  = 211.\\overline{33}\n\\]\nNow find the mean for each plot (block).\n\\[\n\\bar{y}_\\text{Plot 1} = \\bar{y}_{1 1 \\cdot} = \\frac{200 + 202 + 192}{3}  = 198\n\\]\n\\[  \n\\bar{y}_\\text{Plot 4} = \\bar{y}_{1 2 \\cdot} = \\frac{214 + 221 + 211}{3}  = 215.\\overline{33}\n\\]\n\\[   \\bar{y}_\\text{Plot 2} = \\bar{y}_{2 1 \\cdot} = \\frac{214 + 220 + 215}{3}  = 216.\\overline{33} \\]\n\\[   \\bar{y}_\\text{Plot 3} = \\bar{y}_{2 2 \\cdot} = \\frac{214 + 204 + 201}{3}  = 206.\\overline{33} \\]\nNow find the mean for each maize variety. \\[   \\bar{y}_\\text{variety 1} = \\bar{y}_{\\cdot \\cdot 1} = \\frac{200 + 214 + 214 + 214}{4}  = 210.5 \\]\n\\[   \\bar{y}_\\text{variety 2} = \\bar{y}_{\\cdot \\cdot 2} = \\frac{202 + 221 + 220 + 204}{4}  = 211.75 \\]\n\\[   \\bar{y}_\\text{variety 3} = \\bar{y}_{\\cdot \\cdot 3} = \\frac{192 + 211 + 215 + 201}{4}  = 204.75 \\] Now find the mean for each level of the interaction between fungicide and maize variety.\n\\[   \\bar{y}_\\text{Fung. A, variety 1} = \\bar{y}_{1 \\cdot 1} = \\frac{200 + 214}{2}  = 207 \\]\n\\[   \\bar{y}_\\text{Fung. A, variety 2} = \\bar{y}_{1 \\cdot 2} = \\frac{202 + 221}{2}  = 211.5 \\]\n\\[   \\bar{y}_\\text{Fung. A, variety 3} = \\bar{y}_{1 \\cdot 3} = \\frac{192 + 211}{2}  = 201.5 \\]\n\\[   \\bar{y}_\\text{Fung. B, variety 1} = \\bar{y}_{2 \\cdot 1} = \\frac{214 + 214}{2}  = 214 \\]\n\\[   \\bar{y}_\\text{Fung. B, variety 2} = \\bar{y}_{2 \\cdot 2} = \\frac{220 + 204}{2}  = 212 \\]\n\\[   \\bar{y}_\\text{Fung. B, variety 3} = \\bar{y}_{2 \\cdot 3} = \\frac{215 + 201}{2}  = 208 \\]\nThe mean for each level of residual is simply the value of the observation itself, so that is not repeated here. The means are put in the factor level diagram in Figure 6.\n\n\n\n\nFigure 6: Factor Level Means\n\n\n\nNow that the factor level means for every factor are calculated, we can proceed to calculate factor level effects.\n\n\nFactor Level Means\nNow that we have calculated means for the levels of each factor, we can calculate the effects of the factor levels.\nThere is only one level of grand mean and there are no outside factors. Therefore, the effect due to grand mean is 22.76 (equivalent to its mean) and this effect is applied to all 24 observations.\nThe fungicide factor has two levels. The general rule for calculating factor level effects is implemented by subtracting the effect of grand mean (which is the only factor outside of fungicide) from the fungicide level mean:\n\\[\n\\alpha_\\text{Fung. A} = \\alpha_1 = 206.\\overline(6) - 209 = -2.\\overline{3}\n\\]\n\\[\n\\alpha_\\text{Fung. B} = \\alpha_2 = 211.\\overline(3) - 209 = 2.\\overline{3}\n\\]\nTo calculate the effect of plot, recall that plot is nested within fungicide. Therefore, the grand mean factor and fungicide type are both outside of plot. The sum of their factor level effects must be subtracted from the factor level mean of each plot respectively.\n\\[\n\\beta_\\text{plot 1} = \\beta_{11} = 198 - (209 + -2.\\overline{3})  = -8.\\overline{6}\n\\]\n\\[\n\\beta_\\text{plot 4} = \\beta_{11} = 215.\\overline{3} - (209 + -2.\\overline{3})  = 8.\\overline{6}\n\\]\n\\[\n\\beta_\\text{plot 2} = \\beta_{21} = 216.\\overline{3} - (209 + 2.\\overline{3})  = 5\n\\]\n\\[\n\\beta_\\text{plot 3} = \\beta_{22} = 206.\\overline{3} - (209 + 2.\\overline{3})  = -5\n\\]\nVariety is crossed with fungicide and crossed with plot, therefore, grand mean effect is the only factor outside of variety and so it is the only value subtracted from variety’s factor level mean. The effect of Variety is calculated as follows:\n\\[\n\\gamma_\\text{variety 1} = \\gamma_{1} = 210.5 - 209  = -1.5\n\\]\n\\[\n\\gamma_\\text{variety 2} = \\gamma_{2} = 211.75 - 209  = 2.75\n\\]\n\\[\n\\gamma_\\text{variety 3} = \\gamma_{3} = 204.75 - 209  = -4.25\n\\]\nThe interaction between fungicide and variety is the next variable for which to to calculate factor level effects. To calculate the interaction effects, the factors outside of the interaction must be identified. The factors used to create the interaction (fungicide and variety) will always be outside of the interaction factor itself. Is Fungicide by Variety outside of Plot? Overlay one partition of the interaction on the Plot partition and you will see it does not fit neatly inside of a partition. Each partition of the interaction actually spans across to plot partitions. Therefore, Plot is NOT outside of the interaction factor. More generally, the block factor is not outside of the interaction of between-block and within-block factors. Grand mean is outside of the interaction. The interaction level factor effects are calculated below.\n\\[\n(\\alpha \\gamma)_\\text{Fung A, variety 1} = (\\alpha \\gamma)_{11} = 207 - (209 + -2.\\overline{3} + 1.5)  = -1.\\overline{6}\n\\]\n\\[\n(\\alpha \\gamma)_\\text{Fung A, variety 2} = (\\alpha \\gamma)_{12} = 211.5 - (209 + -2.\\overline{3} + 2.75)  = -2.41\\overline{6}\n\\]\nand so on for each combination. The general formula for finding the effect of the interaction is\n\\[\n(\\alpha \\gamma)_{ik} = \\bar{y}_{i \\cdot k} - (\\bar{y}_{\\cdot \\cdot \\cdot} + \\alpha_i + \\gamma_k)\n\\] Finally, the residual effects need to be calculated. All other factors are outside of the residual factor. To calculate a residual, take the observed value and note what level it belongs to for each of the factors. Sum the effects of factor levels the observed value belongs to, then subtract that sum from the observed value.\nAs an example, one of our observations has a value of 220. The particular observation belongs to plot 2 and so received Fungicide B. Also, the observation came from Variety 2 of maize. Adding the effects from those factor levels (including the grand mean and the interaction effect) results in a predicted value of \\(217.08\\overline{3}\\):\n\\[\n209 + 2.\\overline(3) + 5 + 2.75 + -2 = 217.08\\overline{3}\n\\] To find the residual, that predicted value is subtracted from the actual value:\n\\[e_{212} = 220 - 217.08\\overline{3} = 2.91\\overline{6}\\]\nFigure 7 shows the decomposition for all the values in the dataset, with the observed value of \\(y_{212} = 220\\) and its corresponding factor level effects in bold.\n\n\n\n\nFigure 7: Decomposition Into Factor Level Effects (rounded)"
  },
  {
    "objectID": "split_plot.html#degrees-of-freedom",
    "href": "split_plot.html#degrees-of-freedom",
    "title": "The SP/RM[1, 1] Design",
    "section": "Degrees of Freedom",
    "text": "Degrees of Freedom\nIn this example, there are a total of 12 degrees of freedom to be allocated because there were 12 observations.\nAs in the case with other models, one degree of freedom is used to estimate the grand mean.\nOne can use the “counting unique factor effects” approach, or the general rule to find the degrees of freedom for the other factors.\n\n\n\n\n\n\nGeneral Rule for Degrees of Freedom\n\n\n\nDegrees of Freedom for a factor = Total levels of a factor minus the sum of the degrees of freedom of all outside factors\n\n\nThe Fungicide effect, has just two levels. If you know the effect for one level, the other fungicide’s effect can be deduced because factor level effects must sum to zero. This means fungicide has 1 degree of freedom.\nTo use the general rule to determine degrees of freedom for Fungicide you must recognize that Grand Mean is the only factor outside of Fungicide. Therefore, the calculation becomes \\(2-1 = 1\\).\nThe factors outside of Plot include Grand Mean and Fungicide. The degrees of freedom calculation for Plot, which has 4 levels, is \\(4 - (1 + 1)\\), where 1 and 1 are the degrees of freedom for Grand Mean and Fungicide respectively. Alternatively, degrees of freedom for Plot can be found by observing that factor effects of Plot sum to zero within each Fungicide partition. Thus, knowing just 2 factor effects enables you to discover the other two. The degrees of freedom for Plot are therefore 2.\nLikewise, the degrees of freedom for Variety is two because you only need to know 2 of the three effects to determine the value of the third. If the general rule is used, you start by observing that variety has 3 levels. Grand mean is the only factor outside of Variety. Therefore, the degrees of freedom for Variety is 3-1 = 2.\nThere are 6 levels to the the interaction between Fungicide and Variety. Factors outside of the interaction include: Variety (2 d.f.), Fungicide (1 d.f.) and the grand mean (1 d.f.). Using the general rule results in 6 - (2 + 1 + 1) = 2 degrees of freedom for the Fungicide by Variety interaction. effect only has 2 degrees of freedom because each of the fungicide treatment combinations must sum to zero across varieties.\nThere are two other ways to find degrees of freedom for the interaction. First, note that factor level effect sum to zero across rows and columns. Due to that constraint, after 2 of the 6 effects are known the rest become locked in. Lastly, and perhaps the most method for finding degrees of freedom for the interaction, is to multiply the degrees of freedom of the factors that were crossed to create the interaction.\nThe degrees of freedom for residuals is equal to the total number of observations minus the degrees of freedom used to estimate all the other factor effects, \\(12 - (1 + 1 + 2 + 2 + 2) = 4\\). You can also notice a pattern in the residuals and how they sum to zero. Do you see why only 4 residuals need to be known before the rest can be filled in?"
  },
  {
    "objectID": "split_plot.html#completing-the-anova-table",
    "href": "split_plot.html#completing-the-anova-table",
    "title": "The SP/RM[1, 1] Design",
    "section": "Completing the ANOVA Table",
    "text": "Completing the ANOVA Table\nNow that we have calculated degrees of freedom and effects for each factor, we can calculate the remaining pieces of the ANOVA table: Sum of Squares (SS), Mean Squares (MS), F-statistic and p-value. A completed ANOVA summary table contains the information we need for a hypothesis test of the treatment effects.\nIn an ANOVA table, each factor and their associated degrees of freedom are listed on the left. Note: the total degrees of freedom are the total number of observations.\n\n\n\n\n\nSource\ndf\nSS\nMS\nFvalue\npvalue\n\n\n\n\nGrand Mean\n1\n\n\n\n\n\n\nFungicide\n1\n\n\n\n\n\n\nPlot\n2\n\n\n\n\n\n\nVariety\n2\n\n\n\n\n\n\nFung. x Variety\n2\n\n\n\n\n\n\nResidual Error\n4\n\n\n\n\n\n\nTotal\n12\n\n\n\n\n\n\n\n\n\n\n\nTo get the sum of squares (SS) of a factor, the effects of the factor must be squared, and then summed. Figure 7 shows the effects, while Figure 8 shows the squared effects for all but the grand mean.\n\n\n\n\nFigure 8: Squared Effects (rounded)\n\n\n\nThe total sum of squares is obtained by summing the squared observations as shown in Equation 2 . This represents the total variability in the dataset that will then be allocated or partitioned to the various factors, starting with the grand mean.\n\\[\nSS_\\text{total}= 200^2 + 214^2 + ... + 201^2 = 525060\n\\tag{2}\\]\nThe grand mean squared is \\(209^2 = 43681\\) and is listed 12 times since all observations have the grand mean applied as part of their decomposition. Summing the squared effects gets:\n\\[\nSS_\\text{Grand Mean}= 209^2 * 12 = 524,172\n\\]\nSimilarly, we get the sum of squares for the remaining factors.\n\\[\\begin{align}\nSS_\\text{Fungicide} &= 12 * 5.\\overline{4} = 65.\\overline{3}\\\\\n\\\\\nSS_\\text{Plot} &= 6 * (75.\\overline{1} + 25) = 600.\\overline{6}\\\\\n\\\\\nSS_\\text{Variety} &= 4 * (2.25 + 7.5625 + 18.0625) = 111.5\\\\\n\\\\\nSS_\\text{Fung. x Variety Interaction} &= 4 * ( 1.36\\overline{1} + 4.3402\\overline{7} +  0.8402\\overline{7}) = 12.1\\overline{6}\\\\\n\\\\\nSS_\\text{Residuals} &= 2 * (2.\\overline{7} + 25 + .69\\overline{4} + 9 + .69\\overline{4} + 4) = 84.\\overline{3}\n\n\\end{align}\\]\nPutting this information into the ANOVA table gets us the result shown in Table 3.\n\n\n\n\nTable 3: Sums of Squares\n\n\nSource\ndf\nSS\nMS\nFvalue\npvalue\n\n\n\n\nGrand Mean\n1\n524172.00\n\n\n\n\n\nFungicide\n1\n65.33\n\n\n\n\n\nPlot\n2\n600.67\n\n\n\n\n\nVariety\n2\n111.50\n\n\n\n\n\nFung. x Variety\n2\n26.17\n\n\n\n\n\nResidual Error\n4\n84.33\n\n\n\n\n\nTotal\n12\n525060.00\n\n\n\n\n\n\n\n\n\n\n\nTo calculate a mean square (MS), simply divide a factor’s sum of squares by its degrees of freedom. The mean square calculations are:\n\\[\nMS_\\text{factor} = \\frac{SS_\\text{factor}}{df_\\text{factor}}\n\\]\n\n\n\n\n\nTable 4: Mean Squares\n\n\nSource\ndf\nSS\nMS\nFvalue\npvalue\n\n\n\n\nGrand Mean\n1\n524172.00\n524172.00\n\n\n\n\nFungicide\n1\n65.33\n65.33\n\n\n\n\nPlot\n2\n600.67\n300.33\n\n\n\n\nVariety\n2\n111.50\n55.75\n\n\n\n\nFung. x Variety\n2\n26.17\n13.08\n\n\n\n\nResidual Error\n4\n84.33\n21.08\n\n\n\n\nTotal\n12\n525060.00\n43755.00\n\n\n\n\n\n\n\n\n\n\nFor the structural factors in Equation 1 there are a set of hypothesis we can test using the F statistic. The first hypotheses we will address deals with the between-block factor, Fungicide.\n\\[H_0: \\alpha_1 = \\alpha_2 = 0 \\]\n\\[H_a: \\alpha_i \\ne 0 \\text{ for some }i\\]\nAt this point, it becomes super critical to recognize the two levels of experimental units and the amount of replication for each factor. For the Fungicide factor, as pointed out earlier, Plot is the experimental unit.\nThis means that the Sum of Squares for Plot is actually capturing the unexplained variance (i.e. the error) from plot to plot that couldn’t be explained by the difference in Fungicide. In other words, Plot is the error term for Fungicide. Thus, the Mean Squares for Plot should be used in the denominator of the F test for Fungicide.3 The result of this calculation is shown below (rounded to 2 decimal places).\n\\[\nF_\\text{Fungicide} = \\frac{MS_\\text{Fungicide}}{MS_\\text{Plot}} = \\frac{65.\\overline{3}}{300.\\overline{3}} = .216\n\\]\nAn F distribution is defined by two parameters: a numerator degrees of freedom and a denominator degrees of freedom. The numerator degrees of freedom is the degrees of freedom for Fungicide. The denominator in the F statistic calculation was the mean squares for Plot, which has a degrees of freedom of 2. The p-value for an F statistic of 0.216 with 1 and 2 degrees of respectively is 0.69.\nThe p-value is larger than any reasonable level of significance. We therefore have insufficient evidence to suggest that type of Fungicide has an effect on maize yield.\n\n\n\n\n\n\nF-test for between-block factor\n\n\n\nIn general terms, the F-test for the between-block is calculated as:\n\\[\nF_\\text{between-block factor} = \\frac{MS_\\text{between-block factor}}{MS_\\text{block factor}}\n\\tag{3}\\]\nThe numerator degrees of freedom for the F statistic is the degrees of freedom for between-block factor. The denominator degrees of freedom is the degrees of freedom for blocks.\n\n\nWe now work through hypotheses tests for the other terms in Equation 1. The other factors we will test all have sub-plots as their experimental units. Mean squared error represents the unexplained variance from sub-plot to sub-plot. Therefore, like we have done with all past designs, the mean squared error will be the denominator of those F tests.\n\n\nSince Plot is a random, blocking factor, we will not formally work through the hypothesis test here, even though the test is typically included in the ANOVA table.\nThey hypotheses for maize variety is,\n\\[H_0: \\gamma_1 = \\gamma_2 = \\gamma_3 = 0 \\]\n\\[H_a: \\gamma_k \\ne 0 \\text{ for some }k\\]\nand the hypothesis for the interaction of Fungicide and Variety is,\n\\[H_0: (\\alpha \\gamma)_{ik} = 0 \\text{ for all }ik\\]\n\\[H_a: (\\alpha \\gamma)_{ik} \\ne 0 \\text{ for some }ik\\].\nFor both of these tests, the F statistic is obtained by dividing the factor’s mean squares by the mean squared error.\n\\[\n\\begin{align}\nF_\\text{Variety} &= F_\\text{2,4} = \\frac{55.75}{21.08\\overline{3}} =  2.64\\\\\n\\\\\nF_\\text{Interaction} &= F_\\text{2,4} = \\frac{13.08\\overline{3}}{21.08\\overline{3}} =  0.62 \\\\\n\\\\\n\\end{align}\n\\]\nThe numerator degrees of freedom is 2 for both of these F statistics because each of the factors being tested has 2 degrees of freedom respectively. The denominator degrees of freedom for both tests is 4 because both use the mean square error in the denominator of the tests.\nThe p-values associated with these F statistics is 0.186 and 0.583 respectively. Neither factor is significant.\n\n\n\n\n\n\nInterpret the Test for Interaction First\n\n\n\nAs with other designs and models that involve an interaction, the highest order interaction should be interpreted first. If the interaction is significant, hypothesis tests for lower order interactions and main effects should not be interpreted.\nIf the interaction is not significant (as was the case here), you can proceed to evaluate hypothesis tests for lower order interactions (if there are any) and main effects (assuming all higher order interactions involving that factor are not significant).\n\n\nThe ANOVA table is now completed:\n\n\n\nMean Squares \n\n\nSource\ndf\nSS\nMS\nFvalue\npvalue\n\n\n\n\nGrand Mean\n1\n524172.00\n524172.00\n\n\n\n\nFungicide\n1\n65.33\n65.33\n0.22\n0.69\n\n\nPlot\n2\n600.67\n300.33\n14.25\n0.02\n\n\nVariety\n2\n111.50\n55.75\n2.64\n0.19\n\n\nFung. x Variety\n2\n26.17\n13.08\n0.62\n0.58\n\n\nResidual Error\n4\n84.33\n21.08\n\n\n\n\nTotal\n12\n525060.00\n43755.00"
  },
  {
    "objectID": "split_plot.html#create-the-model",
    "href": "split_plot.html#create-the-model",
    "title": "The SP/RM[1, 1] Design",
    "section": "Create the Model",
    "text": "Create the Model\nCreate the model using the aov() function. To see results of the hypothesis test for each factor you can feed your model into a summary() or anova() function.\n\nsprm_aov &lt;- aov(Y ~ between_block_factor + block + within_block_factor + between_block_factor:within_block_factor, data = YourDataSet)\nsummary(sprm_aov)\n\n\nsprm_aov is the user defined name in which the results of the aov() model are stored\nY is the name of a numeric variable in your dataset which represents the quantitative response variable.\nbetween_block_factor is the name of factor in your dataset. It represents the treatment that is applied to blocks.\nblock is a factor used to identify each block. Remember that blocks are nested within between_block_factor and serve as the experimental unit for that factor.\nwithin_block_factor is a factor whose values vary inside of each block.\nbetween_block_factor:within_block_factor is the interaction between two factors.\nYourDataSet is the name of your data set.\n\nNote that each factor should be a categorical variable. If the variable is not already a character or factor data type in R, you can convert it to a factor variable with the factor() function.\n\n\n\n\n\n\nOrder Matters\n\n\n\nThe order in which the variables are specified in the model can effect the resulting F test of the factors. The factors applied to blocks must be listed before the block variable.\nThus the model defined as\naov(yield ~ fungicide*variety + plot, data = sprm)\nstill works, but the following model does not\nsprm_aov &lt;- aov(yield ~ plot + fungicide*variety, data=sprm)\nbecause the block factor comes before the between block factor.\n\n\nThe results using R code should match the results shown in the decomposition performed earlier.\n\n\nCode\nsprm_aov &lt;- aov(yield ~ fungicide + plot + variety + fungicide:variety, data=sprm)\nsummary(sprm_aov)\n\n\n                  Df Sum Sq Mean Sq F value Pr(&gt;F)  \nfungicide          1   65.3   65.33   3.099 0.1532  \nplot               2  600.7  300.33  14.245 0.0152 *\nvariety            2  111.5   55.75   2.644 0.1854  \nfungicide:variety  2   26.2   13.08   0.621 0.5825  \nResiduals          4   84.3   21.08                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nFirst, the hypothesis test of the the interaction of fungicide and variety is evaluated, because the significance of the interaction can impact how/if the hypothesis test for the main effects is evaluated. In this case, we see that the interaction is not significant (p = .5825). We can proceed to evaluate the main effect tests.\nMaize variety (p=0.1854) is also not a significant factor in explaining yield. The p-value (p=0.1532) and F statistic for fungicide in the output are not correct.\n\n\n\n\n\n\nCorrect F Statistic and P-value for a Test of the Between Block Factor\n\n\n\nRemember, that the experimental unit for fungicide is actually plot. Therefore, the Mean Squares for plot needs to be the denominator of the F statistic for fungicide. By default, R uses the Mean Squared Residuals in the denominator, which is not correct in this case.\n\n\nThe correct F-statistic is calculated as:\n\n\nThe rationale/reasoning behind this calculation is directly connected with the explanation of Expected Mean Squares (EMS) given on the Random Factors page.\n\\[\n\\frac{MS_{fungicide}}{MS_{plot}} = \\frac{65.33}{300.33} = 0.2175\n\\]\nThe degrees of freedom for this F statistic are the degrees of freedom of it’s numerator and denominator mean squares, respectively.\n\\[\nF_{1,2} = 0.2175\n\\]\nThe area under the \\(F_{1,2}\\) distribution to the right of the F test statistic constitutes the p-value. In this case, it is p= 0.687. With such a high p-value, we conclude there is insufficient evidence that fungicide effects yield.\nThough we calculated the correct p-value by-hand, there is other R code, not covered in this book, that will calculate the correct F test by default.4\nWe are not formally conducting a hypothesis test for plot, since it is considered a nuisance factor. However, it appears to contribute a significant amount of variation to yield, as evidenced by the low p-value of 0.0152.\nA lack of significance in the factors of interest does not absolve our need to check assumptions. In order to trust these hypothesis test results we need to verify that the assumptions are met."
  },
  {
    "objectID": "split_plot.html#check-assumptions",
    "href": "split_plot.html#check-assumptions",
    "title": "The SP/RM[1, 1] Design",
    "section": "Check Assumptions",
    "text": "Check Assumptions\nFor a more detailed explanation of the code, output, and theory behind these assumptions visit the Assumptions page.\n\nConstant Error Variance\nThere needs to be constant variance across the factor levels. To verify this assumption is met, check the residual plot.\n\nplot(sprm_aov, which = 1)\n\n\n\n\nIf the constant variance assumption is violated, the points in this graph will show a wedge or megaphone shape. In other words, the vertical spread of the points would noticeably increase/decrease as we moved along the x-axis.\nIt is difficult to say whether this assumption is met. The issue is further complicated by the fact that there is a group (fungicide = B, variety = 1) where there is no variation in yield. The standard deviation of the other groups (calculated in the Numerical Summaries) are relatively close to each other. Furthermore, the sample sizes are small, making it difficult to conclude any observed differences are anything more than random chance. In summary, there is not strong evidence that the constant variance assumption is violated.\n\n\nNormally Distributed Error Term\nWe check the assumption that residuals are normally distributed with a QQ plot. Since all the points in the plot closely follow the line, we conclude this assumption is met.\n\nplot(sprm_aov, which = 2)\n\n\n\n\n\n\nIndependent Residuals\nThe dataset we are analyzing was not gathered sequentially or chronologically. The experiment with various treatment combinations was conducted simultaneously. Therefore, the order plot will not be useful to include.\nThe observations were related spatially, however. Because the model has accounted for the nested, spatial relationships by using a hierarchical model (i.e. 2 levels of experimental units), the residuals will not be correlated. Each residual will be independent, not correlated, with other residuals.\n\n\nAssumptions Summary\nThe assumptions could not be refuted, suggesting that an ANOVA model is appropriate to use in this example."
  },
  {
    "objectID": "split_plot.html#footnotes",
    "href": "split_plot.html#footnotes",
    "title": "The SP/RM[1, 1] Design",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSome textbooks prefer to represent the blocking term as another epsilon term, since it represent the random error of the experimental units of the between-block factor. In that case the two error terms may be distinguished with a superscript and look like this:\n\\[   y_{iujt} = \\mu + \\alpha_i + \\epsilon_{iu}^{W}  + \\beta_j + (\\alpha \\beta)_{ij} + \\epsilon_{jiu)}^{S}   \\]\nWhere \\(\\epsilon_{iu}^{W} \\sim N(0, \\sigma_W^2)\\), and \\(\\epsilon_{jiu}^{S} \\sim N(0, \\sigma_S^2)\\). Both error terms are mutually independent.\nWhen replication occurs within a block (whole plot), usually subscript t is used to denote each replicate.\nAnother common convention is the use of parenthesis in the subscripts to emphasize the nested relationship of factors. For example:\n\\[   y_{iuj} = \\mu + \\gamma_i + \\epsilon_{i(u)}^{W}  + \\beta_j + (\\gamma \\beta)_{ij} + \\epsilon_{j(iu)}^{S}   \\]\nHere you can see that the block factor, still represented as \\(\\epsilon^W\\) has the subscript \\(i(u)\\) to emphasize the fact that the levels of the block factor, u, are nested within the between-block factor. The parenthesis flow through to the \\(\\epsilon^S\\) as well. However, convention typically removes them from the subscript of y. Here is the expression again, this time including t subscript for within block replication.\n\\[   y_{iujt} = \\mu + \\gamma_i + \\epsilon_{i(u)}^{W}  + \\beta_j + (\\gamma \\beta)_{ij} + \\epsilon_{jt(iu)}^{S}   \\]↩︎\nIf we think of the subplots at the top as the north plot and the subplot at the bottom of each plot as the south plot, and the one in the middle as a middle plot that may introduce another source of variability. In the example so far we are ignoring the location of the sub-plot within the whole-plot. Or in other words, we are treating all 3 subplot locations as equal, not a source of variability. If the plots are small enough, it does seem reasonable to assume the effect of north vs. south will be negligible. If however, we want to address the risk that location of the subplot within the whole-plot is meaningful we must add another factor to the design. We would want to prevent “unlucky” randomizations that confound the effect of subplot plot location with variety. (If we were doing repeated measures, you can think of confounding order with the within-block factor).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the code below starts by forcing each variety to show up at least once in a North subplot (i.e. first position). This is a strategic way to ensure a better spread and prevent getting “unlucky” random assignment that preserves the bias/confounding from the unrandomized design. Then, for the fourth plot, if there are still gaps in the randomization, a strategic assignment can be made to ensure each variety is represented in each sub-plot level. Or, if there is already sufficient coverage, simply assign each variety randomly to a subplot in the 4th plot.\n\n\nCode\nset.seed(19) \nfirst3 &lt;- c(\"variety1\", sample(c(\"variety2\", \"variety3\")),    \n            \"variety2\", sample(c(\"variety1\", \"variety3\")),\n            \"variety3\", sample(c(\"variety1\", \"variety2\"))) \n\nall4 &lt;- c(first3, sample(c(\"variety1\", \"variety2\", \"variety3\")))  \n\ntibble(variety = all4) %&gt;% bind_cols(   Fungicide = rep(sample(c(\"A\", \"A\", \"B\", \"B\")), each = 3),\n                      `Whole-plot` = rep(1:4, each = 3),   \n                      yield = NA) %&gt;%\n  relocate(`Whole-plot`, Fungicide) %&gt;%\n  pander()\n\n\n\n\n\n\n\n\n\n\n\nWhole-plot\nFungicide\nvariety\nyield\n\n\n\n\n1\nB\nvariety1\nNA\n\n\n1\nB\nvariety2\nNA\n\n\n1\nB\nvariety3\nNA\n\n\n2\nB\nvariety2\nNA\n\n\n2\nB\nvariety3\nNA\n\n\n2\nB\nvariety1\nNA\n\n\n3\nA\nvariety3\nNA\n\n\n3\nA\nvariety1\nNA\n\n\n3\nA\nvariety2\nNA\n\n\n4\nA\nvariety1\nNA\n\n\n4\nA\nvariety3\nNA\n\n\n4\nA\nvariety2\nNA\n\n\n\n\n\n↩︎\nAnother way to think about the F test is a ratio of Sum of Square for a “full model” to a “reduced model”. This is explained here. In this case, the full model includes Fungicide as well as Plots. The reduced model just has Plot. The other factors (variety and the interaction) are ignored since they are at the small experimental unit.↩︎\nFor example, the following code gives the correct F test for fungicide and the other factors in the model\nmyaov&lt;- aov(yield ~ fungicide*variety + Error(plot), data = sprm)\nsummary(myaov)\nIn this case, a different class of object is created, which isn’t compatible with many of the other commands we have learned so far. Other packages are specifically designed to handle models like this, otherwise known as mixed models or hierarchical models. A list of such packages can be found here, but the simplest, most common, and easiest to use in basic cases is lme4 .↩︎"
  },
  {
    "objectID": "response_variable.html",
    "href": "response_variable.html",
    "title": "The Response Variable",
    "section": "",
    "text": "To make the research objectives measurable, it is essential that data are collected that can be used to answer the questions in the research objectives. Without a good experimental design plan, it is not unusual to see researchers collect data and then find out afterwards that the collected data do not help answer the questions defined in the research objectives.\nThe measurements that are recorded during an experiment and used to help evaluate the objectives are called the response variable (or dependent variable). Although it is possible to analyze data with multiple response variables, we will focus only on methods that require one response variable. The response variable is the measurement that measures the outcome of the research objectives and is used to help us determine how different factors influence that outcome."
  },
  {
    "objectID": "response_variable.html#footnotes",
    "href": "response_variable.html#footnotes",
    "title": "The Response Variable",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMore reading about Simpson’s Paradox can be found here and here↩︎"
  },
  {
    "objectID": "referencing_other_file.html",
    "href": "referencing_other_file.html",
    "title": "referencing another file",
    "section": "",
    "text": "Calculating treatment means for one factor:\n\nlibrary(mosaic)\nlibrary(pander)\nfavstats(Y~X, data=YourDataSet)\n\nDP Example code:\n\n\n library(mosaic) mosaic is an R Package that is useful in the teaching of statistics to beginning programmers.  library(pander) pander is an R Package that makes R output look pretty  favstats( a function from the mosaic package that returns a set of favorite summary statistics  Temp This is our response variable. From ?airquality you can see in the help file that Temp is the maximum daily temp in degrees F at La Gaurdia Aiport during 1973  ~ “~” is the tilde symbol. It can be interpreted as “y broken down by x”; “y modeled by x”; “y explained by x”, etc. Where y is on the left of the tilde and x is on the right.   Month, “Month” is a column from the airquality dataset that can be treated as qualitative.  data = airquality You have to tell R what dataset the variables Temp and Month come from. ‘airquality’ is a preloaded dataset in R.   ) Functions must always end with a closing parenthesis.   Click to view output  Click to View Output."
  },
  {
    "objectID": "randomization.html",
    "href": "randomization.html",
    "title": "Randomization: Sampling and Assignment",
    "section": "",
    "text": "Samples should be representative of the population as defined in the research objectives. When the sample is different from the population in a systematic way, then bias occurs. Biased samples may still contain useful information, but the population which the sample proports to represent may be different than the actual population represented. Common sources of bias include:\n\nSelection bias – the sampling plan excludes part of the population during selection. Samples taken out of convenience or only from volunteers are examples of possible selection bias.\nMeasurement bias – the measurement method results in measurements that are different from the true values of the response variable. Measurement bias can be caused by uncalibrated equipment, poorly trained personnel, or any step along the experimental process that is not carefully designed or clearly defined.\nNon-response bias – this occurs when data are not available for all the individuals in the sample. If those individuals missing data would respond in a different way than those that did respond, then a bias is introduced.\n\nCare should be taken to minimize bias.\n\n\n\n\n\n\nWarning\n\n\n\nLarger sample sizes are not a remedy for biased samples."
  },
  {
    "objectID": "randomization.html#footnotes",
    "href": "randomization.html#footnotes",
    "title": "Randomization: Sampling and Assignment",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nCobb, G.W. Introduction to Design and Analysis of Experiments. Wiley, 2014.↩︎"
  },
  {
    "objectID": "multiple_comparisons.html",
    "href": "multiple_comparisons.html",
    "title": "Multiple Comparison",
    "section": "",
    "text": "In this page, the approach to dealing with multiple contrasts is explained. Specifically, whether and how to adjust the tests of multiple contrasts to account for an inflated family wise error rate. A few techniques are described, with a focus on when to use which technique. Lastly, instructions and example code is provided for carrying out each technique in R.\nIn most good experiments, researchers are interested in more than just one contrast. Conducting multiple tests on the levels of a factor can inflate the family wise Type I error rate, as illustrated in the “Multiple T-tests” section of the ANOVA page. There is considerable disagreement among statisticians about how to approach the issue of multiple tests. The debate primarily focuses on whether to proactively take steps that will mitigate the inflated family wise error rate or not. To further complicate matters, if an adjustment is desired, there are multiple techniques to choose from. An exhaustive presentation of the arguments on either side is not attempted in this text. Though, a few of the key ideas will naturally emerge as the different approaches are discussed.\n\n\nThe terms “contrast” and “comparison” are treated here as synonymous. Comparison is used more often to refer to testing a difference in factor level means. Read Contrasts for more explanation of these terms."
  },
  {
    "objectID": "multiple_comparisons.html#bonferroni",
    "href": "multiple_comparisons.html#bonferroni",
    "title": "Multiple Comparison",
    "section": "Bonferroni",
    "text": "Bonferroni\nThe Bonferroni adjustment is best for a handful of pre-planned contrasts. As the number of contrasts grows, the adjustment quickly becomes too conservative (i.e. makes it too hard to find significance). In that case, other methods may strike a better balance between Type I and Type II errors.\n\n\nType I error occurs when a true hypothesis is rejected. Type II error occurs when a false hypothesis is not rejected.\nIt can be shown that in order to not exceed a desired family wise error rate of \\(\\alpha_\\text{fw}\\), for a set of \\(k\\) contrasts, each individual contrast should be tested at a significance level of \\(\\frac{\\alpha_\\text{fw}}{k}\\). For example, if we desire a family wise error rate of 0.05 and plan to do 5 tests, each individual contrast must have a p-value less than \\(\\frac{0.05}{5} = 0.01\\) to be considered significant. The test itself has not changed, only the benchmark p-value for claiming significance. This adjustment is easy to understand and to calculate."
  },
  {
    "objectID": "multiple_comparisons.html#scheffé",
    "href": "multiple_comparisons.html#scheffé",
    "title": "Multiple Comparison",
    "section": "Scheffé",
    "text": "Scheffé\nUsing experimental results to suggest which contrasts to test is often referred to as “data snooping” or exploratory analysis. For example, you may look at graphical and numerical summaries to see which means (or combinations of means) will be promising to test. The problem with this approach is that you have essentially done a quick, informal test of many differences when you looked at descriptive statistics and plots. In other words, you have already tested the means and combinations of the means in your mind. The Type I error rate of the contrasts will be different (higher) than stated because you are only formally applying the test to those that look significant already.\nScheffé’s method is most useful when many contrasts are tested, especially when going beyond pairwise comparisons of factor level means to test combined factor levels (i.e. complex comparisons). In data exploration, there are theoretically an infinite number of contrasts you could test. This is not a problem for Scheffé’s adjustment because, unlike other adjustment techniques, Scheffé’s adjustment does not depend on the number of comparisons to be made. Rather, it is determined only by the number of factor levels and the number of observations.\nThe F statistic used to test a contrast in Scheffé’s adjustment is related to the omnibus F test for the factor itself, and is given by:\n\\[\nF_\\text{Scheffé} = (k-1)*F\n\\]\nWhere \\(F\\) is the statistic for the F test of the factor as usual. \\(k\\) is the number of levels in that factor. \\(F_\\text{Scheffé}\\) has \\(df_\\text{numerator} = k-1\\) and \\(df_\\text{denominator} = df_\\text{residual}\\). Scheffé’s test output is often in terms of a t test. Recall that the t statistic is simply the square root of the F statistic.\nIf you are interested in only a specific set of hypotheses (all pairwise comparisons, or all treatment levels vs. control, or all levels compared to the “best” level, etc.) there may be another adjustment technique that will provide better statistical power. Two of which are mentioned below."
  },
  {
    "objectID": "multiple_comparisons.html#methods-designed-for-all-pairwise-comparisons",
    "href": "multiple_comparisons.html#methods-designed-for-all-pairwise-comparisons",
    "title": "Multiple Comparison",
    "section": "Methods Designed for All Pairwise Comparisons",
    "text": "Methods Designed for All Pairwise Comparisons\nWhether it is an exploratory analysis or a pre-planned set of contrasts, many researchers want to test all factor level means against each other. This is usually referred to as testing all pairwise comparisons. Because this situation is so common, two approaches are described below.\n\nTukey’s HSD\nIf a multiple comparison adjustment is desired for testing all pairwise comparisons, a standard approach is to apply Tukey’s Honest Significant Difference (HSD) technique. Occasionally, in the case of few factor levels, Bonferroni’s adjustment may result in more significant findings. If that is the case, use Bonferroni’s correction instead.\nThe calculation for this test is based on the distribution of \\(Q\\). The test statistic \\(Q\\) is sometimes called the studentized range distribution. “Range” is a reference to the numerator where the difference between a maximum and a minimum is calculated. “Studentized” because we are dividing by the estimated standard error, a technique for standardizing famously employed by Student’s (a.k.a. William Gossett) t test. \\(Q\\) is calculated as:\n\\[\nQ = \\frac{max(T_i) - min(T_i)}{\\sqrt{\\frac{MSE}{n}}}\n\\tag{1}\\]\n\n\\(MSE\\) is an estimate of the random error variance\n\\(n\\) is the number of replicates at each factor level. For unequal sample sizes the formula changes somewhat and the result of this test may have a lower Type I error rate than claimed\nThe observations for each level are generated by a different random variable: \\(k\\) variables total, one for each level. Each random variable is normally distributed with mean zero and standard deviation estimated by the denominator of Equation 1. \\(max(T_i)\\) is the maximum value of the \\(k\\) random variables, \\(min(T_i)\\) is the minimum.\nThe distribution of \\(Q\\) will depend on the number of treatments being compared, \\(k\\), and the number of degrees of freedom for error.\n\n\n\nFisher’s LSD\nMaking adjustments for multiple contrasts is a conservative approach, meaning it is more difficult to claim significance compared to when no adjustment is made. Though these adjustments prevent understating the probability of Type I error, they increase the probability of Type II error for each individual contrast. In exploratory analysis where you are looking for hints of what to study further, a Type II error may be a greater concern than Type I.\nFor example, consider a screening study intended to narrow down the number of factors studied in a future experiment. In this case, accidentally ruling out something early on that actually does have a significant impact on the response is more egregious than letting a non-significant factor through to the next experiment where it’s non-significance will be discovered.\nFisher’s Least Significant Difference (LSD) employs no adjustment at all to the pairwise comparisons. However, before proceeding to test pairwise comparisons, the F test for the factor must be significant. Using the less powerful F test as a gatekeeper to the more powerful pairwise t tests serves as a partial protection against extreme Type I errors inflation.\nIn summary, Fisher’s LSD is a two step process. First, verify the F test for the factor is significant. Second, if it is, proceed with all pairwise comparisons without any adjustment. Fisher’s LSD tends to be used in studies where many factors are present, especially screening/exploratory studies."
  },
  {
    "objectID": "multiple_comparisons.html#bonferonni",
    "href": "multiple_comparisons.html#bonferonni",
    "title": "Multiple Comparison",
    "section": "Bonferonni",
    "text": "Bonferonni\nThere are two ways to implement the Bonferroni adjustment illustrated below using the contrasts that were tested in the R Instructions section of the Contrast page.\n\nRecalculate an Alpha Level by Hand\nFirst, the Bonferroni adjustment is shown using output provided for a test of contrasts without adjustment. The code and output from the R Instructions section of the Contrast page is provided again here for convenience. These two contrasts test the mean of the manual brush against the oscillating brush mean, as well as the manual brush mean against the mean of all the other brushes combined.\n\n\nCode\ncontrast_results &lt;- contrast(brush_means,\n                             list(man_v_osc = c(1,-1,0,0),\n                                  man_v_others = c(1,-(1/3),-(1/3),-(1/3))), \n                             adjust=\"none\")\n\n#kable commands are for formatting the output\ncontrast_results |&gt; \n  kable(digits = 2) |&gt; \n  kable_styling(full_width = TRUE)\n\n\n\n\n\ncontrast\nestimate\nSE\ndf\nt.ratio\np.value\n\n\n\n\nman_v_osc\n3.12\n1.58\n20\n1.97\n0.06\n\n\nman_v_others\n0.44\n1.29\n20\n0.34\n0.74\n\n\n\n\n\n\n\n\nA new alpha level against which to compare the p-values can be calculated by hand. Since there are two tests, if the intent was to keep the family wise error rate of 0.05, the alpha level for each individual test is \\(\\frac{0.05}{2} = 0.025\\). To be considered significant, the contrast’s p-value must be less than 0.025.\nman_v_osc has a p-value of 0.06; man_v_others has a p-value of 0.74. Since neither contrast has a p-value less than 0.025 we conclude that the contrasts are not statistically significant.\n\n\nMake the Adjustment in R\nSecond, the Bonferroni adjustment is applied by changing the value of the adjust = argument in the contrast() function from “none” to “bon”.\n\n\nCode\nbon_results &lt;- contrast(brush_means,\n                        list(man_v_osc = c(1,-1,0,0),\n                             man_v_others = c(1,-(1/3),-(1/3),-(1/3))), \n                        adjust=\"bon\")\n\n#kable commands are for formatting the output\nbon_results |&gt; \n  kable(digits = 2) |&gt; \n  kable_styling(full_width = TRUE)\n\n\n\n\n\ncontrast\nestimate\nSE\ndf\nt.ratio\np.value\n\n\n\n\nman_v_osc\n3.12\n1.58\n20\n1.97\n0.13\n\n\nman_v_others\n0.44\n1.29\n20\n0.34\n1.00\n\n\n\n\n\n\n\n\nIn these results the p-values have simply been multiplied by 2 because there were 2 contrasts in the set being tested. Each test can be compared to our desired family wise error rate as usual. Since each test has a p-value greater than 0.05 we conclude that the contrasts are not statistically significant.\nNote, for man_v_others the p-value is capped at 1.00 since a p-value cannot exceed 1.00."
  },
  {
    "objectID": "multiple_comparisons.html#scheffé-1",
    "href": "multiple_comparisons.html#scheffé-1",
    "title": "Multiple Comparison",
    "section": "Scheffé",
    "text": "Scheffé\nSimilar to how the Bonferroni adjustment was applied, to apply a Scheffé adjustment, change the value of the adjust = argument in the contrast() function from “none” to “scheffe”.\n\n\nCode\ncontrast(brush_means, \n         list(man_v_osc = c(1,-1,0,0),\n              man_v_others = c(1,-(1/3),-(1/3),-(1/3))),\n         adjust=\"scheffe\") |&gt; \n\n  #kable commands are for formatting the output\n  kable(digits = 2) |&gt; \n  kable_styling(full_width = TRUE)\n\n\n\n\n\ncontrast\nestimate\nSE\ndf\nt.ratio\np.value\n\n\n\n\nman_v_osc\n3.12\n1.58\n20\n1.97\n0.17\n\n\nman_v_others\n0.44\n1.29\n20\n0.34\n0.94\n\n\n\n\n\n\n\n Each contrast’s p-value can be compared to our desired family wise error rate of 0.05. Since each test has a p-value greater than 0.05 we conclude that the contrasts are not statistically significant. Notice the p-values with Scheffé’s adjustment are higher than the p-values with Bonferroni’s adjustment. This will always be the case if the number of contrasts being tested is small."
  },
  {
    "objectID": "multiple_comparisons.html#tukey",
    "href": "multiple_comparisons.html#tukey",
    "title": "Multiple Comparison",
    "section": "Tukey",
    "text": "Tukey\nIn our toothbrush example, if we want to compare each factor level mean to the other we can apply Tukey’s HSD (Honest Significant Different) adjustment. To do this, instead of inputting custom contrasts to the contrast() command in R, we alter the method argument to be pairwise. We also need to change the adjust argument to tukey. The output provides an estimate of each pairwise comparison, as well as adjusted p-values.\n\n\nFor Tukey adjustment to all pairwise comparisons, this base R shortcut can also be used. The arguments to the function are the name of the model and the name of the factor whose factor level means should be tested.\n\nTukeyHSD(plaque_aov, \"Brush\")\n\n\n\nCode\ncontrast(brush_means, method = \"pairwise\", adjust = \"tukey\")\n\n\n contrast                 estimate   SE df t.ratio p.value\n Manual - Oscillating        3.117 1.58 20   1.967  0.2332\n Manual - Sonic              0.418 1.58 20   0.264  0.9933\n Manual - Ultrasonic        -2.220 1.58 20  -1.401  0.5130\n Oscillating - Sonic        -2.698 1.58 20  -1.703  0.3480\n Oscillating - Ultrasonic   -5.337 1.58 20  -3.369  0.0149\n Sonic - Ultrasonic         -2.638 1.58 20  -1.666  0.3670\n\nP value adjustment: tukey method for comparing a family of 4 estimates \n\n\nWith a p-value of 0.0149, only the Ultrasonic-Oscillating contrast is significant at the 0.05 level. This significance is driven by the large difference in means between the two levels, Ultrasonic’s mean is 5.337 higher than Oscillating. All other pairwise comparisons have p-values greater than 0.05 and so are not considered significant.\n\n\n\n\n\n\nConfidence Intervals Instead of p-values\n\n\n\nIf confidence intervals are desired instead of p-values, the output of the contrast() command can be saved to an object, which then becomes the input to the confint() command. This is true for ANY contrasts computed with emmeans::contrast(), not just Tukey and not just pairwise comparisons. The adjustment specified in contrast() is then applied to the confidence interval as well.\n\n\nCode\nfor_ci &lt;- contrast(brush_means, method = \"pairwise\", adjust = \"tukey\") \nconfint(for_ci)\n\n\n contrast                 estimate   SE df lower.CL upper.CL\n Manual - Oscillating        3.117 1.58 20    -1.32    7.550\n Manual - Sonic              0.418 1.58 20    -4.02    4.852\n Manual - Ultrasonic        -2.220 1.58 20    -6.65    2.214\n Oscillating - Sonic        -2.698 1.58 20    -7.13    1.735\n Oscillating - Ultrasonic   -5.337 1.58 20    -9.77   -0.903\n Sonic - Ultrasonic         -2.638 1.58 20    -7.07    1.795\n\nConfidence level used: 0.95 \nConf-level adjustment: tukey method for comparing a family of 4 estimates"
  },
  {
    "objectID": "multiple_comparisons.html#sec-fisher",
    "href": "multiple_comparisons.html#sec-fisher",
    "title": "Multiple Comparison",
    "section": "Fisher",
    "text": "Fisher\nFirst, look at the ANOVA summary table to see if the F test for brush is significant.\n\n\nCode\nsummary(plaque_aov)\n\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)  \nBrush        3  86.31  28.769   3.822 0.0258 *\nResiduals   20 150.56   7.528                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nThe p-value for Brush is significant at the 0.05 level. You can then proceed with an un-adjusted test of all pairwise comparisons for the Brush factor. The adjustment argument in the contrast() command in this case is none.\n\n\nCode\ncontrast(brush_means, method = \"pairwise\", adjust = \"none\")\n\n\n contrast                 estimate   SE df t.ratio p.value\n Manual - Oscillating        3.117 1.58 20   1.967  0.0632\n Manual - Sonic              0.418 1.58 20   0.264  0.7944\n Manual - Ultrasonic        -2.220 1.58 20  -1.401  0.1764\n Oscillating - Sonic        -2.698 1.58 20  -1.703  0.1040\n Oscillating - Ultrasonic   -5.337 1.58 20  -3.369  0.0031\n Sonic - Ultrasonic         -2.638 1.58 20  -1.666  0.1114\n\n\n The p-value for Oscillating - Ultrasonic is 0.0031 and is the only significant pairwise comparison at the 0.05 level; though Manual - Oscillating is close, with a p-value of 0.0632."
  },
  {
    "objectID": "multiple_comparisons.html#bonus-estimating-and-testing-effect-sizes",
    "href": "multiple_comparisons.html#bonus-estimating-and-testing-effect-sizes",
    "title": "Multiple Comparison",
    "section": "Bonus: Estimating and Testing Effect Sizes",
    "text": "Bonus: Estimating and Testing Effect Sizes\nIf no custom contrasts are provided to the contrast() command, and no argument for method is provided, the default output is an estimate (and test) of each factor level effect, as seen below!\n\n\nCode\n(effect_sizes &lt;- contrast(brush_means)) \n\n\n contrast           estimate   SE df t.ratio p.value\n Manual effect        0.3287 0.97 20   0.339  0.9273\n Oscillating effect  -2.7879 0.97 20  -2.874  0.0323\n Sonic effect        -0.0896 0.97 20  -0.092  0.9273\n Ultrasonic effect    2.5488 0.97 20   2.627  0.0323\n\nP value adjustment: fdr method for 4 tests \n\n\n\n\nNote the behavior of the defaults is a little different than what might be expected. “fdr” stands for “false detection rate”. It makes adjustments to keep the overall proportion of Type I errors fixed (e.g. 5% of tests will be a Type I error). This is somewhat less stringent than keeping the family-wise Type I error rate fixed.\nYou can use confint() to get confidence intervals for the effects (with or without adjustments) as well.\n\n\nCode\nconfint(effect_sizes)\n\n\n contrast           estimate   SE df lower.CL upper.CL\n Manual effect        0.3287 0.97 20   -2.333    2.991\n Oscillating effect  -2.7879 0.97 20   -5.450   -0.126\n Sonic effect        -0.0896 0.97 20   -2.752    2.573\n Ultrasonic effect    2.5488 0.97 20   -0.113    5.211\n\nConfidence level used: 0.95 \nConf-level adjustment: bonferroni method for 4 estimates"
  },
  {
    "objectID": "key_principles.html",
    "href": "key_principles.html",
    "title": "Key Principles of Experimentation",
    "section": "",
    "text": "When researchers are faced with a question, their response is often to collect data so they can understand answers to that question. While this seems like a great idea, without a plan, it is likely to be inefficient and may lead to erroneous conclusions. Experimental design is the process of creating a plan, implementing that plan, analyzing the results, and making proper conclusions. This plan creates an efficient and effective road map that will help the researcher better understand the problem and the variables that they have questions about. A well-designed experiment will allow the researcher to minimize the effect of variables that are not of interest and reduce unwanted variability, making it easier to better understand the effect of variables that are of interest.\nThis textbook will explore the necessary steps to consider as you plan and perform an experiment. These steps include:\n\nUnderstand the objectives of your research. What questions should be answered?\n\nDetermine the measurements that will be needed to help answer the questions of interest.\nDetermine which variables, which we will call factors, are the sources of variation in the measurements. Determine which of these factors you will be testing and decide which levels of those factors will be included in the study. Determine which of these factors you will hold constant.\nDetermine how sampling and treatment assignments will be done and then collect the data.\nIdentify the analysis technique that will be applied, including how to test hypotheses\nDescribe the data using numerical and graphical summaries.\nPerform the analysis and make sure that the analysis is appropriate.\nMake conclusions.\n\nThis textbook will further discuss each of these steps in detail and offer examples. Steps 1-4 will be discussed in the “Basics of Design” section. Steps 5, 7, and 8 will be discussed in the “Specific Designs” and “Broad Topics” sections. Step 6 will be discussed in the “R Instructions” section.\nBefore detailing each of these steps, it is important to understand the differences between observational studies and designed experiments. In observational studies the researchers do not control the conditions of the study. The data are collected by observation. In this case, the researchers cannot assign the subjects to the conditions and factors in the study. An example of this would be studying to see if there is a difference in the amount of wind at the three Brigham Young University (BYU) campuses. In this study the factor is campus. The campus factor has 3 levels: BYU, BYU-Idaho, and BYU-Hawaii. The measurement taken would be the average wind speed for a particular day. The researchers cannot assign the particular days to a specific campus. Instead, they may select random days and then collect the data for those days. This data would be collected through observation, not through experimentation.\nDesigned experiments are studies in which the researchers control the conditions in which the study is performed. In designed experiments the researchers assign subjects to levels of a factor. The method used in making those assignments will be discussed further in the “Design” section of a Specific Design’s page. An example of this would be determining which of four different toothbrush types are better at reducing plaque. The researchers would randomly assign each subject to one of the toothbrush types and then carry out the experiment.\nExperiments have the advantage over observational studies in isolating a factor’s effect on a response, and thereby proving causality. Sometimes however it is not ethical to assign someone to a condition that is of interest, for example you should not assign anyone to experience the effects of smoking - even if you wanted to study the impact of smoking. In some cases, it may be impossible to assign someone to a condition, for example gender. Furthermore, an observational study may be preferred because it may be a more realistic view of how something will truly play out “in the real world”, rather than in a contrived lab experiment.\nAlthough both types of studies can be used to better understand and answer research questions, this textbook will mostly focus on the steps needed to best design an experiment. The toothbrush study just introduced above will be used to help illustrate how to work thru the process of designing an experiment."
  },
  {
    "objectID": "hoveRmd/DescribeData_backup.html",
    "href": "hoveRmd/DescribeData_backup.html",
    "title": "Describing Data",
    "section": "",
    "text": "In the following explanations\n\nY must be a “numeric” vector of the quantitative response variable.\nX is a qualitative variable. It would represent a treatment factor.\nYourDataSet is the name of your data set.\n\nYou can take a tidyverse approach or a mosaic package approach to calculating numerical summaries for each treatment.\n\nmosaic package:tidyverse approach\n\n\nCalculating treatment means for one factor:\n\nlibrary(mosaic)\nlibrary(pander)\nfavstats(Y~X, data=YourDataSet)\n\nExample code:\n\n\n library(mosaic) mosaic is an R Package that is useful in the teaching of statistics to beginning programmers.  library(pander) pander is an R Package that makes R output look pretty  favstats( a function from the mosaic package that returns a set of favorite summary statistics  Temp This is our response variable. From ?airquality you can see in the help file that Temp is the maximum daily temp in degrees F at La Gaurdia Aiport during 1973  ~ “~” is the tilde symbol. It can be interpreted as “y broken down by x”; “y modeled by x”; “y explained by x”, etc. Where y is on the left of the tilde and x is on the right.   Month, “Month” is a column from the airquality dataset that can be treated as qualitative.  data = airquality You have to tell R what dataset the variables Temp and Month come from. ‘airquality’ is a preloaded dataset in R.   ) Functions must always end with a closing parenthesis.   Click to view output  Click to View Output. \n\n\n\n\n\n  Month min   Q1 median    Q3 max     mean       sd  n missing\n1     5  56 60.0     66 69.00  81 65.54839 6.854870 31       0\n2     6  65 76.0     78 82.75  93 79.10000 6.598589 30       0\n3     7  73 81.5     84 86.00  92 83.90323 4.315513 31       0\n4     8  72 79.0     82 88.50  97 83.96774 6.585256 31       0\n5     9  63 71.0     76 81.00  93 76.90000 8.355671 30       0\n\n\n\nWhen calculating treatment means for combinations of 2 or more factors you can use + or | to separate the factors. | (read as ‘vertical bar’ or ‘pipe’) has the advantage that in addition to calculating means for every factor level combination, favstats will also output the marginal means for each level of the last factor listed.\nNOTE: unlike it’s use in the aov() command, using the * within favstats does not yield expected results and should NOT be used.\n\nlibrary(mosaic)\nfavstats(Y ~ X + Z, data = YourDataSet) \n#OR\nfavstats(Y ~ X | Z, data = YourDataSet)\n\nExample code:\n\n\n library(mosaic) mosaic is an R Package that is useful in the teaching of statistics to beginning programmers.  favstats( a function from the mosaic package that returns a set of favorite summary statistics  mpg This is a quantitative variable (numerical vector) from the mtcars dataset  ~ “~” is the tilde symbol. It can be interpreted as “y broken down by x”; “y modeled by x”; “y explained by x”, etc. Where y is on the left of the tilde and x variables are on the right.   am A qualitative variable from the mtcars dataset. It is coded as 0 and 1 and so therefore is treated as numeric. That is a key distinction when creating the model, but it does not matter when calling favstats().  + This allows us to create additional subgroups within ‘am’ for each level of ‘cyl’.  cyl, A variable from the mtcars dataset with 3 distinct values: 4, 6, and 8. Though it is a numeric column we want to treat it as a factor. This is a key distinction when creating the model, but it does not matter when calling favstats().  data = mtcars You have to tell R what dataset the variables ‘mpg’, ‘am’, and ‘cyl’ come from. ‘mtcars’ is a preloaded dataset in R.   ) Functions must always end with a closing parenthesis.   Click to view output  Click to View Output. \n\n\n\n\n\n  am.cyl  min     Q1 median     Q3  max     mean        sd  n missing\n1    0.4 21.5 22.150  22.80 23.600 24.4 22.90000 1.4525839  3       0\n2    1.4 21.4 25.200  28.85 30.900 33.9 28.07500 4.4838599  8       0\n3    0.6 17.8 18.025  18.65 19.750 21.4 19.12500 1.6317169  4       0\n4    1.6 19.7 20.350  21.00 21.000 21.0 20.56667 0.7505553  3       0\n5    0.8 10.4 14.050  15.20 16.625 19.2 15.05000 2.7743959 12       0\n6    1.8 15.0 15.200  15.40 15.600 15.8 15.40000 0.5656854  2       0\n\n\n\nNotice that the first column in the output contains the factor level combinations of am and cyl. So, ‘0.4’ is interpreted as level 0 for am and level 4 for cyl. Or in other words, the summary statistics on that row are for automatic tranmission, 4 cylinder engine vehicles. The column label of ‘am.cyl’ indicates which factor is represented on which side of the period. The next example uses the same way of labeling the factor level combinations, but the column label is not as intuitive or helpful.\n\n\n library(mosaic) mosaic is an R Package that is useful in the teaching of statistics to beginning programmers.  favstats( a function from the mosaic package that returns a set of favorite summary statistics  mpg This is a quantitative variable (numerical vector) from the mtcars dataset  ~ “~” is the tilde symbol. It can be interpreted as “y broken down by x”; “y modeled by x”; “y explained by x”, etc. Where y is on the left of the tilde and x variables are on the right.   am A qualitative variable from the mtcars dataset. It is coded as 0 and 1 and so therefore is treated as numeric. That is a key distinction when creating the model, but it does not matter when calling favstats().  | Referred to as a vertical bar or pipe, this symbol further defines subgroups of the variable on its left, using the values of the variable on its right side   cyl, A variable from the mtcars dataset with 3 distinct values: 4, 6, and 8. Though it is a numeric column we want to treat it as a factor. This is a key distinction when creating the model, but it does not matter when calling favstats().  data = mtcars You have to tell R what dataset the variables ‘mpg’, ‘am’, and ‘cyl’ come from. ‘mtcars’ is a preloaded dataset in R.   ) Functions must always end with a closing parenthesis.   Click to View Output  Click to View Output. \n\n\n\n\n\n  cyl  min     Q1 median     Q3  max     mean        sd  n missing\n1 0.4 21.5 22.150  22.80 23.600 24.4 22.90000 1.4525839  3       0\n2 1.4 21.4 25.200  28.85 30.900 33.9 28.07500 4.4838599  8       0\n3 0.6 17.8 18.025  18.65 19.750 21.4 19.12500 1.6317169  4       0\n4 1.6 19.7 20.350  21.00 21.000 21.0 20.56667 0.7505553  3       0\n5 0.8 10.4 14.050  15.20 16.625 19.2 15.05000 2.7743959 12       0\n6 1.8 15.0 15.200  15.40 15.600 15.8 15.40000 0.5656854  2       0\n7   4 21.4 22.800  26.00 30.400 33.9 26.66364 4.5098277 11       0\n8   6 17.8 18.650  19.70 21.000 21.4 19.74286 1.4535670  7       0\n9   8 10.4 14.400  15.20 16.250 19.2 15.10000 2.5600481 14       0\n\n\n\n\n\n\nCalculating treatment means for one factor:\n\nlibrary(tidyverse)\nYourDataSet %&gt;%\n  Group_by(X) %&gt;%\n  Summarise(MeanY = mean(Y), sdY = sd(Y), sampleSize = n()) \n\n\n\n library(tidyverse) tidyverse is an R Package that is very useful for working with data.  airquality airquality is a dataset in R.   %&gt;%  The pipe operator that will send the airquality dataset down inside of the code on the following line.     group_by( “group_by” is a function from library(tidyverse) that allows us to split the airquality dataset into “little” datasets, one dataset for each value in the “Month” column.  Month “Month” is a column from the airquality dataset that can be treated as qualitative.  ) Functions must always end with a closing parenthesis.   %&gt;%  The pipe operator that will send the grouped version of the airquality dataset down inside of the code on the following line.     summarise( “summarise” is a function from library(tidyverse) that allows us to compute numerical summaries on data.  aveTemp =  “AveTemp” is just a name we made up. It will contain the results of the mean(…) function.  mean( “mean” is an R function used to calculate the mean.  Temp Temp is a quantitative variable (numeric vector) from the airquality dataset.  ) Functions must always end with a closing parenthesis.  ) Functions must always end with a closing parenthesis.      \nPress Enter to run the code.   Click to View Output  Click to View Output. \n\n\n\n\n\n\n\n\n\n\n\n\nMonth\naveTemp\n\n\n\n\n5\n65.55\n\n\n6\n79.1\n\n\n7\n83.9\n\n\n8\n83.97\n\n\n9\n76.9\n\n\n\n\n\nNote that R calculated the mean Temp for each month in Month from the airquality dataset.\nMay (5), June (6), July (7), August (8), and September (9), respectively.\nFurther, note that to get the “nicely formatted” table, you would have to use\nlibrary(pander)\nairquality %&gt;% \n  group_by(Month) %&gt;%\n  summarise(aveTemp = mean(Temp)) %&gt;%\n  pander()\n\nTo calculate treatment means for each combination of factor levels of 2 or more factors, simply add the additional variable to the group_by() statement.\nExample code:\n\n\n library(tidyverse) tidyverse is an R Package that is very useful for working with data.  mpg mtcars is a dataset in preloaded in R.   %&gt;%  The pipe operator that will send the mtcars dataset down inside of the code on the following line.     group_by( “group_by” is a function from library(tidyverse) that allows us to split the mtcars dataset into “little” datasets, one dataset for each combination of values in the ‘am’ and ‘cyl’ variables  am, cyl ‘am’ and ‘cyl’ are both columns in mtcars. By listing them both here we are going to get output for each combination of ‘am’ and ‘cyl’ that exists in the dataset  ) Functions must always end with a closing parenthesis.   %&gt;%  The pipe operator that will send the grouped version of the airquality dataset down inside of the code on the following line.     summarise( “summarise” is a function from library(tidyverse) that allows us to compute numerical summaries on data.  mean_mpg =  “mean_mpg” is just a name we made up. It will contain the results of the mean(…) function.  mean( “mean” is an R function used to calculate the mean.  mpg mpg is a quantitative variable (numeric vector) from the mtcars dataset.  ) Functions must always end with a closing parenthesis.  ) Functions must always end with a closing parenthesis.      \nPress Enter to run the code.   Click to View Output  Click to View Output. \n\n\n\n\n\n`summarise()` has grouped output by 'am'. You can override using the `.groups`\nargument.\n\n\n\n\n\n\n\n\n\n\nam\ncyl\nmean_mpg\n\n\n\n\n0\n4\n22.9\n\n\n0\n6\n19.12\n\n\n0\n8\n15.05\n\n\n1\n4\n28.07\n\n\n1\n6\n20.57\n\n\n1\n8\n15.4"
  },
  {
    "objectID": "hoveRmd/DescribeData_backup.html#numerical-summaries",
    "href": "hoveRmd/DescribeData_backup.html#numerical-summaries",
    "title": "Describing Data",
    "section": "",
    "text": "In the following explanations\n\nY must be a “numeric” vector of the quantitative response variable.\nX is a qualitative variable. It would represent a treatment factor.\nYourDataSet is the name of your data set.\n\nYou can take a tidyverse approach or a mosaic package approach to calculating numerical summaries for each treatment.\n\nmosaic package:tidyverse approach\n\n\nCalculating treatment means for one factor:\n\nlibrary(mosaic)\nlibrary(pander)\nfavstats(Y~X, data=YourDataSet)\n\nExample code:\n\n\n library(mosaic) mosaic is an R Package that is useful in the teaching of statistics to beginning programmers.  library(pander) pander is an R Package that makes R output look pretty  favstats( a function from the mosaic package that returns a set of favorite summary statistics  Temp This is our response variable. From ?airquality you can see in the help file that Temp is the maximum daily temp in degrees F at La Gaurdia Aiport during 1973  ~ “~” is the tilde symbol. It can be interpreted as “y broken down by x”; “y modeled by x”; “y explained by x”, etc. Where y is on the left of the tilde and x is on the right.   Month, “Month” is a column from the airquality dataset that can be treated as qualitative.  data = airquality You have to tell R what dataset the variables Temp and Month come from. ‘airquality’ is a preloaded dataset in R.   ) Functions must always end with a closing parenthesis.   Click to view output  Click to View Output. \n\n\n\n\n\n  Month min   Q1 median    Q3 max     mean       sd  n missing\n1     5  56 60.0     66 69.00  81 65.54839 6.854870 31       0\n2     6  65 76.0     78 82.75  93 79.10000 6.598589 30       0\n3     7  73 81.5     84 86.00  92 83.90323 4.315513 31       0\n4     8  72 79.0     82 88.50  97 83.96774 6.585256 31       0\n5     9  63 71.0     76 81.00  93 76.90000 8.355671 30       0\n\n\n\nWhen calculating treatment means for combinations of 2 or more factors you can use + or | to separate the factors. | (read as ‘vertical bar’ or ‘pipe’) has the advantage that in addition to calculating means for every factor level combination, favstats will also output the marginal means for each level of the last factor listed.\nNOTE: unlike it’s use in the aov() command, using the * within favstats does not yield expected results and should NOT be used.\n\nlibrary(mosaic)\nfavstats(Y ~ X + Z, data = YourDataSet) \n#OR\nfavstats(Y ~ X | Z, data = YourDataSet)\n\nExample code:\n\n\n library(mosaic) mosaic is an R Package that is useful in the teaching of statistics to beginning programmers.  favstats( a function from the mosaic package that returns a set of favorite summary statistics  mpg This is a quantitative variable (numerical vector) from the mtcars dataset  ~ “~” is the tilde symbol. It can be interpreted as “y broken down by x”; “y modeled by x”; “y explained by x”, etc. Where y is on the left of the tilde and x variables are on the right.   am A qualitative variable from the mtcars dataset. It is coded as 0 and 1 and so therefore is treated as numeric. That is a key distinction when creating the model, but it does not matter when calling favstats().  + This allows us to create additional subgroups within ‘am’ for each level of ‘cyl’.  cyl, A variable from the mtcars dataset with 3 distinct values: 4, 6, and 8. Though it is a numeric column we want to treat it as a factor. This is a key distinction when creating the model, but it does not matter when calling favstats().  data = mtcars You have to tell R what dataset the variables ‘mpg’, ‘am’, and ‘cyl’ come from. ‘mtcars’ is a preloaded dataset in R.   ) Functions must always end with a closing parenthesis.   Click to view output  Click to View Output. \n\n\n\n\n\n  am.cyl  min     Q1 median     Q3  max     mean        sd  n missing\n1    0.4 21.5 22.150  22.80 23.600 24.4 22.90000 1.4525839  3       0\n2    1.4 21.4 25.200  28.85 30.900 33.9 28.07500 4.4838599  8       0\n3    0.6 17.8 18.025  18.65 19.750 21.4 19.12500 1.6317169  4       0\n4    1.6 19.7 20.350  21.00 21.000 21.0 20.56667 0.7505553  3       0\n5    0.8 10.4 14.050  15.20 16.625 19.2 15.05000 2.7743959 12       0\n6    1.8 15.0 15.200  15.40 15.600 15.8 15.40000 0.5656854  2       0\n\n\n\nNotice that the first column in the output contains the factor level combinations of am and cyl. So, ‘0.4’ is interpreted as level 0 for am and level 4 for cyl. Or in other words, the summary statistics on that row are for automatic tranmission, 4 cylinder engine vehicles. The column label of ‘am.cyl’ indicates which factor is represented on which side of the period. The next example uses the same way of labeling the factor level combinations, but the column label is not as intuitive or helpful.\n\n\n library(mosaic) mosaic is an R Package that is useful in the teaching of statistics to beginning programmers.  favstats( a function from the mosaic package that returns a set of favorite summary statistics  mpg This is a quantitative variable (numerical vector) from the mtcars dataset  ~ “~” is the tilde symbol. It can be interpreted as “y broken down by x”; “y modeled by x”; “y explained by x”, etc. Where y is on the left of the tilde and x variables are on the right.   am A qualitative variable from the mtcars dataset. It is coded as 0 and 1 and so therefore is treated as numeric. That is a key distinction when creating the model, but it does not matter when calling favstats().  | Referred to as a vertical bar or pipe, this symbol further defines subgroups of the variable on its left, using the values of the variable on its right side   cyl, A variable from the mtcars dataset with 3 distinct values: 4, 6, and 8. Though it is a numeric column we want to treat it as a factor. This is a key distinction when creating the model, but it does not matter when calling favstats().  data = mtcars You have to tell R what dataset the variables ‘mpg’, ‘am’, and ‘cyl’ come from. ‘mtcars’ is a preloaded dataset in R.   ) Functions must always end with a closing parenthesis.   Click to View Output  Click to View Output. \n\n\n\n\n\n  cyl  min     Q1 median     Q3  max     mean        sd  n missing\n1 0.4 21.5 22.150  22.80 23.600 24.4 22.90000 1.4525839  3       0\n2 1.4 21.4 25.200  28.85 30.900 33.9 28.07500 4.4838599  8       0\n3 0.6 17.8 18.025  18.65 19.750 21.4 19.12500 1.6317169  4       0\n4 1.6 19.7 20.350  21.00 21.000 21.0 20.56667 0.7505553  3       0\n5 0.8 10.4 14.050  15.20 16.625 19.2 15.05000 2.7743959 12       0\n6 1.8 15.0 15.200  15.40 15.600 15.8 15.40000 0.5656854  2       0\n7   4 21.4 22.800  26.00 30.400 33.9 26.66364 4.5098277 11       0\n8   6 17.8 18.650  19.70 21.000 21.4 19.74286 1.4535670  7       0\n9   8 10.4 14.400  15.20 16.250 19.2 15.10000 2.5600481 14       0\n\n\n\n\n\n\nCalculating treatment means for one factor:\n\nlibrary(tidyverse)\nYourDataSet %&gt;%\n  Group_by(X) %&gt;%\n  Summarise(MeanY = mean(Y), sdY = sd(Y), sampleSize = n()) \n\n\n\n library(tidyverse) tidyverse is an R Package that is very useful for working with data.  airquality airquality is a dataset in R.   %&gt;%  The pipe operator that will send the airquality dataset down inside of the code on the following line.     group_by( “group_by” is a function from library(tidyverse) that allows us to split the airquality dataset into “little” datasets, one dataset for each value in the “Month” column.  Month “Month” is a column from the airquality dataset that can be treated as qualitative.  ) Functions must always end with a closing parenthesis.   %&gt;%  The pipe operator that will send the grouped version of the airquality dataset down inside of the code on the following line.     summarise( “summarise” is a function from library(tidyverse) that allows us to compute numerical summaries on data.  aveTemp =  “AveTemp” is just a name we made up. It will contain the results of the mean(…) function.  mean( “mean” is an R function used to calculate the mean.  Temp Temp is a quantitative variable (numeric vector) from the airquality dataset.  ) Functions must always end with a closing parenthesis.  ) Functions must always end with a closing parenthesis.      \nPress Enter to run the code.   Click to View Output  Click to View Output. \n\n\n\n\n\n\n\n\n\n\n\n\nMonth\naveTemp\n\n\n\n\n5\n65.55\n\n\n6\n79.1\n\n\n7\n83.9\n\n\n8\n83.97\n\n\n9\n76.9\n\n\n\n\n\nNote that R calculated the mean Temp for each month in Month from the airquality dataset.\nMay (5), June (6), July (7), August (8), and September (9), respectively.\nFurther, note that to get the “nicely formatted” table, you would have to use\nlibrary(pander)\nairquality %&gt;% \n  group_by(Month) %&gt;%\n  summarise(aveTemp = mean(Temp)) %&gt;%\n  pander()\n\nTo calculate treatment means for each combination of factor levels of 2 or more factors, simply add the additional variable to the group_by() statement.\nExample code:\n\n\n library(tidyverse) tidyverse is an R Package that is very useful for working with data.  mpg mtcars is a dataset in preloaded in R.   %&gt;%  The pipe operator that will send the mtcars dataset down inside of the code on the following line.     group_by( “group_by” is a function from library(tidyverse) that allows us to split the mtcars dataset into “little” datasets, one dataset for each combination of values in the ‘am’ and ‘cyl’ variables  am, cyl ‘am’ and ‘cyl’ are both columns in mtcars. By listing them both here we are going to get output for each combination of ‘am’ and ‘cyl’ that exists in the dataset  ) Functions must always end with a closing parenthesis.   %&gt;%  The pipe operator that will send the grouped version of the airquality dataset down inside of the code on the following line.     summarise( “summarise” is a function from library(tidyverse) that allows us to compute numerical summaries on data.  mean_mpg =  “mean_mpg” is just a name we made up. It will contain the results of the mean(…) function.  mean( “mean” is an R function used to calculate the mean.  mpg mpg is a quantitative variable (numeric vector) from the mtcars dataset.  ) Functions must always end with a closing parenthesis.  ) Functions must always end with a closing parenthesis.      \nPress Enter to run the code.   Click to View Output  Click to View Output. \n\n\n\n\n\n`summarise()` has grouped output by 'am'. You can override using the `.groups`\nargument.\n\n\n\n\n\n\n\n\n\n\nam\ncyl\nmean_mpg\n\n\n\n\n0\n4\n22.9\n\n\n0\n6\n19.12\n\n\n0\n8\n15.05\n\n\n1\n4\n28.07\n\n\n1\n6\n20.57\n\n\n1\n8\n15.4"
  },
  {
    "objectID": "hoveRmd/DescribeData_backup.html#graphical-summaries",
    "href": "hoveRmd/DescribeData_backup.html#graphical-summaries",
    "title": "Describing Data",
    "section": "Graphical Summaries",
    "text": "Graphical Summaries\n\nBoxplots\n\n\n\n\n\n\n\n\nOverviewR InstructionsExplanation\n\n\n\nGraphical depiction of the five-number summary. Great for comparing the distributions of data across several groups or categories. Provides a quick visual understanding of the location of the median as well as the range of the data. Can be useful in showing outliers. Sample size should be larger than at least five, or computing the five-number summary is not very meaningful.\n\n\n\n\n\nBase R ggplot2 plotly\n\n\n\nTo make a boxplot in R use the function:\nboxplot(object)\nTo make side-by-side boxplots:\nboxplot(object ~ group, data=NameOfYourData, ...)\n\nobject must be quantitative data. R refers to this as a “numeric vector.”\ngroup must be qualitative data. R refers to this as either a “character vector” or a “factor.” However, a “numeric vector” can also act as a qualitative variable.\nNameOfYourData is the name of the dataset containing object and group.\n... implies there are many other options that can be given to the boxplot() function. Type ?boxplot in your R Console for more details.\n\nExample Code\nBasic Single Boxplot\n\n\n boxplot An R function “boxplot” used to create boxplots.  ( Parenthesis to begin the function. Must touch the last letter of the function.  airquality “airquality” is a dataset. Type “View(airquality)” in R to see it.  $ The $ allows us to access any variable from the airquality dataset.  Temp “Temp” is a quantitative variable (numeric vector) from the “airquality” dataset.  )\nClosing parenthesis for the function.      \nPress Enter to run the code.   …  Click to View Output. \n\n\n\n\n\n\n\n\n\n\nMore Useful… Basic Side-by-Side Boxplot\n\n\n boxplot An R function “boxplot” used to create boxplots.  ( Parenthesis to begin the function. Must touch the last letter of the function.  Temp “Temp” is a quantitative variable (numeric vector) from the “airquality” dataset.   ~  The ~ is used to tell R that you want one boxplot of the quantitative variable (“Temp”) for each group found in the qualitative variable (“Month”).  Month “Month” is a qualitative variable (in this case a “numeric vector” defining months by 5, 6, 7, 8, and 9) from the “airquality” dataset.  , \nThe “,” is required to start specifying additional commands for the “boxplot()” function.  data=airquality data= is used to tell R that the “Temp” and “Month” variables are located in the airquality dataset. Without this, R will not know where to find “Temp” and “Month” and the command will give an error.  ) Functions always end with a closing parenthesis.      \nPress Enter to run the code.   …  Click to View Output. \n\n\n\n\n\n\n\n\n\n\nAdd Names under each Box\n\n\n boxplot An R function “boxplot” used to create boxplots.  ( Parenthesis to begin the function. Must touch the last letter of the function.  Temp “Temp” is a quantitative variable (numeric vector) from the “airquality” dataset.   ~  The ~ is used to tell R that you want one boxplot of the quantitative variable (“Temp”) for each group found in the qualitative variable (“Month”).  Month “Month” is a qualitative variable (in this case a “numeric vector” defining months by 5, 6, 7, 8, and 9) from the “airquality” dataset.  , \nThe “,” is required to start specifying additional commands for the “boxplot()” function.  data=airquality data= is used to tell R that the “Temp” and “Month” variables are located in the airquality dataset. Without this, R will not know where to find “Temp” and “Month” and the command will give an error.  , \nThe “,” is required to start specifying additional commands for the “boxplot()” function.  names=c(“May”,“June”,“July”,“Aug”,“Sep”) names= is used to tell R what labels to place on the x-axis below each boxplot.  ) Functions always end with a closing parenthesis.      \nPress Enter to run the code.   …  Click to View Output. \n\n\n\n\n\n\n\n\n\n\nAdd Color and Labels\n\n\n boxplot(Temp ~ Month, data=airquality This code was explained in the previous example code.  ,  The comma is used to separate each additional command to a function.  xlab=“Month of the Year” xlab= stands for “x label.” Use it to specify the text to print on the plot under the x-axis. The desired text must always be contained in quotes.  ,  The comma is used to separate each additional command to a function.  ylab=“Temperature” ylab= stands for “y label.” Use it to specify the text to print on the plot next to the y-axis. The desired text must always be contained in quotes.  ,  The comma is used to separate each additional command to a function.  main=“La Guardia Airport Daily Temperatures” main= stands for the “main label” of the plot, which is placed at the top center of the plot. The desired text must always be contained in quotes.  ,  The comma is used to separate each additional command to a function.  col=“wheat” col= stands for the “color” of the plot. The color name “wheat” is an available color in R. Type colors() in the R Console to see more options. The color name must always be placed in quotes.  ) Functions always end with a closing parenthesis.      \nPress Enter to run the code.   …  Click to View Output. \n\n\n\n\n\n\n\n\n\n\n\n\n\nTo make a boxplot in R using the ggplot approach, first ensure\nlibrary(ggplot2)\nis loaded. Then,\nggplot(data, aes(x=groupsColumn, y=dataColumn) +\n  geom_boxplot()\n\ndata is the name of your dataset.\ngroupsColumn is a column of data from your dataset that is qualitative and represents the groups that should each have a boxplot.\ndataColumn is a column of data from your dataset that is quantitative.\nThe aesthetic helper function aes(x= , y=) is how you tell the gpplot to make the x-axis have the values in your groupsColumn of data, the y-axis become your dataColumn. Note if groupsColumn is not a factor, use factor(groupsColumn) instead.\nThe geometry helper function geom_boxplot() causes the ggplot to become a boxplot.\n\n\nExample Code\nBasic Single Boxplot\n\n\n ggplot An R function “ggplot” used to create a framework for a graphic that will have elements added to it with the + sign.  ( Parenthesis to begin the function. Must touch the last letter of the function.  airquality “airquality” is a dataset. Type “View(airquality)” in R to see it.  ,  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice.  aes( The aes or “aesthetics” function allows you to tell the ggplot how it should appear. This includes things like what the y-axis should become.  y=Temp “y=” declares which variable will become the y-axis of the graphic.  )\nClosing parenthesis for the aes function.  )\nClosing parenthesis for the ggplot function.   +  The addition symbol + is used to add further elements to the ggplot.     geom_boxplot() The “geom_boxplot()” function causes the ggplot to become a boxplot. There are many other “geom_” functions that could be used.      \nPress Enter to run the code.   …  Click to View Output. \n\n\n\n\n\n\n\n\n\n Side-by-side Boxplot and Color Change\n\n\n ggplot An R function “ggplot” used to create a framework for a graphic that will have elements added to it with the + sign.  ( Parenthesis to begin the function. Must touch the last letter of the function.  airquality “airquality” is a dataset. Type “View(airquality)” in R to see it.  ,  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice.  aes( The aes or “aesthetics” function allows you to tell the ggplot how it should appear. This includes things like what the x-axis or y-axis should become.  x=factor(Month),  “x=” declares which variable will become the x-axis of the graphic. Since Month is “numeric” we must use “factor(Month)” instead of just “Month”.  y=Temp “y=” declares which variable will become the y-axis of the graphic.  )\nClosing parenthesis for the aes function.  )\nClosing parenthesis for the ggplot function.   +  The addition symbol + is used to add further elements to the ggplot.     geom_boxplot( The “geom_boxplot()” function causes the ggplot to become a boxplot. There are many other “geom_” functions that could be used.  fill=“skyblue”,  The “fill” command controls the color of the insides of each box in the boxplot.  color=“black” The “color” command controls the color of the edges of each box.  )\nClosing parenthesis for the geom_boxplot function.      \nPress Enter to run the code.   …  Click to View Output. \n\n\n\n\n\n\n\n\n\n\nAdd Labels \n\n ggplot An R function “ggplot” used to create a framework for a graphic that will have elements added to it with the + sign.  ( Parenthesis to begin the function. Must touch the last letter of the function.  airquality “airquality” is a dataset. Type “View(airquality)” in R to see it.  ,  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice.  aes( The aes or “aesthetics” function allows you to tell the ggplot how it should appear. This includes things like what the x-axis or y-axis should become.  x=factor(Month),  “x=” declares which variable will become the x-axis of the graphic. Since Month is “numeric” we must use “factor(Month)” instead of just “Month”.  y=Temp “y=” declares which variable will become the y-axis of the graphic.  )\nClosing parenthesis for the aes function.  )\nClosing parenthesis for the ggplot function.   +  The addition symbol + is used to add further elements to the ggplot.     geom_boxplot( The “geom_histogram()” function causes the ggplot to become a histogram. There are many other “geom_” functions that could be used.  fill=“skyblue”,  The “fill” command controls the color of the insides of each box.  color=“black” The “color” command controls the color of the edges of each box.  )\nClosing parenthesis for the geom_boxplot function.   +  The addition symbol + is used to add further elements to the ggplot.     labs( The “labs” function is used to add labels to the plot, like a main title, x-label and y-label.  title=“La Guardia Airport Daily Mean Temperature”,  The “title=” command allows you to control the main title at the top of the graphic.  x=“Month of the Year”,  The “x=” command allows you to control the x-label of the graphic.  y=“Daily Mean Temperature” The “y=” command allows you to control the y-label of the graphic.  )\nClosing parenthesis for the labs function.      \nPress Enter to run the code.   …  Click to View Output. \n\n\n\n\n\n\n\n\n\n\nGallery\nSee what past students have done…\nClick to view.\n\nHover to see code.\n \n ggplot(data = mtcars, aes(x = as.factor(cyl), y = mpg, fill=as.factor(cyl))) +  geom_boxplot()  +  stat_summary(fun.y = mean, geom = \"errorbar\", aes(ymax = ..y.., ymin = ..y..),     width = .75, linetype = \"dashed\", color=\"firebrick\") +  theme_light() +  theme(panel.grid.major=element_blank()) +  scale_fill_brewer(palette=\"Dark2\") +  geom_jitter(width=0.1, height=0) +  labs(title = \"Miles Per Gallon Based on Cylinders\",     x=\"Number of Cylinders\",     fill=\"Cylinders\",     y=\"Miles Per Gallon\")   \n \n ggplot(data = ToothGrowth, aes(x = as.factor(dose), y = len, fill=as.factor(dose))) +  geom_boxplot( )  +  facet_wrap(~supp) +  theme_bw() +  scale_fill_manual(values=c(\"#999999\", \"#E69F00\", \"#56B4E9\")) +  geom_jitter(width=0.1, height=0) +  labs(title = \"Tooth Length Based on Doses     According to Supplement Type\",     fill=\"Doses\",     x=\"Dosage Amount(mg)\",     y=\"Tooth Length\" )   \n\n\n\n\n\nTo make a histogram in plotly first load\nlibrary(plotly)\nThen, use the function:\nplot_ly(dataName, y=~columnNameY, x=~columnNameX, type=\"box\")\n\ndataName is the name of a data set\ncolumnNameY must be the name of a column of quantitative data. R refers to this as a “numeric vector.” This will become the y-axis of the plot.\ncolumnNameX must be the name of a column of qualitative data. This will provide the “groups” forming each individual box in the boxplot.\ntype=\"box\" tells the plot_ly(…) function to create a boxplot.\n\nVisit plotly.com/r/box-plots for more details.\n\nExample Code\nHover your mouse over the example codes to learn more. Click on them to see what they create.\nBasic Boxplot\n\n\n plot_ly An R function “plot_ly” from library(plotly) used to create any plotly plot.  ( Parenthesis to begin the function. Must touch the last letter of the function.  airquality,  “airquality” is a dataset. Type “View(airquality)” in R to see it.  y= The y= allows us to declare which column of the data set will become the y-axis of the boxplot. In other words, the quantitative data we are interested in studying for each group.  ~Temp,   “Temp” is a quantitative variable (numeric vector) from the “airquality” dataset. The ~ is required before column names inside all plot_ly(…) commands.  x= The x= allows us to declare which column of the data set will become the x-axis of the boxplot. In other words, the “groups” forming each separate box in the boxplot.  ~as.factor(Month),   since “Month” is a quantitative variable (numeric vector) from the “airquality” dataset we have to change it to a “factor” which forces R to treat it as a qualitative (groups) variable. The ~ is required before column names inside all plot_ly(…) commands.  type=“box” This option tells the plot_ly(…) function what “type” of graph to make. In this case, a boxplot.  )\nClosing parenthesis for the plot_ly function.      \nPress Enter to run the code.   …  Click to View Output. \n\n\n\n\n\n\n\n\n\n\n\nChange Color\n\n\n plot_ly(airquality, y=~Temp, x=~as.factor(Month), type=“box”,  This code was explained in the first example code.  fillcolor=“skyblue”,  this changes the fill color of the boxes in the boxplot to the color specified, in this case “skyblue.”  line=list(color=“darkgray”, width=3),  this “list(…)” of options that will be specified will effect the edges of the boxes in the boxplot. We are changing their color to “darkgray” and their width to 3 pixels wide.  marker=list( this “list(…)” of options that will be specified will effect the outlying dots shown in the boxplots beyond the “fences” of each box.  color = “orange”,  this will change the color of the dots to orange.  line = list(,  this opens a list of options to specify for the “lines” around the “markers.”  color = “red”,  this will change the color of the lines around the outlier dots to red.  width = 1 this will change the width of the lines around the outlier dots to 1 pixel.  ) Functions always end with a closing parenthesis.  ) Functions always end with a closing parenthesis.  ) Functions always end with a closing parenthesis.      \nPress Enter to run the code.   …  Click to View Output. \n\n\n\n\n\n\n\n\n\n\n\nAdd Titles\n\n\n plot_ly(airquality, y=~Temp, x=~as.factor(Month), type=“box”, fillcolor=“skyblue”, line=list(color=“darkgray”, width=3), marker = list(color=“orange”, line = list(color=“red”, width=1)))  This code was explained in the above example code.  %&gt;% the pipe operator sends the completed plot_ly(…) code into the layout function.  layout( The layout(…) function is used for specifying details about the axes and their labels.  title=“La Guardia Airport Daily Mean Temperatures” This declares a main title for the top of the graph.  xaxis=list( This declares a list of options to be specified for the xaxis. The same can be done for the yaxis(…).  title=“Month of the Year” This declares a title underneath the x-axis.  ),  Functions always end with a closing parenthesis.  yaxis=list( This declares a list of options to be specified for the y-axis.  title=“Temperature in Degrees F” This declares a title beside the y-axis.  ) Functions always end with a closing parenthesis.  ) Functions always end with a closing parenthesis.      \nPress Enter to run the code.   …  Click to View Output. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding how a boxplot is created is the best way to understand what the boxplot shows.\n\nHow Boxplots are Made\n\nThe five-number summary is computed.\nA box is drawn with one edge located at the first quartile and the opposite edge located at the third quartile.\nThis box is then divided into two boxes by placing another line inside the box at the location of the median.\nThe maximum value and minimum value are marked on the plot.\nWhiskers are drawn from the first quartile out towards the minimum and from the third quartile out towards the maximum.\nIf the minimum or maximum is too far away, then the whisker is ended early.\nAny points beyond the line ending the whisker are marked on the plot as dots. This helps identify possible outliers in the data.\n\n\n\n\n\n\n\n\n\nScatterplot, with Means\n\n\n\n\n\n\n\n\nOverviewR Instructions\n\n\nScatterplots of a catgorical variable on the x axis and quantitative variable on the y axis are sometimes called strip charts, or side-by-side strip charts. When sample sizes are not too big and there are not too many repeated value this type of chart is an excellent way to see the variability in the data without the abstraction of a boxplot. By plotting individual observations you also can see the size of the sample for each factor level. Including factor level means on the plot adds additional insight. The mean of each factor level is often connected with a line for visual impact.\n\n\n\n\nmosaic ggplot2 plotly\n\n\n\nTo make a scatterplot use the function:\nxyplot(y~x, data = mydata)\n\ny is the quantitative response variable, i.e., “numeric vector.”\nx is the independent, explanatory variable\nmydata is the name of the dataset containing y and x.\n\nThis will return a scatterplot regardless of how your x variable is stored in R (numeric, character, factor). This function is flexible and with minimal effort can include averages or make an interaction plot. xyplot() is a part of the lattice package, which is loaded when the mosaic package is loaded.\nNote: plot() from base R will also give a scatterplot, but only if the x variable is quantitative. If x is a character or factor variable the default is to return a boxplot plot.\nExample Code\nBy reading the documentation for the dataset (visible when ?ToothGrowth is run in the console) we can see that our “x” variable, “dose”, is a numeric variable in R. Because of this the x-axis maintains the relative distance of doses on the x-axis.\n\n\n\n xyplot An R function used to create a scatterplot.  ( Parenthesis to begin the function. Must touch the last letter of the function.  len “len” is a quantitative variable (numeric vector) from the “ToothGrowth” dataset that is being used as the response variable (y-axis) for this plot.   ~  The ~ is used to tell R that you want a scatterplot with the quantitative variable “len” on the y-axis and the variable “dose” on the x-axis.  dose “dose” is a quantitative variable (numeric vector) from the “ToothGrowth” dataset that is being used as the explanatory variable (x-axis) for this plot.  , \nThe “,” is required to start specifying additional commands for the function.  data=ToothGrowth data= is used to tell R that the “len” and “dose” variables are located in the ToothGrowth dataset. Without this, R will not know where to find “len” and “dose”. Learn more about the dataset by entering ?ToothGrowth into the console.  , \nThe “,” is required to start specifying additional commands for the function.  xlab = “Vitamin C dose in mg/day” xlab= allows you to specify an x-axis label. The units mg/day is explained in the dataset documentation, visible when ?ToothGrowth is run in the console.   , \nThe “,” is required to start specifying additional commands for the function.  ylab = “Length” ylab= allows you to specify an y-axis label.  ) Functions always end with a closing parenthesis.      \nPress Enter to run the code.   …  Click to View Output. \n\n\n\n\n\n\n\n\n\nIf you start with a numeric x variable, you may or may not want to convert it to a factor variable. Doing so will make each factor level, or category, equally spaced along the x-axis. It also “cleans up” the axis by removing additional tic-marks and axis values that aren’t needed. Compare the output of the previous example code with this example code. This example code converts our x variable of “dose” to a factor variable.\n\n\n\n xyplot An R function used to create a scatterplot.  ( Parenthesis to begin the function. Must touch the last letter of the function.  len “len” is a quantitative variable (numeric vector) from the “ToothGrowth” dataset that is being used as the response variable (y-axis) for this plot.   ~  The ~ is used to tell R that you want a scatterplot with the quantitative variable “len” on the y-axis and the variable “dose” on the x-axis.  factor An R function to convert how data is stored in a variable. The variable input into this function will now be treated as a factor variable in this command  ( Parenthesis to begin the function. Must touch the last letter of the function.  dose “dose” is a quantitative variable (numeric vector) from the “ToothGrowth” dataset that is being used as the explanatory variable (x-axis) for this plot. Because it is an input to the factor function it will be treated as a factor variable  ) Functions always end with a closing parenthesis.  , \nThe “,” is required to start specifying additional commands for the function.  data=ToothGrowth data= is used to tell R that the “len” and “dose” variables are located in the ToothGrowth dataset. Without this, R will not know where to find “len” and “dose”. Learn more about the dataset by entering ?ToothGrowth into the console.  , \nThe “,” is required to start specifying additional commands for the function.  xlab = “Vitamin C dose in mg/day” xlab= allows you to specify an x-axis label. The units mg/day is explained in the dataset documentation, visible when ?ToothGrowth is run in the console.   , \nThe “,” is required to start specifying additional commands for the function.  ylab = “Length” ylab= allows you to specify an y-axis label.  ) Functions always end with a closing parenthesis.      \nPress Enter to run the code.   …  Click to View Output. \n\n\n\n\n\n\n\n\n\nTo include the means on the plot and connect them with a line use this code\n\n\n\n xyplot An R function used to create a scatterplot.  ( Parenthesis to begin the function. Must touch the last letter of the function.  len “len” is a quantitative variable (numeric vector) from the “ToothGrowth” dataset that is being used as the response variable (y-axis) for this plot.   ~  The ~ is used to tell R that you want a scatterplot with the quantitative variable “len” on the y-axis and the variable “dose” on the x-axis.  factor An R function to convert how data is stored in a variable. The variable input into this function will now be treated as a factor variable in this command  ( Parenthesis to begin the function. Must touch the last letter of the function.  dose “dose” is a quantitative variable (numeric vector) from the “ToothGrowth” dataset that is being used as the explanatory variable (x-axis) for this plot. Because it is an input to the factor function it will be treated as a factor variable  ) Functions always end with a closing parenthesis.  , \nThe “,” is required to start specifying additional commands for the function.  data=ToothGrowth data= is used to tell R that the “len” and “dose” variables are located in the ToothGrowth dataset. Without this, R will not know where to find “len” and “dose”. Learn more about the dataset by entering ?ToothGrowth into the console.  , \nThe “,” is required to start specifying additional commands for the function.  xlab = “Vitamin C dose in mg/day” xlab= allows you to specify an x-axis label. The units mg/day is explained in the dataset documentation, visible when ?ToothGrowth is run in the console.   , \nThe “,” is required to start specifying additional commands for the function.  ylab = “Length” ylab= allows you to specify an y-axis label.  , \nThe “,” is required to start specifying additional commands for the function.  type =\nThe type argument allows you to add different types of lines to the plot. Run ?panel.xyplot() to read more about values for this argument  c( Combines the following values into one vector. This allows me to pass multiple values as one input to “type =”. Useful for if I want to plot something in addition to the default of plotting points.  ‘p’\nThis requests the points to be plotted. It is the default value. Other acceptable values for the type argument can be seen by running ?panel.xyplot() in the console.  , \nThe “,” is required to start specifying additional commands for the function.  ‘a’ This requests the average for each factor level to be connected with a line. Other acceptable values for the type argument can be seen by running ?panel.xyplot() in the console.  ) Functions always end with a closing parenthesis.      \nPress Enter to run the code.   …  Click to View Output. \n\n\n\n\n\n\n\n\n\n\n\n\n\nTo make a scatterplot in R using the ggplot approach, first ensure\nlibrary(ggplot2)\nis loaded. Then,\nggplot(data, aes(x=groupsColumn, y=dataColumn) +\n  geom_point()\n\ndata is the name of your dataset.\ngroupsColumn is a column of data from your dataset that is qualitative and represents the groups that should each have a boxplot.\ndataColumn is a column of data from your dataset that is quantitative.\nThe aesthetic helper function aes(x= , y=) is how you tell the gpplot to make the x-axis have the values in your groupsColumn of data, the y-axis become your dataColumn. Note if groupsColumn is not a factor, use factor(groupsColumn) instead.\nThe geometry helper function geom_point() causes the ggplot to become a scatterplot; or in other words to draw points to represent data.\n\n\nExample Code\n\n\n ggplot An R function “ggplot” used to create a framework for a graphic that will have elements added to it with the + sign.  ( Parenthesis to begin the function. Must touch the last letter of the function.  ToothGrowth “ToothGrowth” is a dataset. Type “View(ToothGrowth)” in R to see it.  ,  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice.  aes( The aes or “aesthetics” function allows you to tell the ggplot how it should appear. This includes things like what the x-axis or y-axis should become.  x=dose,  “x=” declares which variable will become the x-axis of the graphic.  y=len “y=” declares which variable will become the y-axis of the graphic.  )\nClosing parenthesis for the aes function.  )\nClosing parenthesis for the ggplot function.   +  The addition symbol + is used to add further elements to the ggplot.     geom_boxplot( The “geom_point()” function causes the ggplot to draw data as points, thus it become a scatterplot. There are many other “geom_” functions that could be used.  color=“blue” The “color” command controls the color of the points. It can be confusing to know when to use “color” vs. “fill”.  )\nClosing parenthesis for the geom_boxplot function.      \nPress Enter to run the code.   …  Click to View Output. \n\n\n\n\n\n\n\n\n\n If you start with a numeric x variable, you may or may not want to convert it to a factor variable. You do this by using ‘factor(x)’ instead of just ‘x’ as shown below. Doing so will make each factor level, or category, equally spaced along the x-axis. It also “cleans up” the axis by removing additional tic-marks and axis values that aren’t needed.\nggplot(ToothGrowth, aes(x = factor(dose), y = len)) +   geom_point(color = \"blue\")\nAdding averages to the plot and connecting them with a line requires a little more effort and is demonstrated in the code below. I also add some more descriptive labels to the chart.\nNote the use of stat_summary to indicate I want to add a layer that plots a numerical summary, not the original data. Some geoms have stat summaries built in to them (like geom_bar or geom_boxplot), but in our case we have to define the summary.\nIn the stat_summary I provide additional arguments to the aesthetics helper function. Defining the aesthetics in ggplot() is like a global definition, all additional layers inherit those aesthetic mappings. Defining them in a geom_* or a stats_* allows you to add to or override what was defined in ggplot() for that layer only. The group aesthetic is required in order to use a line geometry. In this case, group could just as easily have been defined in ggplot(aes()).\n\n\n ggplot An R function “ggplot” used to create a framework for a graphic that will have elements added to it with the + sign.  ( Parenthesis to begin the function. Must touch the last letter of the function.  ToothGrowth “ToothGrowth” is a dataset. Type “View(ToothGrowth)” in R to see it.  ,  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice.  aes( The aes or “aesthetics” function allows you to tell the ggplot how it should appear. This includes things like what the x-axis or y-axis should become.  x=dose,  “x=” declares which variable will become the x-axis of the graphic.  y=len “y=” declares which variable will become the y-axis of the graphic.  )\nClosing parenthesis for the aes function.  )\nClosing parenthesis for the ggplot function.   +  The addition symbol + is used to add further elements to the ggplot.     geom_point( The “geom_point()” function causes the ggplot to draw data as points, thus it become a scatterplot. There are many other “geom_” functions that could be used.  color=“blue” The “color” command controls the color of the points. It can be confusing to know when to use “color” vs. “fill”.  )\nClosing parenthesis for the geom_boxplot function.   + The addition symbol + is used to add further elements to the ggplot.    stat_summary( This function will calculate a statistical summary to be plotted on the chart  fun = mean, fun is short for function. The summary function I want to apply to my y variable is “mean”.  geom = “line”, The “geom=” argument is used to tell what kind of geometry should be drawn to represent the means. Here we are asking for the means to be connected with a line.  aes( The aes or “aesthetics” function allows you to tell ggplot what variables should be mapped to what visual aspects of the chart; including what the x-axis or y-axis should become. Including it her means the aesthetic will only be applied to this layer.  group = 1 indicates which variable should be grouped by when drawing multiple lines (one line for each factor level). We write the number 1 to indicate there is just 1 group; we are not further splitting the data.  )\nClosing parenthesis for the geom_boxplot function.  )\nClosing parenthesis for the geom_boxplot function.   + The addition symbol + is used to add further elements to the ggplot.    labs( Function to edit labels of the plot  x = “Vitamin C mg/day”, Edit the x-axis label  y = “Length”, Edit the y-axis label  title = “Tooth Growth in Guinea Pigs” Edit the chart title  )\nClosing parenthesis for the geom_boxplot function.      \nPress Enter to run the code.   …  Click to View Output. \n\n\n\n\n\n\n\n\n\n\n\n\n\nUnder construction.\n\n\n\n\n\n\n\n\n\nInteraction Plot\n\n\n\n\n\n\n\n\nOverviewR Instructions\n\n\nThese plots are used to visualize two categorical factors (mapped to the x-axis and the line color/type) and a quantitative response variable (displayed on the y-axis). A point for each factor level combination mean is plotted, and then points are connected with lines to aid the visual interpretation of the plot. Because they show two factors, they are ideal for two-way ANOVA.\nInteraction plots are a great way to see factor effects. In particular, they can be helpful in understanding the nature of an interaction factor, or detecting the lack thereof. If the line segments in the plot are all (nearly) parallel, this is indicative that no interaction effect exists between the two factors. The more non-parallel the line segments, the more likely a significant interaction effect is present.\nA formal hypothesis test should be conducted to determine the significance of an interaction term, since sometimes the hypothesis test result can run counter to what a quick visual inspection might suggest. When a significant interaction is present, an interaction plot can be a critical part of understanding the nature of the interaction.\nIf there are three factors in a study, multiple interaction plots can be used (one at each value of the third factor) to explore the nature of two-way and three-way interactions. This approach can be extended for analyses involving more than 3 factors. However, interactions involving more than 3 variables are rare in practice. Therefore, if more than 3 factors are present in an analysis, interaction plots are not usually used as an exploratory tool. Instead, statistical tests are used to find significant interactions, and then interaction plots are used to describe the nature of those interactions.\n\n\n\n\nBase R ggplot2 plotly\n\n\n\nTo make a scatterplot use the function:\ninteraction.plot(mydata$factor_x, mydata$factor_line, mydata$response)\n\nmydata is the name of the dataset containing the factors and response.\nfactor_x is factor (or string) variable that will be plotted on the x-axis\nfactor_line is factor (or string) variable that will have different colored/types of lines on the plot\ny is the quantitative response variable, i.e., “numeric vector.”\n\nNote, unlike the other plotting we have done so far, there is no data = argument. Each variable must be specified using the $ notation if it exists inside a dataset. There are many additional arguments to specify colors, line types, legend formatting, etc.\nExample Code\nBy reading the documentation for the dataset (visible when ?ToothGrowth is run in the console) we can see that our “x” variable, “dose”, is a numeric variable in R. Because of this we convert it to a factor variable in the plot command with the factor() command. This changes the nature of dose only within that particular plot, not within the dataset generally.\n\n\n\n interaction.plot( An R function used to create an interaction plot  factor( A function to make a variable categorical.  ToothGrowth Name of the dataset that contains the dose variable.  $ The $ allows you to refer to a variable in a dataset by name  dose “dose” is a quantitative variable (numeric vector) from the “ToothGrowth” dataset that is being used as the explanatory variable (x-axis) for this plot.  ) parenthesis to close the factor function  , separate arguments to a function with commas  ToothGrowth Name of the dataset that contains the variables.  $ The $ allows you to refer to a variable in a dataset by name  supp a different line will be drawn for each value of supp  , separate arguments to a function with commas   ToothGrowth Name of the dataset that contains the dose variable.  $ The $ allows you to refer to a variable in a dataset by name  len name of the response variable in ToothGrowth  , separate arguments to a function with commas ylab= argument to specify y-axis label  “Tooth length” Label for the y-axis must be put in quotes  ,  separate arguments to a function with commas  xlab= argument to specify x-axis label  “Dose (vitamin C mg/day)” Label for the x-axis must be put in quotes  , separate arguments to a function with commas  trace.label= argument to specify the trace label which will appear in the legend  “Delivery Method” Label for the trace factor must be put in quotes  , separate arguments to a function with commas  main= argument to specify the chart title  “Tooth Growth in Guinea Pigs” Chart title must be put in quotes  ) parenthesis to close the interaction.plot function      \nPress Enter to run the code.   …  Toggle Output. \n\n\n\n\n\n\n\n\n\n\nBecause not all the line segments are parallel, you may begin to suspect an interaction is present. To determine if the interaction is significant you can do a hypothesis test for.\nNote, an easy/slick method for changing the legend position does not (currently) exist for interaction.plot(), though some hacked solutions can be used.\nYou can adjust things like line color, plotting points, etc. as shown in this next example.\n\n\n\n interaction.plot( An R function used to create an interaction plot  factor( A function to make a variable categorical.  ToothGrowth Name of the dataset that contains the dose variable.  $ The $ allows you to refer to a variable in a dataset by name  dose “dose” is a quantitative variable (numeric vector) from the “ToothGrowth” dataset that is being used as the explanatory variable (x-axis) for this plot.  ) parenthesis to close the factor function  , separate arguments to a function with commas  ToothGrowth Name of the dataset that contains the variables.  $ The $ allows you to refer to a variable in a dataset by name  supp a different line will be drawn for each value of supp  , separate arguments to a function with commas   ToothGrowth Name of the dataset that contains the dose variable.  $ The $ allows you to refer to a variable in a dataset by name  len name of the response variable in ToothGrowth  , separate arguments to a function with commas ylab= argument to specify y-axis label  “Tooth length” Label for the y-axis must be put in quotes  ,  separate arguments to a function with commas  xlab= argument to specify x-axis label  “Dose (vitamin C mg/day)” Label for the x-axis must be put in quotes  , separate arguments to a function with commas  trace.label= argument to specify the trace label which will appear in the legend  “Delivery Method” Label for the trace factor must be put in quotes  , separate arguments to a function with commas  main= argument to specify the chart title  “Tooth Growth in Guinea Pigs” Chart title must be put in quotes  , separate arguments to a function with commas  type=\nargument to specify whether lines, points, or both should be plotted  ‘b’ b, in quotes, indicates lines and points should be drawn on the plot  , separate arguments to a function with commas   pch=\nargument to specify shape of the points  16 an integer value from 0 to 25 is expected.   , separate arguments to a function with commas  lty=\nargument to specify line type.   1 1 for a solid line. Line type can be specified with an integer from 0 - 6, or text “solid”.  , separate arguments to a function with commas  col=\nargument to specify colors to be used for different values of the trace factor, supp  c( concatenate function used to create a vector  “darkblue” color to be used for first value of supp. Can be given in name form, integer, or rgb specs  ,\nseparate values in a vector with a comma   “deeppink3” color to be used for first value of supp. Can be given in name form, integer, or rgb specs  ) end the vector with a parenthesis  ) parenthesis to close the interaction.plot function      \nPress Enter to run the code.   …  Toggle Output. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo make an interaction plot in R using the ggplot approach, first ensure\nlibrary(ggplot2)\nis loaded. Then,\nggplot(data, aes(x=factor1, color=factor2, group=factor2, y=response) +\n  stat_summary(fun = mean, geom = \"line\")\n\ndata is the name of your dataset.\nfactor1 is a column of data from your dataset that is a qualitative factor whose values you want to plot along the x-axis.\nfactor2 is a column of data from your dataset that is a qualitative factor. You want to draw a different colored line for each value of this factor.\nresponse is the name of the quantitative response variable in your dataset.\nThe aesthetic helper function aes() is how you tell R which variables you want mapped to which aesthetics (i.e. visual attributes) of your chart. This is actually the input to the mapping= argument, but for conciseness mapping= is usually not typed out.\n\nThe group aesthetic indicates that any summary statistics that are calculated should be calculated separately for each value of factor2. It’s similar to a group_by() statement.\nBecause factor2 is also the value for color, each value of factor2 will be represented with a different color.\n\nstat_summary() does two things:\n\ncalculates a summary statistic. In this case we tell it to calculate the mean for each factor level combination with the fun = mean code. fun stands for “function”.\nindicates we want to connect the means with a line. geom stands for our desired geometry, in this case lines.\n\n\nNote if one of your factor variables is not coded as a factor (e.g. it is numeric), use factor() to convert it to the correct data type.\n\nHere is a basic interaction plot using ggplot2 package. In a later example we will add additional formatting, labels, etc.\n\n\n\n ggplot An R function “ggplot” used to create a framework for a graphic that will have elements added to it with the + sign.  ( Parenthesis to begin the function. Must touch the last letter of the function.  ToothGrowth “ToothGrowth” is a dataset. Type “View(ToothGrowth)” in R to see it.  ,  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice.  aes( The aes or “aesthetics” function is actually an input to the mapping= argument. It allows you to which variable is mapped to aspects of the chart. This includes things like what the x-axis or y-axis should become.  x= “x=” declares which variable will become the x-axis of the graphic.  factor(dose), factor converts dose, a numeric variable, into a categorical variable  color=supp,  declares that each value of supp will be represented with a different color  group=supp,  declares that each value of supp must be drawn separately. We specify the geometry to be drawn later with geom.  y=len declares which variable will become the y-axis of the graphic.  )) close the aes and ggplot functions with a parenthesis   + The addition symbol is used to add/tweak chart elements.    stat_summary( this adds a layer to the chart that will show summary statistics  fun = mean,  “mean” is the function used to get a summary statistic  geom=“line” The geometry used on the chart will be lines  ) parenthesis to close the stat_summary function      \nPress Enter to run the code.   …  Toggle Output. \n\n\n\n\n\n\n\n\n\n\nAdd different line types, points for each factor level mean, and improved labelling\nLook at the code below and notice that with the exception of the group aesthetic, a label is applied to each aesthetic mapping. linetype and color have the same label and R is smart enough to therefore combine the legend for these two aesthetic mappings into one legend. If the labels are not identical, each aesthetic will have a unique legend.\n\n\n\n ggplot An R function “ggplot” used to create a framework for a graphic that will have elements added to it with the + sign.  ( Parenthesis to begin the function. Must touch the last letter of the function.  ToothGrowth “ToothGrowth” is a dataset. Type “View(ToothGrowth)” in R to see it.  ,  The comma allows us to specify optional commands to the function. The space after the comma is not required. It just looks nice.  aes( The aes or “aesthetics” function is actually an input to the mapping= argument. It allows you to which variable is mapped to aspects of the chart. This includes things like what the x-axis or y-axis should become.  x= “x=” declares which variable will become the x-axis of the graphic.  factor(dose), factor converts dose, a numeric variable, into a categorical variable  color=supp,  declares that each value of supp will be represented with a different color  group=supp,  declares that each value of supp must be drawn separately. We specify the geometry to be drawn later with geom.  linetype=supp,  declares that each value of supp will be represented with a different line type  y=len declares which variable will become the y-axis of the graphic.  )) close the aes and ggplot functions with a parenthesis   + The addition symbol is used to add/tweak chart elements.    stat_summary( this adds a layer to the chart that will show summary statistics  fun=mean,  “mean” is the function used to get a summary statistic  geom=“line”,  The geometry used on the chart will be lines  size=1 Change the line thickness  ) parenthesis to close the stat_summary function   + The addition symbol is used to add/tweak chart elements.    stat_summary( this adds a layer to the chart that will show summary statistics  fun=mean,  “mean” is the function used to get a summary statistic  geom=“point”,  The geometry used on the chart will be points  size=3 Change the size of the points  ) parenthesis to close the stat_summary function   + The addition symbol is used to add/tweak chart elements.    labs( use this layer to change chart labels  x=“Vitamin C dose (mg/day)”, Put the x-axis label in quotes     y=“Tooth Length”, Put the x-axis label in quotes     title=“Guinea Pig Study”, Put the chart title in quotes     color=“Delivery Method”, Put the color label in quotes     linetype=“Delivery Method” Put the linetype label in quotes  ) Close labs with a parenthesis      \nPress Enter to run the code.   …  Toggle Output. \n\n\n\n\n\n\n\n\n\n\n\n\n\nUnder construction."
  },
  {
    "objectID": "hoveRmd/BF1_R_Instructions_boat.html",
    "href": "hoveRmd/BF1_R_Instructions_boat.html",
    "title": "BF1 R Instructions",
    "section": "",
    "text": "method_aovA name you come up with for your model  &lt;- The assignment operator. The result to the right of it gets stored in an object specified on the left  aov( A function to define the model  procKnow The response, or y, variable in the model. It is numeric.  ~ The ~ is like an equal sign in the model. Items on the left of ~ represent y, on the right you define independent factors (i.e. x’s).  Treatment, The independent variable containing the names for the 4 training methods  data = virtual Tell the model to look in the dataset named “virtual” for procKnow and Treatment variables  ) Functions always end with a closing parenthesis  summary( Give important information about an object. When called on an aov object the default is to print the ANOVA summary table  method_aov  Whatever you named your ANOVA model in the previous line   )  Functions always end with a closing parenthesis  Click to toggle output Click to toggle output. \n\n\n\n\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)\nTreatment    3  31.81  10.604   2.111  0.131\nResiduals   20 100.49   5.024"
  },
  {
    "objectID": "factor_structure.html",
    "href": "factor_structure.html",
    "title": "Factor Structure",
    "section": "",
    "text": "In this section you will learn about factors in context of analyzing results of an experiment:"
  },
  {
    "objectID": "factor_structure.html#inside-and-outside",
    "href": "factor_structure.html#inside-and-outside",
    "title": "Factor Structure",
    "section": "Inside and Outside",
    "text": "Inside and Outside\nThink about our toothbrush example, but ignore toothpaste for a moment. If toothbrush is the only treatment under scrutiny we have three factors in the analysis: the grand mean, toothbrush type, and the residual error.\nRecall that in the simplest version of the toothbrush experiment there were 4 levels of the treatment (toothbrush type), with six replicates for each toothbrush. In the following factor structure diagrams grand mean is represented in red, toothbrush type is drawn in blue, and the residual factor levels are depicted in black.\n\n\n\nA factor is inside of another factor if all the levels of one factor (the inside factor) completely fit within a second factor (the outside factor).\nYou may find this analogy helpful. Pretend that an outside factor is a box, and the inside factor levels are blocks that fit neatly within the box.\n\n\n\n\n\nTo determine if a factor is inside another factor, imagine picking up the levels of the factor one by one and placing them inside the other factor. If they all fit without crossing the partition lines of a factor,then it is considered inside.\nThis is illustrated with the toothbrush example. We will start by taking the levels of toothbrush and placing them inside of grand mean\n\n\n\n\n\nIn the figure above you can see that one entire level of toothbrush can fit inside of a single level of benchmark. Even though they may share a boundary line, the toothbrush level does not cross over any lines or start sharing boundaries with any other level of benchmark (this is of course impossible since benchmark only has one level). You can repeat this for the other 3 levels of toothbrush with the same result. Therefore, we say that toothbrush is inside of benchmark, which is the same as saying that benchmark is outside of toothbrush.\nConsider now the relationship between toothbrush and residual as shown below. If we take a level of toothbrush and overlay it on the residual factor, we can see it does not fit neatly inside one of the levels of residual error. In fact, one level of toothbrush crosses the boundaries of many of the levels of residual error. Therefore, we cannot say that toothbrush is inside of residual error.\n\n\n\n\n\nSince toothbrush is not inside of residual error, does this necessarily mean that toothbrush is outside of residual error? No! This is something that has to be checked. To determine whether toothbrush is outside of residual we must take the levels of residual error one at a time and overlay them on the toothbrush factor structure, as shown below. You can see that one level of residual error does NOT cross any of the toothbrush level boundaries. Therefore, toothbrush is indeed outside of residual error; or equivalently, residual error is inside of toothbrush.\n\n\n\n\n\nLet’s pause here to clarify a common misunderstanding. Consider an experiment where we are looking at the inside vs. outside relationship of two factors: A and B.\n\nWhen factor A is inside of factor B, we can also say factor B is outside of factor A.\nBut, when factor A is not inside of factor B, this does not necessarily mean that factor A is outside of factor B. There are situations where two factors are neither inside nor outside of each other; they are crossed."
  },
  {
    "objectID": "factor_structure.html#crossed-factors",
    "href": "factor_structure.html#crossed-factors",
    "title": "Factor Structure",
    "section": "Crossed Factors",
    "text": "Crossed Factors\nTwo factors are crossed when their partition lines cross in a way that creates new groups of observations that represent every possible combination of the factor levels. More succinctly stated, factors are crossed when all factor level combinations are present in the study. We see this type of relationship in the toothbrush study with two controlled factors: toothbrush type (4 levels) and toothpaste brand (2 levels). Toothbrush and toothpaste are neither inside nor outside of each other; rather, they are crossed.\n\n\n\n\n\n\n\n\n\n\n\nThe crossing of toothpaste brand and toothbrush created an interaction factor.\n\n\n\nWhen two factors are crossed, the resulting (interaction) factor’s levels will always be inside of each of the factors that was crossed to create it. This fact is illustrated below for the toothbrush factor and could similarly be shown for toothpaste brand factor."
  },
  {
    "objectID": "factor_structure.html#nested-factors",
    "href": "factor_structure.html#nested-factors",
    "title": "Factor Structure",
    "section": "Nested Factors",
    "text": "Nested Factors\nTwo factors are nested when each level of one factors appears in only one level of the other factor. Consider these two factors in an experiment: student and classroom. If each student appears in only one of the classrooms then it could be said that student is nested in classroom.\nSimilarly, in sports we often have two factors: player and team. The player is nested in the team. There may be multiple levels of nesting, as team could also be considered nested in a division or league.\nThe diagram below shows data partitions for an experiment with 12 observations. The experiment was on crash test safety ratings of vehicles. Six different vehicles were chosen to be used in the study. A crash test safety score was obtained from a front collision and side collision.\n\n\n\nData Partition for 2 Factors: Impact Location and Vehicle\n\n\nHowever, there was another factor that the researchers wanted to take into account: vehicle type. Vehicle type has two levels: mini-van and sedan. Each vehicle in the experiment belongs to one and only one vehicle type. The full data partition is shown Figure 1, with this third factor partition highlighted in red.\n\n\n\nFigure 1: Vehicle is Nested in Type\n\n\nCrossed factors have a “symmetric” relationship, meaning saying Factor A is crossed with Factor B is equivalent to saying Factor B is crossed with Factor B. This is not true for nested factors. While it is correct to say Vehicle is nested within Type, it is not correct to say that Type is nested within Vehicle.\nOften, the factor that is nested within another factor is an experimental unit. This creates two levels of experimental units. This is discussed at length in the Introduction to Nested Designs and SP/RM[1,1] chapters. Split Plot/Repeated Measure designs are the only designs in this book that deal with nested factors."
  },
  {
    "objectID": "examples/VirtualTrain.html",
    "href": "examples/VirtualTrain.html",
    "title": "Lifeboat Launch Training Methods",
    "section": "",
    "text": "Code\nlibrary(mosaic)\nlibrary(DT)\nlibrary(pander)\nlibrary(car)\nlibrary(tidyverse)\n\n## Data from original article:\nvirtual &lt;- read.csv(\"../data/virtual_training.csv\", header=TRUE)"
  },
  {
    "objectID": "examples/VirtualTrain.html#background",
    "href": "examples/VirtualTrain.html#background",
    "title": "Lifeboat Launch Training Methods",
    "section": "Background",
    "text": "Background\nAn experiment was done to help train people in the procedure to launch a lifeboat. This was a Completely Randomized Design with 16 subjects per treatment, for a total of 64 subjects. The response variable is the performance on a procedural knowledge test. The treatments included: Lecture/Materials (Control) (1), Monitor/Keyboard (2), Head Monitor Display/Joypad (3), and Head Monitor Display/Wearables (4)\nSource: J.Jung and Y.J. Ahn (2018). “Effects of Interface on Procedural Skill Transfer in Virtual Training: Lifeboat Launching Operation Study,” Computer Animation & Virtual Worlds, Vol. 29, pp. e1812. https://doi.org/10.1002/cav.1812"
  },
  {
    "objectID": "examples/VirtualTrain.html#analysis",
    "href": "examples/VirtualTrain.html#analysis",
    "title": "Lifeboat Launch Training Methods",
    "section": "Analysis",
    "text": "Analysis\nApplying a one-way ANOVA to this study, we have the following model:\n\\[\ny_{ij} = \\mu + \\alpha_i + \\epsilon_{ij}\n\\]\nWhere \\(y_{ij}\\) is the \\(j^{th}\\) observation from treatment \\(i\\)\n\\(\\mu\\) is the grand mean of the dataset. Also referred to as an overall mean or benchmark.\n\\(\\alpha_i\\) is the effect of the training method. 1 = control/lecture, 2 = Monitor/keyboard, 3 = head monitor/joypad, 4 = head monitor/wearables.\n\\(\\epsilon_{ik}\\): the error term, or residual term of the model. Since there are 16 subjects for each treatment, \\(j\\) ranges from 1 to 16.\n\nHypothesis Test\nThe null hypothesis is that the effect of all training methods, represented by α, is equal to zero. This is formally written as follows.\n\\[ H_0:\\alpha_\\text{Control} = \\alpha_\\text{Monitor/Keyboard} = \\alpha_\\text{Joypad} = \\alpha_\\text{Wearables} = 0 \\]\nThe alternative hypothesis states that at least one of the effects due to material is different than zero \\[ H_a: \\text{at least one } \\alpha \\text{ is different than 0} \\]\nUsing these hypotheses will allow for us to address the question whether any of the type of training is different to improve test score.\nWe will use a level of significance of α = 0.05 for the analysis.\nFor the analysis, we perform the following one-way ANOVA\n\n\nCode\nvirtual &lt;- virtual %&gt;% \n   mutate(\n         Treatment = case_when(\n           grp.trt %in% 1  ~ \"Control\",\n           grp.trt %in% 2  ~ \"Monitor/Keyboard\",\n           grp.trt %in% 3  ~ \"Joypad\",\n           grp.trt %in% 4  ~ \"Wearables\"\n          )\n        )\n\nvirtual.aov &lt;- aov(procKnow ~ Treatment, data=virtual)\nsummary(virtual.aov) %&gt;% pander()\n\n\n\nAnalysis of Variance Model\n\n\n\n\n\n\n\n\n\n\n \nDf\nSum Sq\nMean Sq\nF value\nPr(&gt;F)\n\n\n\n\nTreatment\n3\n65.66\n21.89\n4.941\n0.003931\n\n\nResiduals\n60\n265.8\n4.43\nNA\nNA\n\n\n\n\n\nThe p-value for this test is significant (p = 0.003931). Based on this result, the null hypothesis is rejected and we have sufficient evidence that at least one of the effects due to training is different for post test score.\n\n\nCheck Requirements\nIn order to trust this result, we must verify the requirements for the ANOVA model are met. The requirement of equal variances appears to be met since the residuals versus fitted plot shows roughly a constant variance within each vertical group of dots.\nThe QQ-plot of residuals on the right is used to check whether residuals are normally distributed. There are a few points outside the boundaries that might be a concern for this ANOVA requirement, but generally there are no strong departures from normality and so we consider this requirement to be met also.\n\n\nCode\npar(mfrow=c(1,2))\nplot(virtual.aov, which=1, pch=16)\nqqPlot(virtual.aov, id=FALSE)\n\n\n\n\n\n\n\nTraining method’s effect on test score\nThe following plot shows which types of training increases the post test score.\n\n\nCode\nxyplot(procKnow ~ as.factor(Treatment), data=virtual, type=c(\"p\",\"a\"), main=\"Score based on Type of Training\", xlab =\"Treatment\", ylab = \"Test Score\")\n\n\n\n\n\nThe averages are illustrated with the blue line in the plot above and the table below shows the means, standard deviations, and sample sizes for each of the five different materials.\n\n\nCode\npander(favstats(procKnow ~ Treatment, data=virtual)[,c(\"Treatment\",\"mean\",\"sd\",\"n\")])\n\n\n\n\n\n\n\n\n\n\n\nTreatment\nmean\nsd\nn\n\n\n\n\nControl\n4.931\n1.94\n16\n\n\nJoypad\n6.736\n2.82\n16\n\n\nMonitor/Keyboard\n7.708\n1.43\n16\n\n\nWearables\n6.875\n1.99\n16"
  },
  {
    "objectID": "examples/VirtualTrain.html#interpretation",
    "href": "examples/VirtualTrain.html#interpretation",
    "title": "Lifeboat Launch Training Methods",
    "section": "Interpretation",
    "text": "Interpretation\nIt appears the the best type of training may be the Monitor/Keyboard type training. The highest mean came from the Monitor/Keyboard where the average procedural knowledge post test score was 7.708. The head monitor training methods may perform better on different types of assessments that were not part of this study. A future study could look into other training methods to improve readiness in lifeboat launching."
  },
  {
    "objectID": "effects_model.html",
    "href": "effects_model.html",
    "title": "Effects Model",
    "section": "",
    "text": "In science, statistics, mathematics, etc. we are interested in discovering and describing truth about the world we live in. We can use models to represent a phenomenon or system. The model’s purpose may be to describe and explain something about the way things work and/or to make prediction. An ANOVA model is a model that uses mathematical terms and constructs, in the form of an equation. The ANOVA model quantifies the relationship between factor(s) and a response.\nIf we were able to observe all items in a population we would be able to quantify with exactness how a factor variable is related to a response variable with an ANOVA model. This “true”, or exact quantification of how the two variables relate is called a parameter. However, with rare exceptions, we cannot observe every item in a population. Instead, we must rely on a sample to calculate estimates of the parameter.\nBy the end of this section you should know how to interpret the terms in an ANOVA model (i.e. know what they mean), and also perform the calculations necessary to estimate ANOVA model parameters.\n\n\n\nConsider an experiment conducted to test 4 different types of toothbrushes: manual, oscillating, sonic and ultrasonic. The response variable is percent of area on teeth that has plaque. Twenty-four individuals participate in the experiment, yielding 6 observations per treatment. We will discuss the creation of an ANOVA model using the context of this specific example. In this section and “Calculating Effect Sizes” section we will be referring to estimates of model parameters using this sample data.\nUp to this point we have primarily been interested in calculating means in order to compare toothbrushes. If the mean percent of teeth surface area with plaque was 23.09 for a manual brush, you would want to know how that value compares to the other types of brushes. A graph or table reporting the other sample means would be necessary to provide context. Figure 1 depicts the data points and the mean for each factor level.\n\n\n\n\n\nFigure 1: Brush means and grand mean\n\n\n\n\nIn an earlier, introductory statistics class you most likely learned how to use sample means in order to test whether populations means for different groups are equal using analysis of variance (ANOVA). This hypothesis test of multiple population means is valid and works when you only have 1 factor. This is often called the cell means model.1 However, it is limited in its ability to include more factors or more complicated designs.\nThere is another metric we use to compare factor levels: the effect size. Reporting the effect of a factor level has the benefit of providing some context on how that factor level is influencing the response variable relative to other levels of the same factor. Using effect sizes (as opposed to factor level means) allows us to model and test much more complicated scenarios than a simple one factor experiment."
  },
  {
    "objectID": "effects_model.html#models",
    "href": "effects_model.html#models",
    "title": "Effects Model",
    "section": "",
    "text": "In science, statistics, mathematics, etc. we are interested in discovering and describing truth about the world we live in. We can use models to represent a phenomenon or system. The model’s purpose may be to describe and explain something about the way things work and/or to make prediction. An ANOVA model is a model that uses mathematical terms and constructs, in the form of an equation. The ANOVA model quantifies the relationship between factor(s) and a response.\nIf we were able to observe all items in a population we would be able to quantify with exactness how a factor variable is related to a response variable with an ANOVA model. This “true”, or exact quantification of how the two variables relate is called a parameter. However, with rare exceptions, we cannot observe every item in a population. Instead, we must rely on a sample to calculate estimates of the parameter.\nBy the end of this section you should know how to interpret the terms in an ANOVA model (i.e. know what they mean), and also perform the calculations necessary to estimate ANOVA model parameters."
  },
  {
    "objectID": "effects_model.html#toothbrush-example",
    "href": "effects_model.html#toothbrush-example",
    "title": "Effects Model",
    "section": "",
    "text": "Consider an experiment conducted to test 4 different types of toothbrushes: manual, oscillating, sonic and ultrasonic. The response variable is percent of area on teeth that has plaque. Twenty-four individuals participate in the experiment, yielding 6 observations per treatment. We will discuss the creation of an ANOVA model using the context of this specific example. In this section and “Calculating Effect Sizes” section we will be referring to estimates of model parameters using this sample data.\nUp to this point we have primarily been interested in calculating means in order to compare toothbrushes. If the mean percent of teeth surface area with plaque was 23.09 for a manual brush, you would want to know how that value compares to the other types of brushes. A graph or table reporting the other sample means would be necessary to provide context. Figure 1 depicts the data points and the mean for each factor level.\n\n\n\n\n\nFigure 1: Brush means and grand mean\n\n\n\n\nIn an earlier, introductory statistics class you most likely learned how to use sample means in order to test whether populations means for different groups are equal using analysis of variance (ANOVA). This hypothesis test of multiple population means is valid and works when you only have 1 factor. This is often called the cell means model.1 However, it is limited in its ability to include more factors or more complicated designs.\nThere is another metric we use to compare factor levels: the effect size. Reporting the effect of a factor level has the benefit of providing some context on how that factor level is influencing the response variable relative to other levels of the same factor. Using effect sizes (as opposed to factor level means) allows us to model and test much more complicated scenarios than a simple one factor experiment."
  },
  {
    "objectID": "effects_model.html#assembly-line-metaphor",
    "href": "effects_model.html#assembly-line-metaphor",
    "title": "Effects Model",
    "section": "Assembly Line Metaphor",
    "text": "Assembly Line Metaphor\nYou can imagine that each data point in your data set is created by going down an assembly line, much like you would find in a factory that makes cars or appliances. All points start with the grand mean value. As it progresses through the assembly line the data point is altered to reflect the effect of the factor levels it belongs to.\n\n\n\n\n\n\nIn the toothbrush and toothpaste experiment all the points start the assembly line at the same value: the grand mean of the data set. The first station on the line receives the data point and adds or subtracts to it based on the type of brush it is. For example, the value would be added upon if the brush type was “manual” because the mean plaque percentage for the manual type was higher than the grand mean. The next station alters the value depending on which toothpaste was used: off-brand or name brand. After going through each station (one station for each factor) in the assembly line the data point arrives at the last station.\nThe last station is worked by a person who makes random adjustments! Some adjustments will be big, and some will be small; some will be positive, and some will be negative. (In a typical factory this person would be fired. But we would rather have randomness than unknown, systematic adjustments , i.e. bias). These random adjustments are driven by any/all factors that we did not explicitly measure.\nFor example, variability in the experimental unit’s diet, hardness of the water used when brushing, the impact of flossing, outside temperature, and the price of rice in China are all factors that were not taken into account. Effects from these and an infinite number of other factors are all lumped into the residual error factor effects. The effects of factors unrelated to the response should be negligible. Effects from other factors that were sufficiently randomized should usually cancel each other out. The point is that all these factor’s effects show up in one (hopefully small) adjustment at the last station and are referred to as “unexplained variance” (review Sources of Variance).\nThough the adjustments at this last station for residual error are (assumed) random, they do follow a pattern. Namely, the mean of the adjustments is zero and they follow a normal distribution."
  },
  {
    "objectID": "effects_model.html#footnotes",
    "href": "effects_model.html#footnotes",
    "title": "Effects Model",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe algebraic form of the cell means model is\n\\[\ny_\\text{ij} = \\mu_i + \\epsilon_\\text{ij}\n\\]\nWhere \\(y_\\text{ij}\\) is an observation, \\(\\mu_i\\) is the mean of factor level \\(i\\), and \\(\\epsilon_\\text{ij}\\) is the residual for each observation. The ANOVA F-test hypothesis for this model is \\(H_0: \\text{All means are equal to each other}\\).↩︎\nPartial fit is a term that refers to an estimated model, or estimated prediction, using only some of the factors. Often, in ANOVA, partial fit refers to a model or prediction estimated using just outside factors. Thus, for this model Equation 5 could be restated as Factor level effect size = Factor level mean - partial fit.↩︎\nYou can think of an effect as a deviation from a mean. Other synonyms for deviation are error and variance. When you calculate an effect, you are also calculating a deviation or error. Thus, residual error effect is a bit redundant. Residual error and residual effect may be used interchangeably.↩︎\nCobb, George. Introduction to Design and Analysis of Experiments. Wiley, 2014. ↩︎\nThe model for the experiment that includes toothpaste brand and toothbrush type is\n\\[\ny_\\text{ijk} = \\mu + \\alpha_i + \\beta_j + (\\alpha\\beta)_\\text{ij} + \\epsilon_\\text{ijk}\n\\]\nHere\n\n\\(\\alpha\\) is the effect of toothbrush, and \\(i\\) goes from 1 to 4 since there are 4 toothbrush types\n\\(\\beta\\) is the effect of toothpaste, and \\(j\\) is either 1 or 2 since there are 2 levels (Name brand and Generic brand).\n\\(\\alpha\\beta)_\\text{ij}\\) is the interaction term. An interaction is discussed on the BF[2] page.\n\\(\\epsilon\\) is the residual error term, and \\(k\\) is the replicate count within a factor level combination.\n\n↩︎"
  },
  {
    "objectID": "examples/mosquito.html",
    "href": "examples/mosquito.html",
    "title": "Mosquitos",
    "section": "",
    "text": "Code\nlibrary(mosaic)\nlibrary(DT)\nlibrary(pander)\nlibrary(car)\nlibrary(tidyverse)\n\n## Data from original article:\nmosquito &lt;- read_csv(\"../data/mosquito_patch.csv\")"
  },
  {
    "objectID": "examples/mosquito.html#background",
    "href": "examples/mosquito.html#background",
    "title": "Mosquitos",
    "section": "Background",
    "text": "Background\nFive pre-treated patches were compared to to see which material did the best in reducing mosquito human contact for the Armed Forces in India. The five treatments included Odomos(1), Deltamethrin (2), Cyfluthrin(3), D+O(4), C+O(5) Each of the treatments included 30 replicates per treatment\nSource: A. Bhatnagar and V.K. Mehta (2007). “Efficacy of Deltamethrin and Cyfluthrin Impregnated Cloth Over Uniform Against Mosquito Bites,” Medical Journal Armed Forces India, Vol. 63, pp. 120-122"
  },
  {
    "objectID": "examples/mosquito.html#analysis",
    "href": "examples/mosquito.html#analysis",
    "title": "Mosquitos",
    "section": "Analysis",
    "text": "Analysis\nApplying a one-way ANOVA to this study, we have the following model:\n\\[\ny_{ij} = \\mu + \\alpha_i + \\epsilon_{ij}\n\\]\nWhere \\(y_{ij}\\) is the \\(j^{th}\\) observation from treatment \\(i\\)\n\\(\\mu\\) is the grand mean of the dataset.\n\\(\\alpha_i\\) is the effect of the treatment as described in the background.\n\\(\\epsilon_{ik}\\): the error term, or residual term of the model. Since there are 30 subjects for each treatment, \\(j\\) ranges from 1 to 30.\nApplying a one-way ANOVA to this study, we have the null hypothesis that the effect of human mosquito contact, represented by α, is equal for each of the factors. This is formally written as follows. \\[ H_0:\\alpha_\\text{Odomos} = \\alpha_\\text{Deltamethrin} = \\alpha_\\text{Cyfluthrin} = \\alpha_\\text{D+O} = \\alpha_\\text{C+O} = 0 \\] The alternative hypothesis states that at least one of the effects due to material is different than zero \\[ H_a: \\text{at least one } \\alpha \\text{ is different than 0} \\] Using these hypotheses will allow for us to address the question whether any of the materials are better at minimizing mosquito-human contact.\n\nHypothesis test\nWe will use a level of significance of α = 0.05 for the analysis.\nFor the analysis, we perform the following one-way ANOVA\n\n\nCode\nmosquito &lt;- mosquito %&gt;% \n   mutate(\n         Treatment = case_when(\n           trt.mosq %in% 1  ~ \"Odomos\",\n           trt.mosq %in% 2  ~ \"Deltamethrin\",\n           trt.mosq %in% 3  ~ \"Cyfluthrin\",\n           trt.mosq %in% 4  ~ \"D+O\",\n           trt.mosq %in% 5  ~ \"C+O\"\n          )\n        )\n\nmosquito.aov &lt;- aov(y.mosq ~ Treatment, data=mosquito)\nsummary(mosquito.aov) %&gt;% pander()\n\n\n\nAnalysis of Variance Model\n\n\n\n\n\n\n\n\n\n\n \nDf\nSum Sq\nMean Sq\nF value\nPr(&gt;F)\n\n\n\n\nTreatment\n4\n184.6\n46.16\n4.48\n0.001924\n\n\nResiduals\n145\n1494\n10.3\nNA\nNA\n\n\n\n\n\nThe p-value for this test is significant (p = 0.001924). Based on this result, the null hypothesis is rejected and we have sufficient evidence that at least one of the effects due to material is different for human mosquito contact.\nThe requirements of equal variances for ANOVA is met. This is shown by the residual versus fitted plot, which shows roughly a constant variance within each vertical group of dots. The QQ-plot of residuals on the right shows some non-normality as evidenced by some of the points outside of the dashed line boundaries. However, it is not severe and we will move forward with the analysis.\n\n\nCode\npar(mfrow=c(1,2))\nplot(mosquito.aov, which=1, pch=16)\nqqPlot(mosquito.aov, id=FALSE)\n\n\n\n\n\nThe following plot shows which types of material minimize the human mosquito contact.\n\n\nCode\nboxplot(y.mosq ~ as.factor(Treatment), data=mosquito, main=\"Human Mosquito Contact Based on Type of Material\", xlab =\"Treatment\", ylab = \"Amount of Mosquito Human Contact\")\n\n\n\n\n\nThe averages are illustrated with the blue line in the plot above and the table below shows the means, standard deviations, and sample sizes for each of the five different materials.\n\n\nCode\nfavstats(y.mosq ~ Treatment, data=mosquito) %&gt;% \n  select(Treatment,mean,sd,n) %&gt;% \n  arrange(mean) %&gt;% \n  pander()\n\n\n\n\n\n\n\n\n\n\n\nTreatment\nmean\nsd\nn\n\n\n\n\nC+O\n5.367\n3.068\n30\n\n\nD+O\n6.333\n3.121\n30\n\n\nOdomos\n7.901\n3.366\n30\n\n\nCyfluthrin\n8.033\n3.01\n30\n\n\nDeltamethrin\n8.133\n3.46\n30\n\n\n\n\n\n\n\nPairwise comparisons\nWe now that smallest mean (C+O) must be different than the largest mean (Deltamethrin) because the F-test was significant above. In order to better understand which treatments perform better than which other treatments we will look at all pairwise comparisons and apply Tukey’s correction to the family error rate.\n\n\nCode\n#This code would work, but...\n# TukeyHSD(mosquito.aov, \"Treatment\")\n\n#I want to do this fancy stuff below to sort the output\nTukeyHSD(mosquito.aov, \"Treatment\")$Treatment %&gt;% \n  as_tibble(rownames = \"id\") %&gt;% \n  arrange(`p adj`) %&gt;% #Have to put the column name in back ticks since it has a space\n  pander() \n\n\n\n\n\n\n\n\n\n\n\n\nid\ndiff\nlwr\nupr\np adj\n\n\n\n\nDeltamethrin-C+O\n2.766\n0.4764\n5.056\n0.009359\n\n\nCyfluthrin-C+O\n2.666\n0.3761\n4.955\n0.01367\n\n\nOdomos-C+O\n2.534\n0.2441\n4.823\n0.02204\n\n\nDeltamethrin-D+O\n1.8\n-0.4899\n4.089\n0.1965\n\n\nD+O-Cyfluthrin\n-1.699\n-3.989\n0.5902\n0.2477\n\n\nOdomos-D+O\n1.567\n-0.7222\n3.857\n0.3268\n\n\nD+O-C+O\n0.9663\n-1.323\n3.256\n0.7707\n\n\nOdomos-Deltamethrin\n-0.2323\n-2.522\n2.057\n0.9986\n\n\nOdomos-Cyfluthrin\n-0.132\n-2.422\n2.158\n0.9999\n\n\nDeltamethrin-Cyfluthrin\n0.1003\n-2.189\n2.39\n1\n\n\n\n\n\nC+O is significantly lower than 3 of the treatments at the 0.05 level; and no other treatment has a sample mean lower than C+O’s."
  },
  {
    "objectID": "examples/mosquito.html#interpretation",
    "href": "examples/mosquito.html#interpretation",
    "title": "Mosquitos",
    "section": "Interpretation",
    "text": "Interpretation\nIt appears the the best type of material is the C+O material to minimize the amount of mosquito human contact. The lowest mean came from the C+O material where the average amount of mosquito/human contact was 5.367. With a mean of 6.333, D+O was not significantly different than C+O and could also be an option. Conducting a new experiment that focuses on the difference between C+O and D+O and gives them a larger sample size in order to better detect significant would be reasonable.\nA future study could look into other types of material as well as doing this analysis at different locations throughout the world."
  },
  {
    "objectID": "experimental_units.html",
    "href": "experimental_units.html",
    "title": "Experimental Units",
    "section": "",
    "text": "The experimental unit is the subject of the experiment. It is the entity to which the treatment factor levels are assigned. The observational unit is the entity on which the response measurements are taken. It is important to differentiate between the two, in cases in which they are different. It is important to consider the research objectives when determining the experimental unit and observational unit. Research objectives need to be revised when the experimental and observational units are not clearly defined. We will illustrate this importance by using two different examples.\nThe first example will study to see if a specific SAT prep class improves individual’s SAT math scores. Twenty randomly selected students are randomly divided into two groups. One group will take the prep class and the other group will not take the class. In this case the experimental unit is the student because factor level assignments were made for each student. The observational unit is also the student because the math scores are measured for each student.\nThe next example will be slightly, but distinctly different. In this case, we will be applying a new teaching method to a math classroom to see if it improves math scores. Six randomly selected classrooms are randomly divided into 2 groups. One group will be randomly assigned the new teaching method and the other group will be randomly assigned the standard method. Within each classroom there are 25 students and math scores will be taken from each student. In this case, the classrooms are the experimental unit because the factor levels are assigned to each classroom. The students are the observational unit because the scores are measured for each student. The new teaching method will have 75 observations (3 classrooms with 25 students in each). However, the research objectives are concerning the classroom, and assignments of the teaching method are made at the classroom level, so the number of experimental units for each teaching method is just 3 for this analyses. Even though there were many observations, the limited number of experimental units limits the ability to make convincing inference to the broader population. We would like more classrooms involved be be more sure the succes/failure of the new teaching method was not due to a few particularly good/bad teachers. The researcher needs to be aware when this type of sampling is occurring so that a more complex analysis technique can be used to tease out the effect of teachers vs. method - and enhance our ability to make inference. This technique is discussed in further detail with the Nested Factor Designs.\nNow let’s apply this to the toothbrush study. Each person will be assigned to use one type of toothbrush, so person is the experimental unit. The measurements will be taken from teeth, so the observational unit will be tooth. If measurements are taken from multiple teeth, then a more complex design, like the Nested Factor designs should be considered."
  },
  {
    "objectID": "hoveRmd/BF1_R_Instructions.html",
    "href": "hoveRmd/BF1_R_Instructions.html",
    "title": "BF1 R Instructions",
    "section": "",
    "text": "df A name you come up with for your dataset  &lt;- The assignment operator. The result to the right of it gets stored in an object specified on the left  read_csv( a command from the tidyverse to read in csv files  “data/toothpaste_BF2.csv” The path to the csv file containing the data  ) Functions always end with a closing parenthesis  plaque_aovA name you come up with for your model  &lt;- The assignment operator. The result to the right of it gets stored in an object specified on the left  aov( A function to define the model  Plaque The response, or y, variable in the model. It is numeric.  ~ The ~ is like an equal sign in the model. Items on the left of ~ represent y, on the right you define independent factors (i.e. x’s).  Brush, The independent variable containing the names for the 4 types of toothbrushes.  data = df Tell the model to look in the dataset named “df” for Plaque and Brush variables  ) Functions always end with a closing parenthesis  summary( Give important information about an object. When called on an aov object the default is to print the ANOVA table  plaque_aov  Whatever you named your ANOVA model in the previous line   )  Functions always end with a closing parenthesis  Click to view output Click to View Output. \n\n\n\n\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)  \nBrush        3  86.31  28.769   3.822 0.0258 *\nResiduals   20 150.56   7.528                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "hoveRmd/contrast_R_instructions.html",
    "href": "hoveRmd/contrast_R_instructions.html",
    "title": "Math326 Notebook",
    "section": "",
    "text": "Caution, the contrasts() function from the stats package in base R will produce the correct p-value for the test of a contrast, but without extra work will not produce the correct estimate of the contrast itself. For this reason, we illustrate estimating and testing the contrast with the emmeans package, which stands for “estimated marginal means”.\nThe first step is to create the model. Then use the emmeans() command to create a grid of factor level summary statistics, including: means, standard deviations, standard error, degrees of freedom associated with the standard error estimate, and confidence intervals around the mean. Unlike a summarize() or favstats() command, emmeans() has the output structured so that it can easily be used in the next step. Store the grid of means into a new object.\n\nmyaov &lt;- aov(Y ~ X, data = df)\nmymeans &lt;- emmeans(myaov, \"X\")\n\n\nmyaov is some name you come up with to store the results of the aov() model.\nY must be a “numeric” vector of the quantitative response variable.\nX is a qualitative variable (should have class(X) equal to factor or character. If it does not, use factor(X) inside the aov(Y ~ factor(X),...) command.\ndf is the name of your data set.\nmymeans is some name you come up with to store the results of the emmeans() command.\n\nDefine the contrasts you are interested in testing inside the contrast() function, which returns the hypothesis test results. You can also feed the result into a confint() function if you prefer confidence intervals over p-values.\n\ncontrast(mymeans,list(name_of_contrast1 = coefficient vector,\n                       name_of_contrast2 = another coefficient vector))) \n\nname_of_contrast are descriptive names you should give to the contrast to help you remember what it represents. The coefficient vector is how you define the contrast.\nWe will repeat the contrasts we did by hand in the sections above, but this time using R.\nExample Code Using Toothbrush Experiment:\n\n\n df The name you want for your dataset  &lt;- The assignment operator. The result to the right of it gets stored in an object specified on the left  read_csv(“../data/toothpaste_BF2.csv”) A tidyverse command to read the data in from the specified path  plaque_aov Name you want for your ANOVA model  &lt;- The assignment operator. The result to the right of it gets stored in an object specified on the left  aov( A function to define the model  Plaque The response, or y, variable in the model. It is numeric.  ~ The ~ is like an equal sign in the model. Items on the left of ~ represent y, on the right you define independent factors (i.e. x’s).  Brush, The independent variable containing the names for the 4 types of toothbrushes.  data = df Tell the model to look in the dataset named “df” for Plaque and Brush variables  ) Functions always end with a closing parenthesis  brush_means The name you want for the output of the emmeans command  &lt;- The assignment operator. The result to the right of it gets stored in an object specified on the left   emmeans(  Function to calculate stats about marginal means  plaque_aov, aov model created in previous step  “Brush” Factor for whose levels you want to calculate means  ) Functions always end with a closing parenthesis  contrast_results Name you want to store contrast results in  &lt;- The assignment operator. The result to the right of it gets stored in an object specified on the left  contrast( Function to define and test contrasts  brush_means, Grid of stats about marginal means you named in the previous step list( create a list object, which allows you to pass multiple contrast coefficient vectors  man_v_osc = A descriptive name to help you remember what the contrast represents  c(1,-1,0,0) Vector of coefficients used to define the contrast  , Seperator to allow additional inputs to the list     man_v_others = A descriptive name to help you remember what the contrast represents  c(1,-(1/3),-(1/3),-(1/3)) Vector of coefficients used to define the contrast  ), A list is closed with a parenthesis  adjust = Specify what type of adjustment (if any) to make for multiple testing. Default is “none” if this argument is not included.  “none” Read help at ?summary.emmGrid for other acceptable values  ) Functions always end with a closing parenthesis  contrast_results View the test results stored in this object in the previous step  confint( Function to create confidence intervals around contrasts  contrast_results Name of object where you stored contrasts  ) Functions always end with a closing parenthesis  Toggle output Toggle Output. \n\n\n\n\n\n contrast     estimate   SE df t.ratio p.value\n man_v_osc       3.117 1.58 20   1.967  0.0632\n man_v_others    0.438 1.29 20   0.339  0.7382\n\n\n\n\n contrast     estimate   SE df lower.CL upper.CL\n man_v_osc       3.117 1.58 20   -0.188     6.42\n man_v_others    0.438 1.29 20   -2.260     3.14\n\nConfidence level used: 0.95"
  },
  {
    "objectID": "hoveRmd/model_diagnostics.html",
    "href": "hoveRmd/model_diagnostics.html",
    "title": "Model Diagnostics",
    "section": "",
    "text": "A key assumption for ANOVA tests is that the error, or residual term, has a constant variance across all factor levels. This is sometimes call homogeneity of variance, or homoscedasticity.\nWe explain three ways to check the assumption: rule of thumb when comparing standard deviations for each factor level, a visual assessment of the residual vs. fitted plot, and Levene’s test. These methods may not always agree. You should be aware of the underlying data. Understanding why this assumption is important and how it will affect results when violated will help you decide how to proceed after checking these diagnostics. It is also worth noting that the ANOVA F-test is robust in the face of mild to moderate violation of this assumption.\nWe will use the pre-loaded dataset ToothGrowth. To learn more about the dataset, run ?ToothGrowth in the console. len will be our response variable, supp is an independent factor, and dose is the other independent factor. We will analyze this as a two-way, basic factorial design. Because dose is stored as a numeric variable, we will convert it to a categorical variable and rename it dose_f before including it in the model. Don’t forget to load the tidyverse in order to use mutate().\nThe second line in the code below creates an ANOVA model, named aov2th.\n\n\n tg The name you want for your modified dataset  &lt;- The assignment operator. The result to the right of it gets stored in an object specified on the left  ToothGrowth A preloaded dataset in R  |&gt; The result on the left is piped into the first argument of the function on the right  mutate( A tidyverse function to compute a new column for a dataset  dose_f = The name you want to give to the new column  factor( A function to convert a variable from numeric to quantitative  dose A numeric variable in ToothGrowth  )) Functions always end with a closing parenthesis  aov2th A name you come up with for your model  &lt;- The assignment operator. The result to the right of it gets stored in an object specified on the left  aov( A function to define the model  len The response, or y, variable in the model. It is numeric.  ~ The ~ is like an equal sign in the model. Items on the left of ~ represent y, on the right you define independent factors (i.e. x’s).  dose_f Column in the tg dataset where doese is stored as a factor  * Crosses two factors. Both simple factors and the interaction factor are included in the model   supp variable with 2 levels of delivery method: orange juice or asorbic acid (vc)  , Seperates multiple input arguments to a function.  data = tg Tell the model that the variable names come from the tg dataset  ) Functions always end with a closing parenthesis \n\n\n\n\n\n\n\n\nA quick rule of thumb to check this assumption is to compare standard deviations across factor levels. If the largest standard deviation is no more than double the smallest standard deviation, then the standard deviations (and the variances) are close enough to be considered equal. Check the R Instructions&gt;Describing Data&gt;Numerical Summaries section of the textbook on how to calculate standard deviations for each factor level.\nIn cases with more than 1 factor, you can compare the standard deviation of each factor level combination (i.e. the interaction factor). Sometimes though, looking at the interaction results in a very small sample size at each level or you may be concerned about a particular factor level of an experimental factor. In that case you may want to apply this rule of thumb to each factor individually. When faced with a situation where the rule of thumb is met for some factors but not for others use your best judgement. An understanding of how a violation may affect your results is critical. You can see that this approach can be tricky to implement, especially as you go beyond studies with just two factors.\n\n\n\nAnother informal approach to checking the constant variance assumption is looking at a residual vs. fitted plot. Similar to the rule of thumb, in situations with more than 1 factor, you can either create a plot that shows all factor level combinations OR look at multiple plots, one for each experimental factor. In order to view this plot, you must first create the ANOVA model. Once the model is created, there are a couple of ways to get a residual vs. fitted plot.\nConstruct the plot manually from vectors within the aov object\nThe plot can be constructed using the vector of residuals and vector of fitted values contained in the aov object.\n\nmyaov &lt;- aov(Y~X*Z, data = YourDataSet)\nplot(myaov$fittedvalues, myaov$residuals)\n\nNote: If you want to know all the named items in an R object, you can run names(object). In this case we have an aov object called myaov. To see what it contains we can run names(myaov) in the console.\nExample code:\n\n\n\n plot( Base R function to create a scatterplot  aov2th$  Look in the aov2th object for the item named on the right of the $  fitted.values  This vector, stored in the aov object, contains the fitted, or predicted, values.  , Seperates multiple input arguments to a function  aov2th$ Look in the aov2th object for the item names on the right of the $  residuals Vector in the aov object that contains model residuals  ) Functions always end with a closing parenthesis  Toggle output Toggle Output. \n\n\n\nNote the resulting plot show 6 vertical groupings, one for each factor level combination. (3 dose levels x 2 levels of supp = 6 factor level combinations)\n\n\n\n\n\n\n\nConstruct the plot with a shortcut\nWhen the function plot() is called on an aov object, 4 diagnostic plots are produced. Instead of viewing all four, chose which ones to see with the which= argument. The first of the four plots is the residual vs. fitted plot.\n\nmyaov &lt;- aov(Y~X*Z, data = YourDataSet)\nplot(myaov, which = 1)\n\nExample code:\n\n\n\n plot( Base R function, when fed an aov object it produces 4 diagnostic plots  aov2th,  The aov object for our model  which = Which of the four diagnostic plots do we want  1 The first of the 4 plots is the residual vs. fitted plot  ) Functions always end with a closing parenthesis  Toggle output Toggle Output. \n\n\n\n\nIgnore the red line on the plot. It does not measure variance and so can be distracting.\n\n\n\n\n\n\n\n\n\n\nLevene’s test is a formal hypothesis test to determine if the variances are equal. In essence, this is an ANOVA F-test comparing sample variances across factor levels (as opposed to comparing sample means). A large p-value for the test indicates there is insufficient evidence to conclude one of the variances is different; and therefore the assumption of constant variance is met.\nThis test comes in handy when there are multiple factors in a study and it is burdensome to informally evaluate all their factor level combinations.\n\nmyaov &lt;- aov(Y~X*Z, data = YourDataSet)\ncar::leveneTest(myaov)\n\nExample code:\n\n\n\n car Levene’s test comes from the car package.   :: Allows you to reference functions from the package named on the left without having to load the entire package.  leveneTest( function to run Levene’s test  aov2th name of the aov object to run the test on  ) Functions always end with a closing parenthesis  Toggle output Toggle Output. \n\n\n\n\n\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(&gt;F)\ngroup  5  1.7086 0.1484\n      54"
  },
  {
    "objectID": "hoveRmd/model_diagnostics.html#constant-variance-assumption",
    "href": "hoveRmd/model_diagnostics.html#constant-variance-assumption",
    "title": "Model Diagnostics",
    "section": "",
    "text": "A key assumption for ANOVA tests is that the error, or residual term, has a constant variance across all factor levels. This is sometimes call homogeneity of variance, or homoscedasticity.\nWe explain three ways to check the assumption: rule of thumb when comparing standard deviations for each factor level, a visual assessment of the residual vs. fitted plot, and Levene’s test. These methods may not always agree. You should be aware of the underlying data. Understanding why this assumption is important and how it will affect results when violated will help you decide how to proceed after checking these diagnostics. It is also worth noting that the ANOVA F-test is robust in the face of mild to moderate violation of this assumption.\nWe will use the pre-loaded dataset ToothGrowth. To learn more about the dataset, run ?ToothGrowth in the console. len will be our response variable, supp is an independent factor, and dose is the other independent factor. We will analyze this as a two-way, basic factorial design. Because dose is stored as a numeric variable, we will convert it to a categorical variable and rename it dose_f before including it in the model. Don’t forget to load the tidyverse in order to use mutate().\nThe second line in the code below creates an ANOVA model, named aov2th.\n\n\n tg The name you want for your modified dataset  &lt;- The assignment operator. The result to the right of it gets stored in an object specified on the left  ToothGrowth A preloaded dataset in R  |&gt; The result on the left is piped into the first argument of the function on the right  mutate( A tidyverse function to compute a new column for a dataset  dose_f = The name you want to give to the new column  factor( A function to convert a variable from numeric to quantitative  dose A numeric variable in ToothGrowth  )) Functions always end with a closing parenthesis  aov2th A name you come up with for your model  &lt;- The assignment operator. The result to the right of it gets stored in an object specified on the left  aov( A function to define the model  len The response, or y, variable in the model. It is numeric.  ~ The ~ is like an equal sign in the model. Items on the left of ~ represent y, on the right you define independent factors (i.e. x’s).  dose_f Column in the tg dataset where doese is stored as a factor  * Crosses two factors. Both simple factors and the interaction factor are included in the model   supp variable with 2 levels of delivery method: orange juice or asorbic acid (vc)  , Seperates multiple input arguments to a function.  data = tg Tell the model that the variable names come from the tg dataset  ) Functions always end with a closing parenthesis \n\n\n\n\n\n\n\n\nA quick rule of thumb to check this assumption is to compare standard deviations across factor levels. If the largest standard deviation is no more than double the smallest standard deviation, then the standard deviations (and the variances) are close enough to be considered equal. Check the R Instructions&gt;Describing Data&gt;Numerical Summaries section of the textbook on how to calculate standard deviations for each factor level.\nIn cases with more than 1 factor, you can compare the standard deviation of each factor level combination (i.e. the interaction factor). Sometimes though, looking at the interaction results in a very small sample size at each level or you may be concerned about a particular factor level of an experimental factor. In that case you may want to apply this rule of thumb to each factor individually. When faced with a situation where the rule of thumb is met for some factors but not for others use your best judgement. An understanding of how a violation may affect your results is critical. You can see that this approach can be tricky to implement, especially as you go beyond studies with just two factors.\n\n\n\nAnother informal approach to checking the constant variance assumption is looking at a residual vs. fitted plot. Similar to the rule of thumb, in situations with more than 1 factor, you can either create a plot that shows all factor level combinations OR look at multiple plots, one for each experimental factor. In order to view this plot, you must first create the ANOVA model. Once the model is created, there are a couple of ways to get a residual vs. fitted plot.\nConstruct the plot manually from vectors within the aov object\nThe plot can be constructed using the vector of residuals and vector of fitted values contained in the aov object.\n\nmyaov &lt;- aov(Y~X*Z, data = YourDataSet)\nplot(myaov$fittedvalues, myaov$residuals)\n\nNote: If you want to know all the named items in an R object, you can run names(object). In this case we have an aov object called myaov. To see what it contains we can run names(myaov) in the console.\nExample code:\n\n\n\n plot( Base R function to create a scatterplot  aov2th$  Look in the aov2th object for the item named on the right of the $  fitted.values  This vector, stored in the aov object, contains the fitted, or predicted, values.  , Seperates multiple input arguments to a function  aov2th$ Look in the aov2th object for the item names on the right of the $  residuals Vector in the aov object that contains model residuals  ) Functions always end with a closing parenthesis  Toggle output Toggle Output. \n\n\n\nNote the resulting plot show 6 vertical groupings, one for each factor level combination. (3 dose levels x 2 levels of supp = 6 factor level combinations)\n\n\n\n\n\n\n\nConstruct the plot with a shortcut\nWhen the function plot() is called on an aov object, 4 diagnostic plots are produced. Instead of viewing all four, chose which ones to see with the which= argument. The first of the four plots is the residual vs. fitted plot.\n\nmyaov &lt;- aov(Y~X*Z, data = YourDataSet)\nplot(myaov, which = 1)\n\nExample code:\n\n\n\n plot( Base R function, when fed an aov object it produces 4 diagnostic plots  aov2th,  The aov object for our model  which = Which of the four diagnostic plots do we want  1 The first of the 4 plots is the residual vs. fitted plot  ) Functions always end with a closing parenthesis  Toggle output Toggle Output. \n\n\n\n\nIgnore the red line on the plot. It does not measure variance and so can be distracting.\n\n\n\n\n\n\n\n\n\n\nLevene’s test is a formal hypothesis test to determine if the variances are equal. In essence, this is an ANOVA F-test comparing sample variances across factor levels (as opposed to comparing sample means). A large p-value for the test indicates there is insufficient evidence to conclude one of the variances is different; and therefore the assumption of constant variance is met.\nThis test comes in handy when there are multiple factors in a study and it is burdensome to informally evaluate all their factor level combinations.\n\nmyaov &lt;- aov(Y~X*Z, data = YourDataSet)\ncar::leveneTest(myaov)\n\nExample code:\n\n\n\n car Levene’s test comes from the car package.   :: Allows you to reference functions from the package named on the left without having to load the entire package.  leveneTest( function to run Levene’s test  aov2th name of the aov object to run the test on  ) Functions always end with a closing parenthesis  Toggle output Toggle Output. \n\n\n\n\n\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(&gt;F)\ngroup  5  1.7086 0.1484\n      54"
  },
  {
    "objectID": "hoveRmd/model_diagnostics.html#normal-distribution",
    "href": "hoveRmd/model_diagnostics.html#normal-distribution",
    "title": "Model Diagnostics",
    "section": "Normal Distribution",
    "text": "Normal Distribution\nAnother key assumption for ANOVA tests is that the error, or residual, term follows a normal distribution. We use the Q-Q plot to check this assumption. There are two ways to create the Q-Q plot.\nWe will continue to use the aov2th model that was created at the beginning of the Constant Variance section.\nConstruct the Q-Q plot with a shortcut\nWhen the function plot() is called on an aov object, 4 diagnostic plots are produced. Instead of viewing all four, chose which ones to see with the which= argument. The second of the four plots is a normal Q-Q plot.\nExample code:\n\n\n\n plot( Base R function, when fed an aov object it produces 4 diagnostic plots  aov2th,  The aov object for our model  which = Which of the four diagnostic plots do we want  2 The second of the 4 plots is the normal Q-Q plot  ) Functions always end with a closing parenthesis  Toggle output Toggle Output. \n\n\n\n\n\n\n\n\n\n\n\nThe advantage of this method is that you can easily get the residual vs. fitted plot and the normal Q-Q plot with one command by providing the which = argument a vector containing the values 1 and 2. (The : is shorthand to create a vector that starts at the value of the left of the : and increments by 1 until reaching the value on the right side of the :.)\n\nmyaov &lt;- aov(Y~X*Z, data = YourDataSet)\nplot(myaov, which = 1:2)\n\nThe disadvantage of this method is that it can be difficult to determine if the points follow the line closely enough. To help with this decision, you may prefer to use the Q-Q plot from the car package.\nConstruct the Q-Q plot from the car package\nThe Q-Q plot from the car package provides boundary lines. When points are out of the boundaries that is evidence that the normal residual assumption is violated.\nYou can customize the way the acceptable region for points is designated. The default is shading a region. Below is code to draw dashed-line boundary.\nExample code:\n\n\n\n car qqPlot comes from the car package.   :: Allows you to reference functions from the package named on the left without having to load the entire package.  qqPlot( function to a Q-Q plot from the car package  aov2th, name of the aov object  envelope = Argument to control the formatting for the acceptable region for points  list( There are potentially many arguments to affect the envelope, so they are provided as a list.  style = “filled” shading, boundary “lines”, or none  “lines” designate the acceptable region with lines  ) Functions always end with a closing parenthesis  , A comma separates the inputs to a function, in this case the qqPlot function  id = FALSE This argument tells R to display the row number of the 2 most extreme vertical values or not.  ) Functions always end with a closing parenthesis  Toggle output Toggle Output."
  },
  {
    "objectID": "hoveRmd/model_diagnostics.html#independent-errors",
    "href": "hoveRmd/model_diagnostics.html#independent-errors",
    "title": "Model Diagnostics",
    "section": "Independent Errors",
    "text": "Independent Errors\nAn order plot can serve as a partial check of the assumption that the residuals are independent. If there are patterns/trends in the plot that may be grounds to say the assumption is violated.\nThe plot assumes that the dataset is sorted in the same order the data was recorded. If the data has been re-sorted or is from an observational study (i.e. the chronology of collection is unknown or irrelevant) the order plot does not make sense as a check of independence.\nExample code:\n\n\n\n plot( A function to plot the data  aov2th Name you gave your model  $ Access a named object within an object  residuals residuals of the model  ) Functions always end with a closing parenthesis  Toggle output Toggle Output."
  },
  {
    "objectID": "latin_square.html",
    "href": "latin_square.html",
    "title": "Latin Square",
    "section": "",
    "text": "The CB[1] design works well when there is only one variable to block on. What can be done when there are two nuisance factors to block on? If those two blocking factors and the treatment all have the same number of levels, then a latin square design should be used.\n\n\n\n\n\n\nWhen to use Latin Square\n\n\n\nLatin Square designs are appropriate when\n\nTwo blocking factors and one treatment factor\nAll three factors have the same number of levels\nTreatments can be assigned to experimental units\n\n\n\nPreviously, in the CB[1] design we used the toothbrush study and blocked on participant. If we only had 4 participants and we considered “order” a nuisance factor, we could use a Latin Square. By adding “order” as a blocking variable, we can ensure that the order of the treatments does not all become the same by random chance alone.\nBlocking on subjects and order of treatments is one of the most common applications of a Latin Square design in psychology. (Treatments, of course, must be something that can be assigned to experimental units). Be aware that in these types of experiments carry-over effects (such as learning and fatigue) can be problematic. Experimental protocols need to proactively address potential carry-over effects1.\nLatin Square designs are also common in agriculture, where they were originally developed. The field is divided into a square grid and treatments are randomly applied to each cell of the grid. The blocking variables are the row position and column position in the grid respectively.\nFigure 1 shows two images of Latin Square designs in agriculture, pulled from Bailey, Cameron, and Connelly’s 2008 article in American Mathematical Monthly 2. Figure 1 (a) is a picture of a 5×5 forestry experiment on a hill in Beddgelert in Wales. The experiment was “designed by Fisher, laid out in 1929, and photographed in about 1945”. Figure 1 (b) shows “a 6×6 experiment to compare methods of controlling aphids; conducted by Lesley Smart at Rothamsted Research, photographed in 2004.”\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n(b)\n\n\n\n\nFigure 1: Agricultural examples of latin squares\n\n\nRegardless of whether the application is psychology, agriculture or something else, the key design feature of a Latin Square design is that each treatment appears exactly once in each row and in each column of the square.\nThere are various extensions to this basic Latin Square idea. Graeco Latin Squares can be used if you have more than 2 variables to block on (provided all factors have the same number of levels, and the number is not 6). Replicated Latin Squares is useful if the number of experimental units is a multiple of (instead of exactly equal to) the number of treatment factor levels. Replicated Latin Squares (including “Latin Rectangles”) is discussed briefly here and here.\n\n\nIn the factor diagram, one nuisance factor’s levels are associated with the row partitions of the dataset. The other nuisance factor’s levels are marked by partitioning the dataset by columns. The treatment is assigned a letter inside the factor diagram. Figure 2 is a factor structure diagram for a Latin Square design where the controlled factors all have 4 levels. This is an unrandomized layout of the treatments: each row (and column) follows the same sequence of treatments. In the Design section we will learn to randomize the design.\n\n\n\nFigure 2: Factor Structure for Latin Square Design with 4 Treatment Levels\n\n\n\n\n\nEach factor (i.e. meaningful partition of the data) in Figure 2 corresponds to a term on the right hand side of Equation 1:\n\\[\ny_{ijk} = \\mu + \\alpha_i + \\beta_j + \\gamma_k + \\epsilon_{ijk}\n\\tag{1}\\]\nWhere\n\n\\(y_{ij}\\) is the observation that belongs to level i of \\(\\alpha\\), level j of \\(\\beta\\), and level k of \\(\\gamma\\).\n\\(\\mu\\) is the grand mean of the entire dataset.\n\\(\\alpha\\) is the effect of the block factor partitioned by rows\n\\(\\beta\\) is the effect of the block factor partitioned by columns\n\\(\\gamma\\) is the effect of the treatment factor, designated by a letter value. Each treatment appears exactly once in each row and each column.\n\\(\\epsilon\\) is the residual error term\n\nThe Latin Square is an incomplete block design. In other words, each treatment does not show up in each block. In other words, not all subscript combinations of i, j, and k will be observed. Therefore there are insufficient observations to estimate and test interaction effects.\nThe hypothesis for the treatment is\n\\[H_0: \\gamma_k = 0 \\text{ for all }k\\]\n\\[H_a: \\gamma_k \\ne 0 \\text{ for some }k\\]\n\n\nThe focus of the study is on the treatment factor. However, you can test the block factors in the same way you test the treatment factor.\n\n\n\nAn ANOVA model may be used to analyze data from a LS[1] design if the following requirements are satisfied.\n\n\n\n\n\n\n\n\nRequirements\nMethod for Checking\nWhat You Hope to See\n\n\n\n\nConstant variance across factor levels\nResidual vs. Fitted Plot\nNo major disparity in vertical spread of point groupings\n\n\nNormally Distributed Residuals\nNormal Q-Q plot\nStraight line, majority of points in boundaries\n\n\nIndependent residuals\nOrder plot\nNo pattern/trend\n\n\n\nFamiliarity with/critical thinking about the experiment\nNo potential source for bias"
  },
  {
    "objectID": "latin_square.html#factor-structure",
    "href": "latin_square.html#factor-structure",
    "title": "Latin Square",
    "section": "",
    "text": "In the factor diagram, one nuisance factor’s levels are associated with the row partitions of the dataset. The other nuisance factor’s levels are marked by partitioning the dataset by columns. The treatment is assigned a letter inside the factor diagram. Figure 2 is a factor structure diagram for a Latin Square design where the controlled factors all have 4 levels. This is an unrandomized layout of the treatments: each row (and column) follows the same sequence of treatments. In the Design section we will learn to randomize the design.\n\n\n\nFigure 2: Factor Structure for Latin Square Design with 4 Treatment Levels"
  },
  {
    "objectID": "latin_square.html#model-and-hypotheses",
    "href": "latin_square.html#model-and-hypotheses",
    "title": "Latin Square",
    "section": "",
    "text": "Each factor (i.e. meaningful partition of the data) in Figure 2 corresponds to a term on the right hand side of Equation 1:\n\\[\ny_{ijk} = \\mu + \\alpha_i + \\beta_j + \\gamma_k + \\epsilon_{ijk}\n\\tag{1}\\]\nWhere\n\n\\(y_{ij}\\) is the observation that belongs to level i of \\(\\alpha\\), level j of \\(\\beta\\), and level k of \\(\\gamma\\).\n\\(\\mu\\) is the grand mean of the entire dataset.\n\\(\\alpha\\) is the effect of the block factor partitioned by rows\n\\(\\beta\\) is the effect of the block factor partitioned by columns\n\\(\\gamma\\) is the effect of the treatment factor, designated by a letter value. Each treatment appears exactly once in each row and each column.\n\\(\\epsilon\\) is the residual error term\n\nThe Latin Square is an incomplete block design. In other words, each treatment does not show up in each block. In other words, not all subscript combinations of i, j, and k will be observed. Therefore there are insufficient observations to estimate and test interaction effects.\nThe hypothesis for the treatment is\n\\[H_0: \\gamma_k = 0 \\text{ for all }k\\]\n\\[H_a: \\gamma_k \\ne 0 \\text{ for some }k\\]\n\n\nThe focus of the study is on the treatment factor. However, you can test the block factors in the same way you test the treatment factor."
  },
  {
    "objectID": "latin_square.html#assumptions",
    "href": "latin_square.html#assumptions",
    "title": "Latin Square",
    "section": "",
    "text": "An ANOVA model may be used to analyze data from a LS[1] design if the following requirements are satisfied.\n\n\n\n\n\n\n\n\nRequirements\nMethod for Checking\nWhat You Hope to See\n\n\n\n\nConstant variance across factor levels\nResidual vs. Fitted Plot\nNo major disparity in vertical spread of point groupings\n\n\nNormally Distributed Residuals\nNormal Q-Q plot\nStraight line, majority of points in boundaries\n\n\nIndependent residuals\nOrder plot\nNo pattern/trend\n\n\n\nFamiliarity with/critical thinking about the experiment\nNo potential source for bias"
  },
  {
    "objectID": "latin_square.html#r-randomization",
    "href": "latin_square.html#r-randomization",
    "title": "Latin Square",
    "section": "R randomization",
    "text": "R randomization\n\nWide format\nIf the design is laid out similar to the factor structure diagram it is considered to be in wider format. This code creates a table in wider format of the unrandomized design.\n\n#First create the unrandomized layout design, labeling the rows and columns as 1, 2, 3, 4\nls_design &lt;- tibble(`1` = c(`1`  = \"A\", `2` =\"B\", `3`=  \"C\", `4` = \"D\"), \n                             `2` = c(\"B\", \"C\", \"D\", \"A\"), \n                             `3` = c(\"C\", \"D\", \"A\", \"B\"), \n                             `4` = c(\"D\", \"A\", \"B\", \"C\"))\nls_design\n\n# A tibble: 4 × 4\n  `1`   `2`   `3`   `4`  \n  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n1 A     B     C     D    \n2 B     C     D     A    \n3 C     D     A     B    \n4 D     A     B     C    \n\n\nOne simple way is to carry out the randomization is to use the sample() command in conjunction with the [] notation. The sample() command randomly shuffles the values it is given. The square brackets allow you to reference rows and columns of a matrix. The first argument in the square brackets refers to rows, the second argument refers to columns.\n\n#Then randomize rows\nset.seed(42)\nrow_randomized &lt;- ls_design[sample(nrow(ls_design)), ]\n\nset.seed(42)\n#Then randomize columns\nall_randomized &lt;- row_randomized[ , sample(ncol(row_randomized))]\n\nset.seed(17)\n#The above randomization steps can be combined into one command\nls_design[ sample(nrow(ls_design)), sample(ncol(ls_design))]\n\n# A tibble: 4 × 4\n  `3`   `4`   `1`   `2`  \n  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n1 D     A     B     C    \n2 C     D     A     B    \n3 B     C     D     A    \n4 A     B     C     D    \n\n\n\n\nLonger format\nThe R language often wants data in longer format. To create the unrandomized design in longer format, use this code:\n\ntibble(row_blocks = rep(1:4, each = 4), \n       column_blocks = rep(1:4, times = 4),\n       treatment = c(\"A\", \"B\", \"C\", \"D\", \n                     \"B\", \"C\", \"D\", \"A\", \n                     \"C\", \"D\", \"A\", \"B\",\n                     \"D\", \"A\", \"B\", \"C\"))\n\n# A tibble: 16 × 3\n   row_blocks column_blocks treatment\n        &lt;int&gt;         &lt;int&gt; &lt;chr&gt;    \n 1          1             1 A        \n 2          1             2 B        \n 3          1             3 C        \n 4          1             4 D        \n 5          2             1 B        \n 6          2             2 C        \n 7          2             3 D        \n 8          2             4 A        \n 9          3             1 C        \n10          3             2 D        \n11          3             3 A        \n12          3             4 B        \n13          4             1 D        \n14          4             2 A        \n15          4             3 B        \n16          4             4 C        \n\n\nTo randomize the rows and columns, put the vector 1:4 within the sample() command when defining the row blocks and the column blocks, as shown below:\n\n#randomize rows\nset.seed(42)\nrandomized_design &lt;- tibble(row_blocks = rep(sample(1:4), each = 4),\n                            column_blocks = rep(sample(1:4), times = 4),\n                            treatment = c(\"A\", \"B\", \"C\", \"D\", \n                                          \"B\", \"C\", \"D\", \"A\", \n                                          \"C\", \"D\", \"A\", \"B\",\n                                          \"D\", \"A\", \"B\", \"C\"))\nrandomized_design\n\nAfter carrying out the experiment and gathering data, a vector containing the observed values can be added to the dataset using mutate(), cbind(), or dplyr::bind_col() from the tidyverse. Only the cbind() option is shown in the code below.\n\n#Store observed data in a vector. \n# These observed values are copmletely made up\nobserved_values &lt;- c(-1, 2, 6, 11, -5, 2, 7, 5, 4,-2, -3, 8, 8, 2, 1, 0)\ncbind(randomized_design, observed_values)"
  },
  {
    "objectID": "latin_square.html#degrees-of-freedom",
    "href": "latin_square.html#degrees-of-freedom",
    "title": "Latin Square",
    "section": "Degrees of Freedom",
    "text": "Degrees of Freedom\nAs is the case with other designs learned so far, the grand mean factor is outside of all other factors. There is only one grand mean associated with the dataset, so there is just one degree of freedom associated with grand mean.\nThe general rule for calculating degrees of freedom states that the degrees of freedom for a factor are equal to the number of levels of that factor minus the sum of degrees of freedom of all outside factors.\nGrand mean is the only factor outside of intersection (rows), time of day (column), and treatment (algorithm)5. Therefore, for each of these factors we can take their number of levels and subtract one; the degrees of freedom for the 3 structural factors are 4-1 = 3.\nThere is one residual for each observation, therefore the number of factor levels for residual is equal to the sample size, 16. The residual factor is inside of all other factors, so degrees of freedom for residual is 16 – (1 + 3 + 3 + 3) = 66."
  },
  {
    "objectID": "latin_square.html#factor-effects",
    "href": "latin_square.html#factor-effects",
    "title": "Latin Square",
    "section": "Factor Effects",
    "text": "Factor Effects\nTo calculate factor effects, we start by calculating means for each level of every factor.\n\nFactor Means\nBelow is a table that contains observed throughput for the combinations of intersection, time of day, and algorithm (represented as A, B, C or D).\n\nObserved Throughput for Traffic Light Timing Algorithm Experiment\n\n\n\n8am\n11am\n2pm\n5pm\n\n\n\n\nIntersection 1\nA (32)\nB (33)\nC (47)\nD (53)\n\n\nIntersection 2\nB (36)\nD (53)\nA (42)\nC (54)\n\n\nIntersection 3\nC (51)\nA (44)\nD (62)\nB (49)\n\n\nIntersection 4\nD (81)\nC (78)\nB (72)\nA (73)\n\n\n\nThe grand mean and the mean of each factor level will be calculated for the 3 structural factors.\nThe grand mean is simply the mean of all the observations and is equal to 53.8.\nThe calculation to find the mean of an intersection (row) are as follows:\n\\[\n\\bar{y}_\\text{intersection 1} = \\bar{y}_{1\\cdot\\cdot} = \\frac{32 + 33 + 47 +  53}{4} = 41.3\n\\]\n\nSimilar calculations can be applied to obtain the row mean for the other rows.\nNow find the mean for each time of day (column). The calculation for the first column is shown below. Similar calculations can be applied to obtain the mean for each of the other columns as well.\n\\[\n\\bar{y}_\\text{8am} = \\bar{y}_{\\cdot 1 \\cdot} = \\frac{32 + 36 + 51 + 81}{4} = 50\n\\]\nTo find the mean for each algorithm, we add together the 4 observations that belong to each algorithm and divide by four.\n\\[\\begin{align}\n\\bar{y}_A = \\bar{y}_{\\cdot \\cdot 1} &= \\frac{32 + 44 + 42 + 73}{4} = 47.8 \\\\\n\n\\bar{y}_B = \\bar{y}_{\\cdot \\cdot 2} &= \\frac{36 + 33 + 72 + 79}{4} = 47.5 \\\\\n\n\\bar{y}_C = \\bar{y}_{\\cdot \\cdot 3} &= \\frac{51 + 78 + 47 + 54}{4} = 57.5 \\\\\n\n\\bar{y}_D = \\bar{y}_{\\cdot \\cdot 4} &= \\frac{81 + 53 + 62 + 53}{4} = 62.3\n\\end{align}\\]\nWe do not need to calculate means for residual error factor for two reasons. First, there is only one observation per level of residual error, so the mean is the observation itself. Second, nothing is inside of residual error. It is the last step in the process and its mean is not needed to calculate factor effects.\nThe means for each factor level are shown in Figure 4.\n\n\n\nFigure 4: Factor Level Means\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe means in the image are rounded to 1 decimal place to save on space, but calculations should take advantage of full decimal precision."
  },
  {
    "objectID": "latin_square.html#factor-effects-1",
    "href": "latin_square.html#factor-effects-1",
    "title": "Latin Square",
    "section": "Factor Effects",
    "text": "Factor Effects\nNow that means for each level of each factor are calculated, we can move on to calculate effects of the factor levels. We will use the general formula for calculating effect size,\n\\[\n\\text{factor level effect} = \\text{factor level mean} - \\text{sum of all outside factor effects}\n\\]\nFor the grand mean, there is only one level and there are no outside factors. Therefore, the effect due to grand mean is 53.8 (equivalent to its mean) and this affect is applied to all 16 observations.\nThe intersection factor has four levels: one for each intersection. To calculate the effect of an intersection, take the intersection mean and subtract it from the effect due to the grand mean factor. For the intersection 1 this looks like:\n\\[\n41.25 - 53.75 = -12.5\n\\]\nThis result indicates that the mean throughput at intersection 1 is 12.5 fewer cars than the grand mean. Effects for the other 3 intersections are found with a similar calcultion.\nTo find the effect of a specific time of day, subtract the grand mean from the level’s mean. For “8am”, the calculation is\n\\[\n50 - 53.75 = -3.75\n\\]\nEffects of the other times of day are similarly calculated.\nTo find the effect of timing algorithm A, subtract the grand mean from the mean of algorithm A:\n\\[\n47.75 - 53.75 = -6\n\\]\nSimilarly, the effect of algorithm B is \\(47.5 - 53.75 = 3.25\\), algorithm C’s effect is \\(57.5 - 53.75 = 3.75\\) and D’s effect is \\(62.26 - 53.75 = 8.5\\).\nLastly, the residuals (or residual effects) need to be calculated. The mean for each level of residual is simply the observation itself. Effects associated with an observation’s factor levels are subtracted from the observed value. Whatever is left over is considered the residual. In other words, we have applied the general rule for calculating effect size. For the residual factor, the effect can concisely be stated as “observed value - predicted value”.\nAs an example, the residual in the top left corner of the residual factor was obtained with this calculation:\n\\[\n32 - (53.75 + -12.5 + -3.25 + -6) = 0.5\n\\]\nFigure 5 uses the factor structure diagram to show the build-up of each observations as the summation of each of its factor level effects.\n\n\n\n\nFigure 5: Factor Effects"
  },
  {
    "objectID": "latin_square.html#completing-the-anova-table",
    "href": "latin_square.html#completing-the-anova-table",
    "title": "Latin Square",
    "section": "Completing the ANOVA Table",
    "text": "Completing the ANOVA Table\nNow that we have calculated degrees of freedom and effects for each factor, we can calculate the remaining pieces of the ANOVA table: Sum of Squares (SS), Mean Squares (MS), F-statistic and p-value. A completed ANOVA summary table contains the information we need for a hypothesis test of the treatment effect.\nIn an ANOVA table, each factor and their associated degrees of freedom are listed on the left. The total degrees of freedom are the total number of observations.\n\n\n\n\n\nSource\ndf\nSS\nMS\nFvalue\npvalue\n\n\n\n\nGrand Mean\n1\n\n\n\n\n\n\nIntersection (row)\n3\n\n\n\n\n\n\nTime of day (column)\n3\n\n\n\n\n\n\nAlgorithm (treatment)\n3\n\n\n\n\n\n\nResidual Error\n6\n\n\n\n\n\n\nTotal\n16\n\n\n\n\n\n\n\n\n\n\n\nTo get the sum of squares (SS) of a factor, the effects of the factor must be squared, and then summed. The factor effects were displayed in Figure 5 above. Figure 6 (below) shows squared effects for the factors, excluding the grand mean and the observations.\n\n\n\n\nFigure 6: Squared Factor Effects\n\n\n\nThe total sum of squares (\\(SS_{total}\\)) represents all the squared variability that we will need to allocate to the various factors. It is calculated by squaring each observation and then summing them together:\n\\[\nSS_{total} = 32^2 + 33^2 + … + 73^2 = 49856\n\\]\nTo get the Sum of Squares for the grand mean factor we first square the effect of grand mean, \\(53.75^2 = 2889.0625\\). That value occurs 16 times in the dataset (once for each observation), so \\(SS_\\text{grand mean} = 2889.0625 * 16 = 46225\\).\nTo get the sum of squares for each factor, we simply add all the squared effects. Since for intersections, time of day, and timing algorithm, each effect is repeated exactly 4 times, we will use some multiplication to simplify the calculation:\n\\[\\begin{align}\n\nSS_\\text{intersections} &= 4*(156.3 + 56.3 + 5.1 + 495.1) = 2850.5 \\\\\n\nSS_\\text{time of day} &= 4 *(14.1 + 3.1 + 4 + 12.3) = 133.5 \\\\\n\nSS_\\text{algorithm} &= 4 * (36 + 39.1 + 14.1 + 72.3) = 645.5 \\\\\n\nSS_\\text{residual} &= 4 * (0.25 + 0.06 + 0 + 0.06) = 1.5\n\\end{align}\\]\nPutting this information into the ANOVA table gives us Table 1.\n\n\n\n\nTable 1: Sums of squares\n\n\nSource\ndf\nSS\nMS\nFvalue\npvalue\n\n\n\n\nGrand Mean\n1\n46225.0\n\n\n\n\n\nIntersection (row)\n3\n2850.5\n\n\n\n\n\nTime of day (column)\n3\n133.5\n\n\n\n\n\nAlgorithm (treatment)\n3\n645.5\n\n\n\n\n\nResidual Error\n6\n1.5\n\n\n\n\n\nTotal\n16\n49856.0\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou can verify that we have successfully partitioned out the SS_total, try adding the sum of squares for all the factors together. The result should be equal to the sum of squares you got by squaring the observed values and summing them.\nThe next step is to covert the total variability (sum of squares) to an average variability per factor (mean squares). To create an average from a total, you must divide by the number of informative pieces of information that were summed to create the total, in this case the degrees of freedom. The mean squares can be thought of as the sample variance between factor level means.\nObtain the mean squares for each factor by dividing its sum of squares by its degrees of freedom.\n\n\n\n\n\nSource\ndf\nSS\nMS\nFvalue\npvalue\n\n\n\n\nGrand Mean\n1\n46225.0\n46225.00\n\n\n\n\nIntersection (row)\n3\n2850.5\n950.17\n\n\n\n\nTime of day (column)\n3\n133.5\n44.50\n\n\n\n\nAlgorithm (treatment)\n3\n645.5\n215.17\n\n\n\n\nResidual Error\n6\n1.5\n0.25\n\n\n\n\nTotal\n16\n49856.0\n3116.00\n\n\n\n\n\n\n\n\n\nThe objective of the study was to evaluate differences in timing algorithms of traffic lights. The intersection and time of day factors were simply nuisance factors we blocked on to better isolate the effect of timing algorithm. For that reason, we will only show the hypothesis test of the treatment factor (algorithm) here, though a similar test could be done for the blocking factors (intersection and time of day).\nIn Equation 1, \\(\\gamma\\) represents the effect of algorithm. Our hypotheses therefore are\n\\[\nH_o: \\gamma_k = 0 \\text{, for all } k\n\\]\n\\[\nH_a: \\gamma_k \\ne 0 \\text{, for some } k\n\\]\nTo test the hypothesis, we need to compare the mean square (MS) for algorithm to the mean square for residual error (abbreviated as MSE). This ratio of variances is called an F statistic.\n\\[\n\\text{F statistic} = \\frac{MS_{algorithm}}{MS_{error}} = \\frac{215.1\\overline{6}}{0.25} = 860.\\overline{6}\n\\]\nThis F statistic has 3 and 6 degrees of freedom, written as \\(F_{3,6} = 860.\\overline{6}\\). This is a huge F statistic.\nThe associated p-value is approximately zero as calculated in Excel with the function = f.dist.rt(860.6667, 3, 6).\n\n\n\n\n\nSource\ndf\nSS\nMS\nFvalue\npvalue\n\n\n\n\nGrand Mean\n1\n46225.0\n46225.00\n\n\n\n\nIntersection (row)\n3\n2850.5\n950.17\n\n\n\n\nTime of day (column)\n3\n133.5\n44.50\n\n\n\n\nAlgorithm (treatment)\n3\n645.5\n215.17\n860.667\n0\n\n\nResidual Error\n6\n1.5\n0.25\n\n\n\n\nTotal\n16\n49856.0\n3116.00\n\n\n\n\n\n\n\n\n\nWe can conclude from these results that at least one of the timing algorithms has a statistically significant effect on the number of cars flowing through an intersection. As calculated earlier, Algorithm D had the largest positive effect on throughput at an intersection and it seems reasonable to recommend this algorithm to those in charge of traffic lights."
  },
  {
    "objectID": "latin_square.html#check-assumptions",
    "href": "latin_square.html#check-assumptions",
    "title": "Latin Square",
    "section": "Check Assumptions",
    "text": "Check Assumptions\nFor a more detailed explanation of the code, output, and theory behind these assumptions visit the Assumptions page.\n\nConstant Variance of Residuals\nThe residuals need to demonstrate constant variance, regardless of the fitted or predicted value. We use a residual plot to check this assumption.\n\n\nCode\nplot(my_ls_aov, which = 1)\n\n\n\n\n\nFigure 8: Checking constant variance\n\n\n\n\n\n\nIgnore the red line in this plot\nThe x-axis of Figure 8 shows the fitted values, also called predicted values. An observation’s fitted value is the sum of the effects contributing to a datapoint: it includes all effects except for the residual effect. The residual is plotted on the y-axis.\nThe points in the plot do not show any increase (or decreasing) in vertical spread as we move along the x-axis. Therefore, we conclude this requirement is met.\n\n\nNormally Distributed Residuals\nWe check the assumption that residuals are normally distributed in Figure 9. All the points are in the shaded region.\n\n\nCode\ncar::qqPlot(my_ls_aov$residuals, id = FALSE)\n\n\n\n\n\nFigure 9: Checking normality of residuals\n\n\n\n\n\n\nIndependent Residuals\nThe dataset we are analyzing does not include information about the order in which the data was collected. In fact, it is possible some conditions of the experiment were run simultaneously and there is no specific order. From what we know, there is no reason to think there is a potential order bias."
  },
  {
    "objectID": "latin_square.html#summary",
    "href": "latin_square.html#summary",
    "title": "Latin Square",
    "section": "Summary",
    "text": "Summary\nThe ANOVA model assumptions all appear to be met. We can trust the p-values in the ANOVA summary table. Thus we conclude algorithm has a significant effect on throughput. To gain further insight, pairwise comparisons for the algorithm levels could be run."
  },
  {
    "objectID": "latin_square.html#footnotes",
    "href": "latin_square.html#footnotes",
    "title": "Latin Square",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA carry-over effect occurs when the effect of a treatment applied to block spreads beyond the borders of the block. For example, a fertilizer applied to a particular area is carried by wind or water to some adjacent area that is supposed to be receiving a different treatment.\nIn the case of temporal (rather than spatial) blocks, a carry-over effect occurs when the effect of the previous treatment influences the outcome for a particular subject even after the subject has begun a new phase of the experiment under a different treatment condition.\nCarry over effects also include when effects of repetition (such as learning or fatigue) are mixed with effects of the treatment, and the two become confounded.↩︎\nBailey, R. & Cameron, Peter & Connelly, R.. (2008). Sudoku, Gerechte Designs, Resolutions, Affine Space, Spreads, Reguli, and Hamming Codes. American Mathematical Monthly. 115. 10.1080/00029890.2008.11920542.↩︎\nSince this sequence is randomly generated, running the same code on your machine will give a different sequence each time you run it. You can create the sequence by first running set.seed(6), and then sample(1:4). If sample(1:4) command is run again without resetting the random seed or doing any other random sampling, the result will be the sequence which was obtained for the columns in step 2.↩︎\nSince this sequence is randomly generated, running the same code on your machine will give a different sequence each time you run it. You can create the sequence by first running set.seed(6), and then sample(1:4). If sample(1:4) command is run again without resetting the random seed or doing any other random sampling, the result will be the sequence which was obtained for the columns in step 2.↩︎\nDeciding whether the treatment factor is inside or outside of another factor is a little unusual since the partitions for treatment are not contiguous. However, the definition of outside or inside is still the same. Let’s look at the relationship between treatment at grand mean.\n\nTreatment A fits nicely inside of the Grand Mean partition. The partitions for the other treatment levels also fit in Grand Mean; so Treatment is “inside” of Grand Mean.\nTreatment and the Column Factor can also be investigated. In this case, level A of Treatment does not fit nicely inside of the column factor. In fact, one level of Treatment spans all 4 levels of the column factor. Treatment is certainly not inside of the Column Factor.\n\nWe can also see that the Column Factor partitions do not fit inside of Treatment.\n\nThe factors are crossed since each level of Treatment appears in combination with each level of the Column Factor. A similar process can be carried out to determine that Treatment and the Row Factor are also crossed.↩︎\nThe counting free numbers approach of determining degrees of freedom for residuals may be less clear to apply than it was in other designs due to the non-continuous partitions of the treatment factor. There are 3 structural factors: rows, columns, and treatments. (In the traffic example, that corresponds to intersections, times of day, and timing algorithm respectively). The residuals in each of these partitions must sum to zero.\nFor the treatment, all the residuals for observations from Treatment A must sum to zero. Similarly, the observations from Treatment B will have residuals that sum to zero. And so on for each treatment. It is not obvious in the factor diagram which observation should be chosen as “locked” and which ones we can count as free to vary. But consider for a moment that in any row, each of the treatments appears only once. We will select a row (let’s use the top row for convenience), and say that each of those residuals is “locked”. This essentially reduces the residual factor from a 4x4 table of free values, to a 3x4 table.\n\nNow apply the fact that in order to sum to zero across rows, the last value in each row is “locked”. Similarly, the residuals for each level of the column factor of the remaining 3x4 table of free values must also sum to zero. Therefore, the last (bottom) residual of each column is also “locked”.\n\nThis leads to a general result for finding residual degrees of freedom for a Latin Square: (# rows – 2) x (# columns – 1).↩︎"
  },
  {
    "objectID": "nested_intro.html",
    "href": "nested_intro.html",
    "title": "Introduction to Nested Designs",
    "section": "",
    "text": "The Split Plot, or Repeated Measures design is very similar to the Complete Block design, CB[1], with an extra wrinkle. Namely, an additional factor, referred to as the between-block factor, is added to the design. In the Complete Block design, blocks all come from the same population. In this new design, blocks can be assigned to different treatments (or for an observational factor, each block belongs to a different group). Thus, each level of the block factor is nested inside of the between-block factor; as a result, we often call these types of designs “nested designs”.\nAn example can make this more clear. Let’s start with a Complete Block experiment we are already familiar with…(wait for it)…the toothbrush example! Recall that a Complete Block design was described in detail on the Complete Block page. In summary, in that complete block design each person used each of the toothbrushes for a period of time. The order in which the brushes were used was randomized within each individual person.\nBut what if each block is different in some way? How can variation in the response due to different types of blocks be accounted for. For example, maybe some of the blocks (people) are adults and some are children. I may be interested in understanding how each brush performs in the two populations. I could even investigate the interaction of brush with age. For example, if one of the brushes performs better with children than with adults, I may want to market the brush as a children’s brush. In this case, “adult” and “child” are levels of the age factor, which is called a between-block factor.\n\n\n\nNov. 2023, https://childrensdentaldallas.com/what-happens-to-kids-who-dont-brush-their-teeth/\n\n\nBetween-block factors are not always observational factors, they can be experimental factors too (i.e. factors whose levels can be assigned). For example, in the toothbrush study, instead of comparing adults to children you may just be studying adults. You could randomly assign one set of people to floss and one group to not floss. Or, each person (block) may be assigned to eat a specific diet: protein vs. vegan vs. no restriction.\nEach block belongs to one level of the between-block factor. Thus, we say the block is nested within the between-block factor. The factor whose levels are randomized within each block is referred to as the within-block factor. There are 2 levels of experimental units:\n\nblocks serve as an experimental unit for the between block factor\nsub-divisions of the block or time slots serve as the experimental unit for the within block factor.\n\n\n\nThe models used to analyze nested designs are also called hierarchical models.\nThe block is usually a random factor. Models that have a mix of fixed and random factors are called mixed models."
  },
  {
    "objectID": "nested_intro.html#building-on-the-complete-block-design",
    "href": "nested_intro.html#building-on-the-complete-block-design",
    "title": "Introduction to Nested Designs",
    "section": "",
    "text": "The Split Plot, or Repeated Measures design is very similar to the Complete Block design, CB[1], with an extra wrinkle. Namely, an additional factor, referred to as the between-block factor, is added to the design. In the Complete Block design, blocks all come from the same population. In this new design, blocks can be assigned to different treatments (or for an observational factor, each block belongs to a different group). Thus, each level of the block factor is nested inside of the between-block factor; as a result, we often call these types of designs “nested designs”.\nAn example can make this more clear. Let’s start with a Complete Block experiment we are already familiar with…(wait for it)…the toothbrush example! Recall that a Complete Block design was described in detail on the Complete Block page. In summary, in that complete block design each person used each of the toothbrushes for a period of time. The order in which the brushes were used was randomized within each individual person.\nBut what if each block is different in some way? How can variation in the response due to different types of blocks be accounted for. For example, maybe some of the blocks (people) are adults and some are children. I may be interested in understanding how each brush performs in the two populations. I could even investigate the interaction of brush with age. For example, if one of the brushes performs better with children than with adults, I may want to market the brush as a children’s brush. In this case, “adult” and “child” are levels of the age factor, which is called a between-block factor.\n\n\n\nNov. 2023, https://childrensdentaldallas.com/what-happens-to-kids-who-dont-brush-their-teeth/\n\n\nBetween-block factors are not always observational factors, they can be experimental factors too (i.e. factors whose levels can be assigned). For example, in the toothbrush study, instead of comparing adults to children you may just be studying adults. You could randomly assign one set of people to floss and one group to not floss. Or, each person (block) may be assigned to eat a specific diet: protein vs. vegan vs. no restriction.\nEach block belongs to one level of the between-block factor. Thus, we say the block is nested within the between-block factor. The factor whose levels are randomized within each block is referred to as the within-block factor. There are 2 levels of experimental units:\n\nblocks serve as an experimental unit for the between block factor\nsub-divisions of the block or time slots serve as the experimental unit for the within block factor.\n\n\n\nThe models used to analyze nested designs are also called hierarchical models.\nThe block is usually a random factor. Models that have a mix of fixed and random factors are called mixed models."
  },
  {
    "objectID": "nested_intro.html#recognizing-when-to-use-a-nested-factor",
    "href": "nested_intro.html#recognizing-when-to-use-a-nested-factor",
    "title": "Introduction to Nested Designs",
    "section": "Recognizing When To Use a Nested Factor",
    "text": "Recognizing When To Use a Nested Factor\nNested factor designs commonly occur when one level of the experiment cannot be easily randomized because of physical constraints. Between-block factors often have limited replications compared to the within-block factor. Nested factor designs are common in agriculture, psychology and laboratory experiments as illustrated in the following 3 examples.\n\n\nIf both experimental factors of interest are easy to randomize and assign, then using a split-plot/repeated measures design unnecessarily complicates things. It may make sense to use a simpler design, such as using a 2x2 factorial design inside of blocks or a 2-way basic factorial assigned completely at random.\n\nAgriculture\nA researcher wants to test the impact of humidity, fungicide and their interaction on tomato yield in greenhouses. It is impossible to randomly assign humidity levels within a section of greenhouse. Therefore, each greenhouse is treated as a block. Greenhouse is the experimental unit for humidity because humidity level is randomly assigned to each green house. Humidity is the between-block factor.\nBecause levels of fertilizer are randomly assigned to different sections within each greenhouse, fertilizer is the within-block factor. Each Section within a greenhouse is an experimental unit for fertilizer.\n\n\nPsychology\nResearchers wanted to investigate the impact of sleep deprivation on cognitive performance. Recruiting participants who are willing to undergo sleep deprivation can be challenging due to potential physical and mental strain involved.\nEach participant was randomly assigned to one of these 3 sleep deprivation levels: one full night, one half night, and no deprivation. Then, the next day their cognitive performance was assessed in the morning, the evening, and again a full 24 hours after deprivation, and 48 hour after the sleep deprivation.\nParticipants are the blocks in this case. Because a level of sleep deprivation is assigned to each participant, sleep deprivation is the between block factor. All levels of time after deprivation appear in each and every block, so it is the within-block factor.\n\n\nLaboratory\nA lab manager would like to test the impact of freezer temperature on the life cycle of certain reagents. It is impossible to randomly assign temperature within a given freezer. The manager needs 2 or more freezers set at the same temperature level to get true replications of the between-block factor. The manager can replicate reagent (within-block factor) within freezers."
  },
  {
    "objectID": "nested_intro.html#common-pitfall-lack-of-replication",
    "href": "nested_intro.html#common-pitfall-lack-of-replication",
    "title": "Introduction to Nested Designs",
    "section": "Common pitfall: Lack of Replication",
    "text": "Common pitfall: Lack of Replication\nAs was mentioned earlier, in nested designs there are two levels of experimental units. Replication is needed for each level. A common pitfall is to overlook the need for replication of the between-block factor levels. The following story illustrates the problem.\nFarmer John would like to test the effectiveness of a name-brand fungicide spray (A) compared with the less expensive, generic brand (B). Fungi can have a severely negative impact on crop yield. Farmer John could plant his field as usual and spray half with fungicide A and half with fungicide B which would look like:\n\n\n\nFigure 1: Farmer John’s Field\n\n\nFarmer John’s design has no replication! Replication is an independent repetition of the same conditions on different experimental units. Recall that an experimental unit is the object to which a treatment is independently applied. In this case, each plot of land receives the entire application. There is only one experimental unit receiving fungicide A and one experimental unit receiving fungicide B…no replication.\nNow imagine Farmer John’s son is visiting home from college and recognizes the problem and comes up with a brilliant solution. He says they can get all the replication they need by sampling from different places within the field and observing the yields at each sampling point. They could get ten “replicates” within each field and analyze the data using standard statistical tools.\n\n\n\nFarmer John’s sub-samples\n\n\nBy simply looking at the resulting dataset below, the statistical software cannot tell that these data points are not independent applications of the fungicide. (A trusting or a naive statistician would also be fooled unless they asked follow-up questions about the design of the experiment). In reality though, the fungicide was only applied to one experimental unit. Though multiple measurements were taken, there is still no replication.\nclick twice to show/hide data\n\n\n\n\n\n\n\n\n\n\n\nreplicate\nfungicide\nyield\n\n\n\n\n1\nA\n501\n\n\n2\nA\n507\n\n\n3\nA\n482\n\n\n4\nA\n522\n\n\n5\nA\n498\n\n\n6\nA\n487\n\n\n7\nA\n497\n\n\n8\nA\n501\n\n\n9\nA\n500\n\n\n10\nA\n442\n\n\n1\nB\n506\n\n\n2\nB\n536\n\n\n3\nB\n514\n\n\n4\nB\n516\n\n\n5\nB\n510\n\n\n6\nB\n524\n\n\n7\nB\n519\n\n\n8\nB\n504\n\n\n9\nB\n499\n\n\n10\nB\n539\n\n\n\n\n\n\nSub-samples can provide useful information about how yields vary within a plot, but they are not replications of fungicide application. Treating sub-samples as a replication of fungicide is like counterfeiting degrees of freedom. It is important to be able to identify false replication. False replication gives a false sense of statistical sophistication for a simple anecdote. We should be careful to compare treatments with the appropriate degrees of freedom for error.\nIf we are not interested in the within-field variation, the easiest thing to do is average over the sub-samples and analyze the experiment in a typical manner. In the above example it becomes clear that we have no replication and cannot get an F-statistic.\n\n\n\n\n\n\nCaution\n\n\n\nIt may be helpful to review Nested Factors (on the Factor structure page) and the page on Experimental Units.\nA review of blocking may also be useful."
  },
  {
    "objectID": "nested_intro.html#other-terminology",
    "href": "nested_intro.html#other-terminology",
    "title": "Introduction to Nested Designs",
    "section": "Other Terminology",
    "text": "Other Terminology\nWe conclude this chapter with a discussion about different terminologies used to describe these designs. This text uses the generic terms “between-block factor”, “block”, and “within-block factor” to describe the factors in a nested design. However, depending on the field (pun intended!) in which you are working, different names are used to describe these factors and designs.\n\nSplit Plot\nAgricultural contexts prefer to use the terms “whole-plot factor”, “whole-plot”, and “sub-plot factor” respectively. This provides greater meaning since in agriculture these designs were developed to deal with planting different plots, or fields. The larger experimental unit is a whole field, and the smaller experimental unit is a sub-division of a field. Nested designs are more commonly referred to as Split Plot designs because a plot of land is being split-up, or sub-divided, in order to treat it as a block.\n\n\nRepeated Measures\nA separate set of terms will likely be used if the researcher is interested in the effect of a treatment over time. Blocks are not created by sub-dividing, but through repeated measurements of the experimental unit over different time periods. Hence the name, Repeated Measures.\nIn this context, blocks are usually subjects (people) and are the larger experimental unit. The factor applied to subjects is called the “between-subjects factor”. The time period, or time slot, is the smaller experimental unit. The factor varied over time is the within-subjects factor.\nIn a repeated measures design we do not refer to subjects as whole-plots and repeated measurements across time as sub-plots, though they behave functionally the same in the analysis. Th table summarizes the terms.\n\nTerminology Summary Table\n\n\n\n\n\n\n\n\n\nGeneric\nSub-dividing Experimental Units to Get Blocks\nRe-using Experimental Units to Get Blocks\n\n\n\n\nDesign Name\nNested Designs\nSplit Plot\nRepeated Measures\n\n\nLarger Experimental Unit\nBlock\nWhole Plot\nSubject\n\n\nFactor Applied to Blocks\nBetween-Block Factor\nWhole Plot Factor\nBetween Subject Factor\n\n\nSmaller Experimental Unit\nUnit, or observation\nSub-plot\nPoint in time, Time slot\n\n\nFactor Applied to Unit\nWithin block Factor\nSub-plot Factor\nWithin Subject Factor"
  },
  {
    "objectID": "random_v_fixed.html",
    "href": "random_v_fixed.html",
    "title": "Random Factors and EMS",
    "section": "",
    "text": "This page of the book seeks to introduce you to the concept of random factors, as opposed to fixed factors. Definitions of these terms and guidance about when a factor should be considered fixed or random is given. An explanation of how the F test for a factor is affected by random factors is given."
  },
  {
    "objectID": "random_v_fixed.html#what-is-a-random-factor",
    "href": "random_v_fixed.html#what-is-a-random-factor",
    "title": "Random Factors and EMS",
    "section": "What is a Random Factor?",
    "text": "What is a Random Factor?\nIn the designs we have dealt with so far, we have been primarily focused on the effect of a treatment. These treatment factors have been fixed factors. The term “fixed” is meant to reinforce the fact that the levels of these factors is what we are interested in studying. The levels of these factors are chosen deliberately. They represent the population of factor levels we would like to study. If the levels of these fixed factors changed, the research question we are attempting to know would also need to change.\nIn contrast, levels of a random factor represent a subset of all possible levels the factor can take on. Though we would like to observe all levels of the factor, it may be impossible to do because there are too many levels. So a sample of the factor levels is taken. Actually, at least one random factor has been present in every design we have worked with: the residual factor is a random factor. The residual factor represents the effect of each observational unit. Though we would like to observe everyone in the population, most populations are too large for that to happen. So we take a random sample of people. Each observational unit is a level of the residual factor.\nWhen a random factor is included in a study, the researcher is typically interested in measuring the variability in the random factor, rather than estimating the effect of each level in the study. Again, the residual factor is a good example. The effect of an individual is not of interest, rather estimating the mean squared error (i.e. the variability among individuals) is the focus.\n\n\n\n\n\n\nDefinitions\n\n\n\nRandom Factor: A factor whose levels represent a sample from a population of levels the factor has.\nFixed Factor: A factor whose levels represent the entire population (or nearly so) of values the factor can have.\n\n\nDeciding whether a factor should be treated as fixed or random depends on the context and purposes of the research. Consider the following example, which illustrates how the same factor could be treated as fixed or random, depending on the situation.\n\nTyping Speed Example\nA researcher was interested in estimating the mean typing speed of college freshman at her university. A random, representative sample of 300 freshman was obtained. The researcher plans to ask each freshman to type a passage that is 200 characters long.\nHer colleague points out the mean typing speed can depend on the nature of the passage freshman are asked to type. The researcher uses artificial intelligence (AI) to create 10 different passages. The 10 passages represent a sample of an infinite number of passages that could have been created. Each freshman is randomly assigned one of the 10 passages to type.\nThe researcher is not interested in the effect of any one passage in particular, just like she is not interested in any one student’s typing speed. Rather, the researcher can estimate how much variability in a person’s typing speed is due to variability in the passage being typed. The null hypothesis regarding the different passages is whether the variance in typing speed between the passages is zero.\n\n\nThe null hypothesis is similar to the null hypothesis of a fixed factor, but is conceptually and computationally different. Rather than estimating and testing each passage’s mean speed (which uses up many degrees of freedom), we simply estimate the variance in mean speeds (which uses just 1 degree of freedom).\nIn the example above, the passages were “randomly” sampled (or created) without consideration of content. However, if each of the 10 passages was created to represent a specific type of passage, then passage should be treated as a fixed variable. For example, one passage may incorporate many numbers. Another could have been created to include lots of punctuation. Another could include lots of unusual letters such as ‘q’ and ‘z’. Yet another passage might include a lot of capitalization, and so on. When each passage is deliberately meant to represent a particular category of interest, it may be more appropriate to treat the passage factor as fixed.\n\n\nWidget Manufacturing Example\nConsider another example that illustrates the difference between fixed and random factors.\nA manufacturer of widgets is interested in improving the quality of its products. The operator of the machine that makes the product is a major source of potential variation. The manufacturing company has designed an experiment that will gather widget quality data made by 4 different machine operators.1\nMachine operator as a fixed factor: There are only a handful of people at the company that operate the machine. They have been with the company for quite some time. If this is the case, treating the machine operator factor as fixed makes a lot of sense. The 4 levels of machine operator may represent the entire population (or nearly so) of operators. Knowing which operator is producing sub-par quality would be important so that training could be provided to that individual.\nEach machine operator will have their own quality distribution for the widgets they made. Figure 1 shows the quality distribution for the 4 operators in the study. In this case, we would gather a few observations from each of the distributions. We could use the data to estimate the true mean and effect of each operator.\n\n\n\n\n\nFigure 1: Operator as a Fixed Factor\n\n\n\n\nMachine operator as a random factor: There is quite a lot of churn at the machine operator position. No one person stays for very long. In this situation, 4 operators would represent a small sample from the population of machine operators that have previously worked, currently work, or may work the machine in the future. If the factor for machine operator turns out to be significant, the company may choose to improve overall processes or hiring practices rather than identifying individual machine operators to train.\nFigure 2 illustrates that there are many operators in the population, each with their own distribution of widget quality. However, not all of them can be observed. We will use observations from 4 randomly selected operators (shown in color) to estimate the variance in machine operator means.\n\n\n\n\n\nFigure 2: Operator as a Random Factor\n\n\n\n\nIn Figure 2, where machine operator is treated as a random factor, it can be seen that there is more variability in the overall distribution responses than can be observed from looking at just 4 operators.\nIn essence, there are 2 levels of sampling:\n\nsample of operators\nsample of widgets produced by the selected operators.\n\nThis extra sampling step of choosing operators adds uncertainty, i.e. increased standard error, to parameter estimates. In the typing example there were also two samples contributing variability: sampling of the freshman, and a sample of passages to be typed.\nThis increased variability can be seen when comparing the mathematical expression of the models. First, let’s revisit what the one-way anova model with a fixed factor looks like algebraically.\nIf machine operator is treated as a fixed factor, the fixed effects model can be written as\n\\[\nY\\_{ij} = \\mu + \\alpha\\*i +\\* \\epsilon{ij} \\text{,    where}\n\\epsilon\\_{ij} \\sim N(0, \\sigma\\^2)\n\\tag{1}\\]\nThis notation means that the errors follow a normal distribution with mean equal to zero and a variance of sigma-squared. The errors are independent of each other. A more abbreviated form of Equation 1 could be rewritten as\n\\[\nY_{ij} \\sim N(\\mu + \\alpha_i, \\sigma^2)\n\\tag{2}\\]\nIn contrast, Equation 3 is the algebraic model when machine operator is treated as a random factor. Note the similarities and differences between Equation 3 and Equation 1. Machine operator effects is denoted with a capital alpha rather than the lower case alpha to help distinguish the two models.\n\\[\nY_{ij} = \\mu + A_i + \\epsilon_{ij}, \\text{  where  }\\\\\n\\\\\n\\epsilon \\sim N(0, \\sigma^2), \\text{ and } A_i \\sim N(0, \\sigma_A^2)\n\\tag{3}\\]\nMore importantly, the machine operator effects are now treated as observations from a random variable, which has its own mean (zero) and variance (\\(\\sigma_A^2\\)).\n\n\nUnlike the fixed effect model where the sum of the factor level effects (\\(\\alpha_i\\)) sum to zero, the sum of the observed random effects, (\\(A_i\\)), do not necessarily sum to zero.\nSince both random effects, \\(A \\text{ and } \\epsilon\\), have a mean of zero, the mean of y is still mu. However, the variance of y needs to incorporate the variance contributed by both random effects, \\[\n\\sigma_y^2 = \\sigma_A^2 + \\sigma^2\n\\tag{4}\\]\n\n\nEquation 4 assumes that epsilon and alpha are mutually independent, in other words, their covariance is zero.\nAs mentioned earlier in the typing example, the null hypothesis for a random factor is that the variance of the factor level means is zero and can be written as\n\\[\nH_o: \\sigma_A^2 = 0\n\\]"
  },
  {
    "objectID": "random_v_fixed.html#how-to-decide-if-a-factor-should-be-treated-as-random-or-not",
    "href": "random_v_fixed.html#how-to-decide-if-a-factor-should-be-treated-as-random-or-not",
    "title": "Random Factors and EMS",
    "section": "How to Decide if a Factor Should be Treated as Random or Not",
    "text": "How to Decide if a Factor Should be Treated as Random or Not\nIn reality, it can be difficult to determine whether a factor should be treated as fixed or random. Cobb (p. 561) provides some rules of thumb that may help you make the distinction of whether to treat a variable as random or fixed.2\n\nObservational units (i.e. the residual factor) should always be random\nBlocks, and other nuisance factors, are usually random\nA factor nested inside of another factor is usually random\nExperimental factors (factors that are purposefully manipulated in an experiment) are usually fixed\n\nHere are some other questions to ask yourself as you grapple with the decision.3\n\nDo the levels represent a tiny fraction of all possible levels (random), or a nearly exhaustive list of all possible levels (fixed)?\nIs the underlying nature of the variable continuous? If so, the variable should be treated as fixed. The reason being that levels are chosen to span a range of reasonable values on the continuum, the levels were (hopefully) not chosen at random.\n\nIf a variable is put into the model as a continuous variable, then notion of fixed vs. random no longer applies (it is considered fixed).\n\nAre there 4 or fewer levels? If so, it often makes sense to treat the variable as fixed, even if it is truly random. This is because estimating a variance from a sample of just 3 or 4 is not very reliable and it greatly complicates the model.\nAre you interested in reporting the mean of each level (fixed) or the variance between levels (random)?\nIf you repeated the experiment, would/could the levels of the factor stay the same (fixed), or change (random)?"
  },
  {
    "objectID": "random_v_fixed.html#mixed-models-and-mixed-effects",
    "href": "random_v_fixed.html#mixed-models-and-mixed-effects",
    "title": "Random Factors and EMS",
    "section": "Mixed Models and Mixed Effects",
    "text": "Mixed Models and Mixed Effects\nWhen a model has fixed and random factors it is called a “mixed model”. An example of this is the SP/RM model.\nWhat about the interaction of two factors? If two random factors are crossed, their interaction is treated as a random factor. If two fixed factors are crossed, their interaction is treated as a fixed factor. If a random factor and a fixed factor are crossed, the result is called a mixed factor or mixed effect. Dealing with mixed factors is outside the scope of this book."
  },
  {
    "objectID": "random_v_fixed.html#how-random-factors-impact-the-f-test",
    "href": "random_v_fixed.html#how-random-factors-impact-the-f-test",
    "title": "Random Factors and EMS",
    "section": "How Random Factors Impact the F-Test",
    "text": "How Random Factors Impact the F-Test\n\nRevisit the F-Test\nOn the ANOVA and the F-test page the F test is presented as a ratio of variances. Specifically, the ratio of variance between factor level means to the variance of individual observations within factor levels. Let’s take a deeper look at this ratio.\nIn a simple one-factor ANOVA, there are actually two components contributing to the variance between factor level means: the variance due to different treatments and the variance due to individuals (which is also referred to as residual error). Recognizing that “Mean Squares” is synonymous with “variance”, we can write the following to express this concept:\n\\[\nMS_{treatment} = \\text{ treatment effects} + \\text{ residual error}\n\\]\n\n\nThe residual error component is why we estimate factor level means instead of knowing them with certainty. If there was no random errors, we would know the population parameters with certainty\nThe denominator in an F statistic calculation has traditionally been the Mean Squared error (or residual error). Thus,\n\\[\nF = \\frac{\\text{treatment effects} + \\text{residual error}}{\\text{residual error}}\n\\tag{5}\\]\nWhen the treatment effects are small or insignificant, we expect the F statistic to be close to 1.\n\\[\nF =\n\\frac{0 + \\text{residual error}}{\\text{residual error}} = \\frac{\\text{residual error}}{\\text{residual error}} = 1\n\\tag{6}\\]\nIn Equation 5, the only difference between the numerator and the denominator is the variance due to treatment. Thus, the F statistic ratio is how we isolate the effect of the treatment. Our approach is to deliberately construct the F statistic such that the only difference in the numerator and denominator is the addition to the numerator of the factor of interest.\nOf course, even if the true effect of the treatment was zero, due to random sampling the F statistic will not always be exactly 1. However, if the true effect was zero and we were able to observe all possible samples (in some fantasy, hypothetical world), the mean of the F statistics would be 1. This notion of resampling and getting different statistics is called a sampling distribution. Each variance estimate has its own sampling distribution (just like a sample mean has a sampling distribution)\n\n\nSee Math221 chapter 6 for a review of sampling distributions.\nThe term Expected Mean Square (EMS) is used to refer to the mean of a sampling distribution of the variance. Rather than calculate a value for the EMS, our goal is simply to identify factors of a design that contribute to a factor’s EMS. This will help our understanding of the appropriate F statistic calculation.\n\n\n\n\n\n\nImportant\n\n\n\nThe pieces of a model that contribute to a factor’s expected mean square (EMS) are the treatment factor itself and all random, inside factors.\n\n\nThis means there will be cases when the denominator of an F statistic is not simply the Mean Squared Error. Let’s walk through construction of the F statistic in 3 different examples:\n\nA completely randomized two-way ANOVA, where both experimental factors are fixed\nA completely randomized two-way ANOVA, where both experimental factors are random\nA split-plot / repeated-measures design, where blocks are random.\n\n\n\nExample 1: BF[2] Where Both Factors Are Fixed\nThis generic example gives a rationale for the analysis presented on the BF[2] page. The table below is another way to verify that F statistics calculated on the BF[2] page are correct. The first column lists each of the factors in the design. The second column shows the EMS, or in other words, what factors are contributing to the observed variability in that factor. The third column displays the F-test calculation and the last column explicitly identifies the denominator in the F-test calculation.\n\nF-test Calculation for BF[2] with Two Fixed Factors\n\n\n\n\n\n\n\n\n\nEMS\nF statistic\nDenominator of F statistic\n\n\n\n\nGrand Mean (G)\nG + E\n\\(\\frac{G + E}{E}\\)\nMean Square Error\n\n\nFactor A\nA + E\n\\(\\frac{A + E}{E}\\)\nMean Square Error\n\n\nFactor B\nB + E\n\\(\\frac{B + E}{E}\\)\nMean Square Error\n\n\nA x B\nAB + E\n\\(\\frac{AB + E}{E}\\)\nMean Square Error\n\n\nError term (E) (random)\nE\n-\n\n\n\n\nNotice that in every F statistic the denominator is just the variance of the error term (i.e. Mean Squared Error). Because all the factors are fixed, the only other source of variation contributing to the variance in factor level means (besides the factor itself) is the error term. When both factors are random, we shall see this is not the case.\n\n\nExample 2: BF[2] Where Both Factors Are Random\nRemember, the expected mean square (EMS) is composed of the treatment itself and all random, inside factors.\n\nF-test Calculation for BF[2] with Two Random Factors\n\n\n\n\n\n\n\n\n\nEMS\nF statistic\nDenominator of F statistic\n\n\n\n\nGrand Mean (G)\nG + A + B + AB + E\nNA\nNA\n\n\nFactor A\nA + AB + E\n\\(\\frac{A + AB + E}{AB + E}\\)\nMean Square for A x B\n\n\nFactor B\nB + AB + E\n\\(\\frac{B + AB + E}{AB + E}\\)\nMean Square for A x B\n\n\nA x B\nAB + E\n\\(\\frac{AB + E}{E}\\)\nMean Square Error\n\n\nError term (E) (random)\nE\n-\n\n\n\n\nFirst, notice that there is no appropriate test for the grand mean. To correctly isolate the effect of the grand mean in the F test numerator, we would need a factor with an expected mean square of A + B + AB + E to put in the denominator. None of the factors have that for their EMS, and therefore the F test for grand mean cannot be done. This is not too concerning though because we generally already assume the value is not zero, or simply don’t care.\nThe F test for Factor A and Factor B both use the Mean Squares of AB interaction in the denominator of the F test. This is needed so that the only difference between the numerator and denominator of the F statistic is the contribution of the factor being tested (in the numerator).\n\n\nExample 3: Split Plot/Repeated Measures\nLet’s look at a bit more complex (and realistic) scenario. For more about split plot / repeated measure designs, read that chapter.\n\nF-test Calculations for SP/RM design\n\n\n\n\n\n\n\n\n\nEMS\nF statistic\nDenominator of F statistic\n\n\n\n\nGrand Mean (G)\nG + S + E\n\\(\\frac{G + S + E}{S + E}\\)\nMean Square for Blocks\n\n\nFactor A\nA + S + E\n\\(\\frac{A + S + E}{S + E}\\)\nMean Square for Blocks\n\n\nBlocks Factor, S (random)\nS + E\n\\(\\frac{S + E}{E}\\)\nMean Square Error\n\n\nFactor B\nB + E\n\\(\\frac{B + E}{E}\\)\nMean Square Error\n\n\nA x B\nAB + E\n\\(\\frac{AB + E}{E}\\)\nMean Square Error\n\n\nError term (E) (random)\nE\n-\n\n\n\n\nIn this hierarchical4, mixed model the mean squares for block (S) is used as the denominator for the parts of the model that are at the “higher” level of the hierarchy. In other words, the blocks are the experimental units to which Factor A is applied, and so their Mean squares (i.e. variance) is the denominator for the F test. The block factor and everything else uses the Mean Squared error in the denominator of their F test. Unless you are using a software/package designed for mixed models (such as R’s lme4), the software may output the incorrect F statistic by default."
  },
  {
    "objectID": "random_v_fixed.html#summary",
    "href": "random_v_fixed.html#summary",
    "title": "Random Factors and EMS",
    "section": "Summary",
    "text": "Summary\nIn summary, knowing what is considered a random factor vs. fixed factor is important because it can effect calculation of the F test and other parts of the model In practice, deciding whether a factor should be treated as fixed or random can be a difficult decision to make. Use of random factors and fixed factors in the same study introduces a whole new class of models called “mixed models”. Identifying contributors to a factor’s EMS is critical to creating the right F statistic in a mixed model. Generally, the strategy is to structure the F statistic such that the only difference in the numerator and denominator of the F statistic calculation is the variance of the test you are attempting to test."
  },
  {
    "objectID": "random_v_fixed.html#footnotes",
    "href": "random_v_fixed.html#footnotes",
    "title": "Random Factors and EMS",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLater in this chapter we will suggest that it may be preferrable to treat random factors with very few levels in a design as a fixed effect purely from a practical stand point.↩︎\nCobb, G. W. (2014). Introduction to Design and Analysis of Experiments. Wiley.↩︎\nhttps://dynamicecology.wordpress.com/2015/11/04/is-it-a-fixed-or-random-effect/\nCobb, G. W. (2014). Introduction to Design and Analysis of Experiments. Wiley. p. 559.↩︎\nHierarchical means there are two levels of experimental units.↩︎"
  },
  {
    "objectID": "research_objectives.html",
    "href": "research_objectives.html",
    "title": "Research Objectives",
    "section": "",
    "text": "Research objectives focus on the outcomes that the researchers hope to accomplish during their study. This may be done by forming questions of interest and determining how those questions can be answered. An effective way to define research objectives is to follow the SMART process: specific, measurable, achievable, relevant, and time-based.\nSpecific: Define your desired outcomes. These outcomes need to be concisely written. They should be written in a way to reduce confusion. Refine these outcomes as you consider the next steps along the SMART process.\nMeasurable: Objectives will be better understood and accomplished when they are measurable. Designed experiments require the collection of data to determine how objectives are met. The population of the study should be considered, so that the collected data are representative of that population. The collected data need to be appropriate and able to answer the questions posed by the objectives. Some concepts, like happiness, depression, or engagement are not directly measurable. Ensure that whatever is measured in the study can adequately address research objectives. In other words, your study needs to have validity. Validity is the degree a study or a measure actually represents what it claims to represent. The validity of a study may be affected by the quality of the sample and/or the choice of response variable. It is discussed more in the Response Variable page.\nAchievable: Realistic expectations need to be considered when determining objectives. Studies are more effective when the number of objectives is limited and well-focused. Time, resources, and budget need to be strongly considered when creating achievable objectives.\nRelevant: Be careful to make sure that your objectives are relevant to your research and overall goals. Review the literature available concerning your research to help drive your objectives towards relevant outcomes that will be appreciated by others in that research area.\nTime-based: Create a schedule for the main elements of the experiment to keep your research on track. This can be done by defining inch-stones (small tasks and deadlines) and milestones (major tasks and deadlines).\nNow, we’ll apply the SMART process to the toothbrush study example.\n\nSpecific: The research objectives for this study will be 1) determining how well each toothbrush type reduces plaque and 2) determining if there are any significant differences in plaque reduction between these toothbrush types.\nMeasure: Plaque coverage over teeth, measured as a percent of surface area, is measurable with the use of plaque staining dye, an oral camera, and software.\nAchievable: There are only two objectives mentioned in the research objectives, both of which can be answered with the defined measure, keeping this study simple and focused.\nRelevant: Considering that the purpose of our research may be to determine which toothbrush to use, this study should be relevant to those that brush their teeth.\nTime-based: Three inch-stones along the schedule will help us keep on track: 1) find the subjects to be used in the study; 2) collect the data, which would include a 2-month brushing window before collecting the data; and 3) analyze the data. The milestone would be the final report."
  },
  {
    "objectID": "sources_of_variances.html",
    "href": "sources_of_variances.html",
    "title": "Sources of Variation - Factors and Conditions",
    "section": "",
    "text": "Once the response variable is determined, it is important to identify all possible factors and conditions that could influence the data measurements. Factors are often referred to as explanatory variables because they explain the variation in the response variable. They are also called independent variables since the response variable is thought to depend on the value of the factor. For each factor you identify, determine how you want it to influence your experiment. List those factors that you will be controlling during the experiment. List those factors and conditions that you will purposely hold constant. List those factors and conditions that you cannot control and determine if the values of those factors can be measured or not. Each of these lists will now be discussed in further detail.\n\n\nThese factors are of high importance because the experiment is specifically designed to determine if each controlled factor is influencing the data values. When you decide to control a factor, you must then decide which groups (or values) will be tested for that factor. These groups are called levels. It is important to remember that samples need to be taken at each level, so a reasonable, but not overwhelming, number of levels should be included, somewhere from 2 to 8 levels.\nOne level of the factor to consider is a control group. A control group is often used to establish a baseline for comparisons and can establish a cause-and-effect relationship by helping the researcher understand its effect on the response variable. A control group is often given a placebo. For example, in a study testing a medication’s efficacy, subjects in the control group receive no actual medication, but are asked to take a sugar pill. The goal of the placebo is to make the experience of the control group and the group that received medication as similar as possible. The very act of taking a pill, or thinking you are receiving medicine, can affect the way you perceive a situation and can even effect the way your body physiologically responds.\nPlacebo does not just refer to medicinal studies. In a study of whether a supervisor’s positive encouragement or negative critiques were more effective at increasing productivity of employees, a placebo group may receive neutral comments. This way, employees’ experience of having an interaction with the supervisor is the same for all 3 groups, but the placebo condition is designed specifically with the goal of not being efficacious.\nAnother issue to consider, which is closely related to placebos, is determining whether the factor level assignment made to a participant should be known to the participants and/or the researchers. When assignments are not known to an individual, the experiment is known as blind. Blinding will reduce potential for bias in the results. Blinding is often used when bias could be introduced if participants know which factor level they are assigned to. If researchers study the impact of pricing on sales of a new product, the participants reaction could be different if they realized other participants were offered a much lower price. If both the participants and researchers do not know the factor level assignments, then it is called double blind.\nIn our toothbrush study, we only discussed looking at the factor of type of toothbrush. This is the factor that we will control in our experiment. The levels would be the 4 different toothbrush types: handheld manual brush, sonic brush, oscillating brush, and ultra-sonic brush. The manual handheld brush could be considered the control group, since it is typically the most common brush, and it makes sense to compare the others to it. The participants would be assigned to a randomly determined brush. It is not possible to blind the participants as to which brush they are using. When the data are collected from the participants it is possible to blind the researchers who are applying the plaque identifying dye and taking the oral pictures. This will remove any biases that could occur from the researchers knowing which brush each participant is using. An example of bias would be if the researcher knew that the participant used a manual handheld brush, so the researcher did not carefully apply the dye or take good pictures because they were not as interested in manual brush results.\n\n\n\nAfter the factors that will be controlled are identified, it is important to list other factors that could influence the data measurements. From that list of factors, identify which factors should be held constant during the experiment, and determine at what value they should be held. Many of the factors you identify may be characteristics of your experimental units. The more uniform that the experimental units and experimental procedures are, the less random uncertainty will be introduced into the experiment. Reducing this unwanted variability will increase the ability of the designed experiment to find significant differences in the controlled factors. This will be discussed in further detail at the end of this section.\nThere are many other factors in our toothbrush study that can affect the amount of plaque on a tooth. We want to hold as many of these constant as we can. Examples of the factors to be held constant and what level to use include: having all participants use the same toothpaste, instructing all participants on how to properly brush their teeth, asking participants to refrain from plaque inducing foods and drinks (give them a specific list of foods/drinks to avoid), having participants brush for the same number of weeks before the data are collected, and asking participants not to floss during the study.\n\n\n\nWhen a factor is not controlled or held constant in an experiment, it would be ideal to be able to measure it, so the researchers can determine if it is influencing the data measurements. This may lead to the necessity of a more complex analysis technique. Later in the course we will discuss using these types of measurements, which are called covariates, and analyzing this data using the analysis of covariance method.\nUnfortunately, uncontrolled factors that change throughout an experiment have the potential to introduce confounding to the study. Confounding occurrs when the effects of one factor cannot be teased apart from the effect of another factor. Consider investigating whether ice cream sales is related to the number of crimes committed in a day. It’s quite possible that higher ice cream sales will occur when higher numbers of crimes are committed. This might lead to the erroneous conclusion that higher ice cream sales result in more crime. In reality, there may be a confounding factor that is influencing both ice cream sales and crime. In this case, that factor could be temperature. As temperature increases, ice cream sales and crime increase. If temperature was not included in the study of crime and ice cream sales, then temperature’s effect cannot be distinguished from the effect of ice cream sales on crime rates. In other words, temperature is confounded with ice cream sales.\nAs shown from this example, confounding can lead to your analysis showing that a controlled factor has significant influence on the response variable when, in fact, there was another factor that may be the real cause of the influence. Measuring an uncontrolled factor will at least allow you to determine if confounding was an issue, it will not, however, fix an issue of confounding. The best way to avoid confounding is to hold uncontrolled factors constant or include the possibly confounding factor as a controlled factor in a well-designed experiment. Even factors that are both included in an experiment can be confounded, especially if the experiment is not designed well.\nWith the toothbrush study, it is quite possible that each participant starts with some amount of plaque which could confound the effect of the brush. One solution would be to clean all the teeth, as well as possible, so that the participant’s initial plaque amount is at (or very close) to zero. Thus, the experimental units would be more uniform. Even after cleaning, the teeth may not be fully clean. So, in addition, you could redefine the response variable to be the difference in initial plaque and ending plaque amount. You could also include the initial plaque amount as a covariate factor in the analysis.\n\n\n\nFrom the list of factors that you will not be controlling or holding constant, there may be factors that cannot be measured. These factors have the potential to cause issues with your experiment. Randomization in your assignment of factor levels to participants and randomization within the experimental process are the best way to minimize the bias that may be introduced due to these factors. Randomization will be discussed in more detail in the next section.\nThe variability caused by factors that are neither controlled nor measured is called random error. These factors may be known or completely unknown. If there is a systematic pattern with the random error, then the random errors will be biased. If the measurements are quite spread out, then the random error is large and the measurements have low reliability. Reliability is a measure of how consistently repeatable the measurements are when values of the measured/controlled factors are identical.\nFigure 1 shows how bias and reliability work. Consider each target, where the bullseye (center of the target) is the truth. Plot 1 is ideal: the measurements (the blue points) have low bias, meaning the measurements are centered around the bullseye, and are close together, meaning the reliability is high. Plot 2 shows substantial bias (points are centered far from the bullseye), although the measurements are highly reliable. Plot 3 shows a low bias in the measurements as they centered near, but not exactly around the bullseye; but the random error is large, meaning low reliability. Plot 4 shows a high bias (measurements centered far from the bullseye) and a low reliability (large variability in the measurements).\nThe systematic variability that causes the high bias as shown in Plots 2 and 4 distorts the results from the truth, which can cause erroneous conclusions to be made. While highly reliable measurements are desirable, measurements that result in low reliability may still result in meaningful conclusions. Though an experiment should strive for low bias and high reliability, reducing bias is usually the top priority. Randomization, as discussed in the next section, is a way to reduce possible bias.\n\n\n\nFigure 1: Four targets illustrating bias and reliability\n\n\nExperiments designed to achieve high reliability increase a study’s statistical power. Statistical power is the probability of finding significant differences between levels of a factor, when, in fact, those differences are real. We will now discuss conceptually how different sources of variation make up the analyses."
  },
  {
    "objectID": "sources_of_variances.html#factors-controlled-during-the-experiment",
    "href": "sources_of_variances.html#factors-controlled-during-the-experiment",
    "title": "Sources of Variation - Factors and Conditions",
    "section": "",
    "text": "These factors are of high importance because the experiment is specifically designed to determine if each controlled factor is influencing the data values. When you decide to control a factor, you must then decide which groups (or values) will be tested for that factor. These groups are called levels. It is important to remember that samples need to be taken at each level, so a reasonable, but not overwhelming, number of levels should be included, somewhere from 2 to 8 levels.\nOne level of the factor to consider is a control group. A control group is often used to establish a baseline for comparisons and can establish a cause-and-effect relationship by helping the researcher understand its effect on the response variable. A control group is often given a placebo. For example, in a study testing a medication’s efficacy, subjects in the control group receive no actual medication, but are asked to take a sugar pill. The goal of the placebo is to make the experience of the control group and the group that received medication as similar as possible. The very act of taking a pill, or thinking you are receiving medicine, can affect the way you perceive a situation and can even effect the way your body physiologically responds.\nPlacebo does not just refer to medicinal studies. In a study of whether a supervisor’s positive encouragement or negative critiques were more effective at increasing productivity of employees, a placebo group may receive neutral comments. This way, employees’ experience of having an interaction with the supervisor is the same for all 3 groups, but the placebo condition is designed specifically with the goal of not being efficacious.\nAnother issue to consider, which is closely related to placebos, is determining whether the factor level assignment made to a participant should be known to the participants and/or the researchers. When assignments are not known to an individual, the experiment is known as blind. Blinding will reduce potential for bias in the results. Blinding is often used when bias could be introduced if participants know which factor level they are assigned to. If researchers study the impact of pricing on sales of a new product, the participants reaction could be different if they realized other participants were offered a much lower price. If both the participants and researchers do not know the factor level assignments, then it is called double blind.\nIn our toothbrush study, we only discussed looking at the factor of type of toothbrush. This is the factor that we will control in our experiment. The levels would be the 4 different toothbrush types: handheld manual brush, sonic brush, oscillating brush, and ultra-sonic brush. The manual handheld brush could be considered the control group, since it is typically the most common brush, and it makes sense to compare the others to it. The participants would be assigned to a randomly determined brush. It is not possible to blind the participants as to which brush they are using. When the data are collected from the participants it is possible to blind the researchers who are applying the plaque identifying dye and taking the oral pictures. This will remove any biases that could occur from the researchers knowing which brush each participant is using. An example of bias would be if the researcher knew that the participant used a manual handheld brush, so the researcher did not carefully apply the dye or take good pictures because they were not as interested in manual brush results."
  },
  {
    "objectID": "sources_of_variances.html#factors-held-constant-during-the-experiment",
    "href": "sources_of_variances.html#factors-held-constant-during-the-experiment",
    "title": "Sources of Variation - Factors and Conditions",
    "section": "",
    "text": "After the factors that will be controlled are identified, it is important to list other factors that could influence the data measurements. From that list of factors, identify which factors should be held constant during the experiment, and determine at what value they should be held. Many of the factors you identify may be characteristics of your experimental units. The more uniform that the experimental units and experimental procedures are, the less random uncertainty will be introduced into the experiment. Reducing this unwanted variability will increase the ability of the designed experiment to find significant differences in the controlled factors. This will be discussed in further detail at the end of this section.\nThere are many other factors in our toothbrush study that can affect the amount of plaque on a tooth. We want to hold as many of these constant as we can. Examples of the factors to be held constant and what level to use include: having all participants use the same toothpaste, instructing all participants on how to properly brush their teeth, asking participants to refrain from plaque inducing foods and drinks (give them a specific list of foods/drinks to avoid), having participants brush for the same number of weeks before the data are collected, and asking participants not to floss during the study."
  },
  {
    "objectID": "sources_of_variances.html#factors-not-controlled-but-measured-during-the-experiment",
    "href": "sources_of_variances.html#factors-not-controlled-but-measured-during-the-experiment",
    "title": "Sources of Variation - Factors and Conditions",
    "section": "",
    "text": "When a factor is not controlled or held constant in an experiment, it would be ideal to be able to measure it, so the researchers can determine if it is influencing the data measurements. This may lead to the necessity of a more complex analysis technique. Later in the course we will discuss using these types of measurements, which are called covariates, and analyzing this data using the analysis of covariance method.\nUnfortunately, uncontrolled factors that change throughout an experiment have the potential to introduce confounding to the study. Confounding occurrs when the effects of one factor cannot be teased apart from the effect of another factor. Consider investigating whether ice cream sales is related to the number of crimes committed in a day. It’s quite possible that higher ice cream sales will occur when higher numbers of crimes are committed. This might lead to the erroneous conclusion that higher ice cream sales result in more crime. In reality, there may be a confounding factor that is influencing both ice cream sales and crime. In this case, that factor could be temperature. As temperature increases, ice cream sales and crime increase. If temperature was not included in the study of crime and ice cream sales, then temperature’s effect cannot be distinguished from the effect of ice cream sales on crime rates. In other words, temperature is confounded with ice cream sales.\nAs shown from this example, confounding can lead to your analysis showing that a controlled factor has significant influence on the response variable when, in fact, there was another factor that may be the real cause of the influence. Measuring an uncontrolled factor will at least allow you to determine if confounding was an issue, it will not, however, fix an issue of confounding. The best way to avoid confounding is to hold uncontrolled factors constant or include the possibly confounding factor as a controlled factor in a well-designed experiment. Even factors that are both included in an experiment can be confounded, especially if the experiment is not designed well.\nWith the toothbrush study, it is quite possible that each participant starts with some amount of plaque which could confound the effect of the brush. One solution would be to clean all the teeth, as well as possible, so that the participant’s initial plaque amount is at (or very close) to zero. Thus, the experimental units would be more uniform. Even after cleaning, the teeth may not be fully clean. So, in addition, you could redefine the response variable to be the difference in initial plaque and ending plaque amount. You could also include the initial plaque amount as a covariate factor in the analysis."
  },
  {
    "objectID": "sources_of_variances.html#factors-not-controlled-and-not-measured",
    "href": "sources_of_variances.html#factors-not-controlled-and-not-measured",
    "title": "Sources of Variation - Factors and Conditions",
    "section": "",
    "text": "From the list of factors that you will not be controlling or holding constant, there may be factors that cannot be measured. These factors have the potential to cause issues with your experiment. Randomization in your assignment of factor levels to participants and randomization within the experimental process are the best way to minimize the bias that may be introduced due to these factors. Randomization will be discussed in more detail in the next section.\nThe variability caused by factors that are neither controlled nor measured is called random error. These factors may be known or completely unknown. If there is a systematic pattern with the random error, then the random errors will be biased. If the measurements are quite spread out, then the random error is large and the measurements have low reliability. Reliability is a measure of how consistently repeatable the measurements are when values of the measured/controlled factors are identical.\nFigure 1 shows how bias and reliability work. Consider each target, where the bullseye (center of the target) is the truth. Plot 1 is ideal: the measurements (the blue points) have low bias, meaning the measurements are centered around the bullseye, and are close together, meaning the reliability is high. Plot 2 shows substantial bias (points are centered far from the bullseye), although the measurements are highly reliable. Plot 3 shows a low bias in the measurements as they centered near, but not exactly around the bullseye; but the random error is large, meaning low reliability. Plot 4 shows a high bias (measurements centered far from the bullseye) and a low reliability (large variability in the measurements).\nThe systematic variability that causes the high bias as shown in Plots 2 and 4 distorts the results from the truth, which can cause erroneous conclusions to be made. While highly reliable measurements are desirable, measurements that result in low reliability may still result in meaningful conclusions. Though an experiment should strive for low bias and high reliability, reducing bias is usually the top priority. Randomization, as discussed in the next section, is a way to reduce possible bias.\n\n\n\nFigure 1: Four targets illustrating bias and reliability\n\n\nExperiments designed to achieve high reliability increase a study’s statistical power. Statistical power is the probability of finding significant differences between levels of a factor, when, in fact, those differences are real. We will now discuss conceptually how different sources of variation make up the analyses."
  },
  {
    "objectID": "testqmd.html",
    "href": "testqmd.html",
    "title": "Describing Data",
    "section": "",
    "text": "A section to reference another file\n\nCalculating treatment means for one factor:\n\nlibrary(mosaic)\nlibrary(pander)\nfavstats(Y~X, data=YourDataSet)\n\nDP Example code:\n\n\n library(mosaic) mosaic is an R Package that is useful in the teaching of statistics to beginning programmers.  library(pander) pander is an R Package that makes R output look pretty  favstats( a function from the mosaic package that returns a set of favorite summary statistics  Temp This is our response variable. From ?airquality you can see in the help file that Temp is the maximum daily temp in degrees F at La Gaurdia Aiport during 1973  ~ “~” is the tilde symbol. It can be interpreted as “y broken down by x”; “y modeled by x”; “y explained by x”, etc. Where y is on the left of the tilde and x is on the right.   Month, “Month” is a column from the airquality dataset that can be treated as qualitative.  data = airquality You have to tell R what dataset the variables Temp and Month come from. ‘airquality’ is a preloaded dataset in R.   ) Functions must always end with a closing parenthesis.   Click to view output  Click to View Output. \n\n\n\n\nNew section\nCalculating treatment means for one factor:\n\nlibrary(mosaic)\nlibrary(pander)\nfavstats(Y~X, data=YourDataSet)\n\nExample code:\n\n\n library(mosaic) mosaic is an R Package that is useful in the teaching of statistics to beginning programmers.  library(pander) pander is an R Package that makes R output look pretty  favstats( a function from the mosaic package that returns a set of favorite summary statistics  Temp This is our response variable. From ?airquality you can see in the help file that Temp is the maximum daily temp in degrees F at La Gaurdia Aiport during 1973  ~ “~” is the tilde symbol. It can be interpreted as “y broken down by x”; “y modeled by x”; “y explained by x”, etc. Where y is on the left of the tilde and x is on the right.   Month, “Month” is a column from the airquality dataset that can be treated as qualitative.  data = airquality You have to tell R what dataset the variables Temp and Month come from. ‘airquality’ is a preloaded dataset in R.   ) Functions must always end with a closing parenthesis.   Click to view output  Click to View Output."
  },
  {
    "objectID": "unbalanced.html",
    "href": "unbalanced.html",
    "title": "Analysis of Unbalanced Data",
    "section": "",
    "text": "A balanced dataset is one in which there are an equal number of replicates for each and every factor level combination. Unbalanced datasets are those that have an unequal number of replicates in at least one factor level combination. In reality, it is more common to have unbalanced data than balanced data. This is particularly true for observational studies. Therefore, it is critical to learn how to analyze unbalanced data.\nThe formulas for unbalanced data are more complicated than when the data is balanced. This is not an issue since we will let computers do the calculations. The bigger issue deals with the fact that unbalanced datasets often result in correlated factor effects. Due to the equal number of replicates, factors in a balanced dataset are orthogonal. Factors in an unbalanced dataset are not guaranteed this desirable property.1\n\n\nFigure 1 is a graphical depiction of an unbalanced dataset for two factors A and B, each with 2 levels, coded as -1 and 1. You can see that each factor level combination appears an equal number of times2.\n\n\n\n\n\nFigure 1: Balanced, orthogonal data\n\n\n\n\nThe correlation of these two columns of 1’s and -1’s is zero.\n\n\nWe are measuring the correlation of the coded factors with each other. We have not even bothered to define the response variable in these examples, since it is irrelevant to the discussion.\nNow consider Figure 2, where there are fewer observations for the A=1, B=-1 combination, and more observations for A=1, B=1.\n\n\n\n\n\nFigure 2: Unbalanced, non-orthogonal data\n\n\n\n\nThe correlation between the two factors is NOT zero.\nThe issue with correlated factors is that their effects on the response are also confounded. When the response varies it cannot be determined how much of the variation in the response is due to factor A and how much was due to factor B."
  },
  {
    "objectID": "unbalanced.html#type-i-1",
    "href": "unbalanced.html#type-i-1",
    "title": "Analysis of Unbalanced Data",
    "section": "Type I",
    "text": "Type I\nFor an example of when Type I may be useful we do not need to look any further than the plant study we have been using all along on this page. Say our research question aims to test the effectiveness of thinning after accounting for the plant’s initial size our hypothesis matches a Type I approach. The research question specifies the order in which we want to consider the factors. Since Type I ANOVA is dependent on the order in which factors are specified in the model, we are able to incorporate the desired order from our research question into the analysis."
  },
  {
    "objectID": "unbalanced.html#type-ii-1",
    "href": "unbalanced.html#type-ii-1",
    "title": "Analysis of Unbalanced Data",
    "section": "Type II",
    "text": "Type II\nYou may want to use a Type II ANOVA if the disparity in sample sizes reflects a similar disparity in population size. Perhaps your end goal is to apply the treatment to all members of the population and you want any difference observed in the study to be present in the population. In that case, Type II may be the best choice. This will be more common in observational studies.\nFor example, a web developer randomly assigns website visitors to one of two website designs. The population that visits this website is predominantly male. Therefore, Design A and Design B receive many more males than females. In this case, the lopsided nature of the gender distribution may accurately reflect the population of website visitors being predominantly male.\nThe test of the main effects for gender and website design will be influenced by sample sizes. Say we conclude that people tend to buy more products when presented with website B. This conclusion will be influenced more heavily by males since males represented a larger proportion of the sample. But in this case that’s okay because males also represent a larger part of our target audience (website visitors)."
  },
  {
    "objectID": "unbalanced.html#type-iii-1",
    "href": "unbalanced.html#type-iii-1",
    "title": "Analysis of Unbalanced Data",
    "section": "Type III",
    "text": "Type III\nLet’s continue with the website design and gender study mentioned above. Now, instead of taking the role of a business owner, we assume the role of a cognitive psychologist. We are not interested in trying to maximize profits for a target market. Rather, we are interested in knowing how individuals respond to the two different site designs. The fact that we partnered with a website with predominantly male clientele is incidental. We want to arrive at conclusions that will be useful in explaining/predicting behavior of an individual.\nIf we use Type II ANOVA the result of the significance test will be confounded with the fact that males account for a larger portion of the sample. A Type III ANOVA will not weight by sample size and will treat the estimated means of males and females equally. Thus our results can be used to predict an individual’s behavior based on their gender and website design. Our findings can now be used by other websites with a different clientele (e.g. predominantly female) to correctly predict/explain individual’s actions."
  },
  {
    "objectID": "unbalanced.html#summary",
    "href": "unbalanced.html#summary",
    "title": "Analysis of Unbalanced Data",
    "section": "summary()",
    "text": "summary()\nMany students are in the habit of using summary() to evaluate model hypothesis tests. When called on an aov object (i.e. an object created using the aov() command), summary()’ prints a Type I ANOVA table.\nHowever, when called on an lm() object, summary() does not print an ANOVA table at all. The default output is to print variable coefficients, standard deviations, and t-test results (t statistic and p-value). If the object has an independent, categorical factor variable a t-test will be conducted for the factor levels. (Actually, all but 1 factor level will be tested. The missing factor level is considered the reference level and is included in the intercept. The reference level is whatever level comes first alphabetically, unless defined otherwise.). If there are only two factor levels, the p-value from the t-test of that factor will be equal to the p-values from a Type III ANOVA applied to the same model.\n\n\nlm() is usually preferred over aov() if there are more than 1 numeric independent variables in the model.\nTo get an ANOVA table and the F-tests for a factor’s overall significance you can call car::Anova() or anova() on an lm() object."
  },
  {
    "objectID": "unbalanced.html#footnotes",
    "href": "unbalanced.html#footnotes",
    "title": "Analysis of Unbalanced Data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor a brief, accessible description of how orthogonality relates to balanced designs, read here: https://www.statisticshowto.com/orthogonality/. The last sentence states “Orthogonality is present in a model if any factor’s effects sum to zero across the effects of any other factors in the table.” We know a factor’s effects sum to zero, and when factors are crossed, the above quoted statement is also true IF there are an equal number of replicates at each factor level. When the number of replicates is not equal, the sum across factor levels is not guaranteed to be exactly zero; so, orthogonality is no longer guaranteed.↩︎\nhttps://stats.stackexchange.com/questions/552702/multifactor-anova-what-is-the-connection-between-sample-size-and-orthogonality↩︎\n Journal of Animal Ecology, Volume: 79, Issue: 2, Pages: 308-316, First published: 05 February 2010, DOI: (10.1111/j.1365-2656.2009.01634.x)↩︎\nShaw, R.G. & Mitchell-Olds, T. (1993) ANOVA for unbalanced data: an overview. Ecology, 74, 1638–1645.↩︎\n Journal of Animal Ecology, Volume: 79, Issue: 2, Pages: 308-316, First published: 05 February 2010, DOI: (10.1111/j.1365-2656.2009.01634.x)↩︎\nThere is also the possibility the total sum of squares is more than the sum of squares total due to “double counting” the variation. This is much more difficult to illustrate with Venn diagrams.↩︎"
  }
]